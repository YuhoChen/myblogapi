-- MySQL dump 10.13  Distrib 5.7.25, for Linux (x86_64)
--
-- Host: localhost    Database: yuhao_blog
-- ------------------------------------------------------
-- Server version	5.7.21-0ubuntu0.16.04.1

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `admin`
--

DROP TABLE IF EXISTS `admin`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `admin` (
  `aid` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `user_id` varchar(128) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '用户id',
  `username` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '账号',
  `password` varchar(255) NOT NULL COMMENT '密码',
  `salt` varchar(255) NOT NULL COMMENT '秘钥',
  `access_token` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL COMMENT 'access_token',
  `token_expires_in` int(13) DEFAULT NULL COMMENT 'token有效期至',
  `create_time` int(13) NOT NULL COMMENT '创建时间',
  `status` bit(1) NOT NULL DEFAULT b'0' COMMENT '状态，0为正常，默认0',
  `last_login_time` int(13) DEFAULT NULL COMMENT '最后登录时间',
  PRIMARY KEY (`aid`,`user_id`) USING BTREE,
  UNIQUE KEY `user_id` (`user_id`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `admin`
--

LOCK TABLES `admin` WRITE;
/*!40000 ALTER TABLE `admin` DISABLE KEYS */;
INSERT INTO `admin` VALUES (2,'2Op8o8cnVxYbMoRzDPHNvW','admin','7740d0c483a315f0439fa594f4829332','$2y$10$esoqWXn/Q46B05tIo775OuuiAI1WdOZDnW6ogHpaJMd5p/W7vt/Ji','79sd5DVsEUYXcFMKY8yBm0',1551497347,1550148298,_binary '\0',1550892547);
/*!40000 ALTER TABLE `admin` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `article`
--

DROP TABLE IF EXISTS `article`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `article` (
  `aid` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `id` varchar(128) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '文章id',
  `title` varchar(255) DEFAULT NULL COMMENT '文章标题',
  `category_id` varchar(128) DEFAULT NULL COMMENT '文章分类id',
  `create_time` int(13) NOT NULL COMMENT '创建时间',
  `delete_time` int(13) DEFAULT NULL COMMENT '删除时间',
  `update_time` int(13) DEFAULT NULL COMMENT '更新时间',
  `publish_time` int(13) DEFAULT NULL COMMENT '发布时间',
  `status` tinyint(1) DEFAULT '0' COMMENT '状态，0-正常（发布），1-删除，2-记录（待发布）',
  `content` text COMMENT '内容',
  `html_content` text COMMENT '生成的html',
  `cover` text COMMENT '封面图',
  `sub_message` text COMMENT '文章简介',
  `pageview` int(11) DEFAULT '0' COMMENT '文章阅读数',
  `is_encrypt` bit(1) DEFAULT b'0' COMMENT '是否加密，0否，1是，默认0',
  PRIMARY KEY (`aid`,`id`) USING BTREE,
  UNIQUE KEY `id` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `article`
--

LOCK TABLES `article` WRITE;
/*!40000 ALTER TABLE `article` DISABLE KEYS */;
INSERT INTO `article` VALUES (1,'7hgdrxJvILWPMvwgcAtsX6','测试','',1550673332,1550832753,1550832753,1550673332,2,'测试文章\ntest\ntest\ntest','<p>测试文章<br>test<br>test<br>test</p>\n','http://www.yuhaochen.top:8888/imgs/2019/02/8ccd0d6fede0aa5d.jpg','test',15,_binary ''),(2,'E0zqyEeRilsDSjSwFAU5I','用深度学习让你的照片变得更美丽','5DKFCekkrGH981BYKRrqt8',1550816095,NULL,1550816159,1550816095,0,'![](https://i.imgur.com/FirxWRj.png)\n## Abstract\nDespite a rapid rise in the quality of built-in smartphone cameras, their physical limitations — small sensor size, compact lenses and the lack of specific hardware, — impede them to achieve the quality results of DSLR cameras. In this work we present an end-to-end deep learning approach that bridges this gap by translating ordinary photos into DSLR-quality images.We propose learning the translation function using a residual convolutional neural network that improves both color rendition and image sharpness. Since the standard mean squared loss is not well suited for measuring perceptual image quality, we introduce a composite perceptual error function that combines content, color and texture losses. The first two losses are defined analytically, while the texture loss is learned in an adversarial fashion. We also present DPED, a large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera. Our quantitative and qualitative assessments reveal that the enhanced image quality is comparable to that of DSLR-taken photos, while the methodology is generalized to any type of digital camera.\n> 尽管内置智能手机相机的质量迅速提升，但它们的物理限制 - 小尺寸传感器，小巧镜头和缺乏特定硬件 - 阻碍了它们实现数码单反相机的高质量效果。在这项工作中，我们提出了一种端到端的深度学习方法，通过将普通照片转换为DSLR质量的图像来弥补这一差距。我们建议使用残余卷积神经网络来学习翻译功能，从而改善色彩再现和图像清晰度。由于标准均方损失不适合测量感知图像质量，我们引入了一种复合感知误差函数，它结合了内容，颜色和纹理损失。前两个损失是分析定义的，而纹理损失是以对抗方式学习的。我们还展示了DPED，这是一个大型数据集，包括从三个不同手机和一个高端反光相机拍摄的真实照片。我们的定量和定性评估表明，增强的图像质量可与DSLR拍摄的照片相媲美，而该方法可推广到任何类型的数码相机。\n\n## 1 Introduction\nDuring the last several years there has been a significant improvement in compact camera sensors quality, which has brought mobile photography to a substantially new level. Even low-end devices are now able to take reasonably good photos in appropriate lighting conditions, thanks to their advanced software and hardware tools for post-processing. However, when it comes to artistic quality, mobile devices still fall behind their DSLR counterparts. Larger sensors and high-aperture optics yield better photo resolution, color rendition and less noise,whereas their additional sensors help to fine-tune shooting parameters. These physical differences result in strong obstacles, making DSLR camera quality unattainable for compact mobile devices.\n>在过去几年中，紧凑型相机传感器质量得到了显着改善，这使得移动摄影技术达到了一个新的水平。 由于其先进的软件和硬件工具可用于后期处理，即使是低端设备现在也能够在适当的照明条件下拍摄相当不错的照片。 然而，在艺术品质方面，移动设备仍然落后于数码单反相机。 较大的传感器和高光圈光学系统可提供更好的照片分辨率，色彩还原和更低的噪点，而其附加传感器有助于微调拍摄参数。 这些物理差异导致了强大的障碍，使得紧凑型移动设备无法实现DSLR相机质量。\n\nWhile a number of photographer tools for automatic image enhancement exist, they are usually focused on adjusting only global parameters such as contrast or brightness, without improving texture quality or taking image semantics into account.Besides that, they are usually based on a pre-defined set of rules that do not always consider the specifics of a particular device. Therefore, the dominant approach to photo post-processing is still based on manual image correction using specialized retouching software.\n>虽然存在许多用于自动图像增强的摄影师工具，但它们通常专注于仅调整对比度或亮度等全局参数，而不会改善纹理质量或考虑图像语义。此外，它们通常基于预定义 一组规则并不总是考虑特定设备的细节。 因此，照片后处理的主要方法仍然是基于使用专门的修饰软件的手动图像校正。\n\n### 1.1 Related work\nThe problem of automatic image quality enhancement has not been addressed in its entirety in the area of computer vision,though a number of sub-tasks and related problems have been already successfully solved using deep learning techniques. Such tasks are usually dealing with image-to-image translation problems, and their common property is that they are targeted at removing artificially added artifacts to the original images. Among the related problems are the following:\n**Image super-resolution** aims at restoring the original image from its downscaled version. In [4] a CNN architecture and MSE loss are used for directly learning low to high resolution mapping. It is the first CNN-based solution to achieve top performance in single image super-resolution, comparable with non-CNN methods [20]. The subsequent works developed deeper and more complex CNN architectures (e.g., [10, 18,16]). Currently, the best photo-realistic results on this task are achieved using a VGG-based loss function [9] and adversarial networks [12] that turned out to be efficient at recovering plausible high-frequency components.\n**Image deblurring/dehazing** tries to remove artificially added haze or blur from the images. Usually, MSE is used as a target loss function and the proposed CNN architectures consist of 3 to 15 convolutional layers [14, 2, 6] or are bi-channel CNNs [17].\n**Image denoising/sparse** inpainting similarly targets removal of noise and artifacts from the pictures. In [28] the authors proposed weighted MSE together with a 3-layer CNN, while in [19]it was shown that an 8-layer residual CNN performs better when using a standard mean square error. Among other solutions are a bi-channel CNN [29], a 17-layer CNN [26] and a recurrent CNN [24] that was reapplied several times to the produced results.\n**Image colorization**. Here the goal is to recover colors that were removed from the original image. The baseline approach for this problem is to predict new values for each pixel based on its local description that consists of various hand-crafted features [3]. Considerably better performance on this task was obtained using generative adversarial networks [8] or a 16-layer CNN with a multinomial cross-entropy loss function [27].\n**Image adjustment**. A few works considered the problem of image color/contrast/exposure adjustment. In [25] the authors proposed an algorithm for automatic exposure correction using hand-designed features and predefined rules. In [23], a more general algorithm was proposed that – similarly to [3] – uses local description of image pixels for reproducing various photographic styles. A different approach was considered in [13], where images with similar content are retrieved from a database and their styles are applied to the target picture. All of these adjustments are implicitly included in our end-to-end transformation learning approach by design.\n>虽然已经使用深度学习技术成功地解决了许多子任务和相关问题，但是在计算机视觉领域尚未完全解决自动图像质量增强的问题。这些任务通常涉及图像到图像的转换问题，它们的共同特性是它们的目标是将人为添加的伪像移除到原始图像。其中相关问题如下：\n**图像超级分辨率**旨在从缩小版本恢复原始图像。在[4]中，CNN架构和MSE损耗用于直接学习低分辨率到高分辨率的映射。这是第一个在单图像超分辨率下实现最佳性能的基于CNN的解决方案，与非CNN方法相当[20]。随后的工作开发了更深入和更复杂的CNN架构（例如，[10,18,16]）。目前，使用基于VGG的损失函数[9]和对抗性网络[12]实现了这项任务的最佳照片般逼真的结果，结果证明这种结果能够有效地恢复似乎合理的高频成分。\n**图像去模糊/去雾**试图从图像中去除人为添加的雾霾或模糊。通常，MSE用作目标损耗函数，并且所提出的CNN架构由3到15个卷积层[14,1,2]组成，或者是双通道CNN [17]。\n**图像去噪/稀疏**修复类似地用于从图片中去除噪声和伪影。在[28]中，作者提出了加权MSE和3层CNN，而在[19]中显示，当使用标准均方误差时，8层残余CNN表现更好。其他解决方案包括双通道CNN [29]，17层CNN [26]和复发CNN [24]，它们多次重新应用于产生的结果。\n**图像着色**。这里的目标是恢复从原始图像中删除的颜色。此问题的基线方法是根据由各种手工制作的特征组成的本地描述来预测每个像素的新值[3]。使用生成对抗网络[8]或具有多项交叉熵损失函数的16层CNN，可以获得相当好的性能[27]。\n**图像调整**。一些作品考虑了图像颜色/对比度/曝光调整的问题。在[25]中，作者提出了一种使用手工设计的特征和预定义规则进行自动曝光校正的算法。在[23]中，提出了一种更通用的算法 - 类似于[3]  - 使用图像像素的局部描述来再现各种摄影风格。在[13]中考虑了一种不同的方法，其中从数据库中检索具有相似内容的图像，并将它们的样式应用于目标图像。所有这些调整都隐含在我们的端到端转型学习方法中。\n\n### 1.2 Contributions\nThe key challenge we face is dealing with all the aforementioned enhancements at once. Even advanced tools cannot notably improve image sharpness, texture details or small color variations that were lost by the camera sensor, thus we can not generate target enhanced photos from the existing ones. Corrupting DSLR photos and training an algorithm on the corrupted images does not work either: the solution would not generalize to real-world and very complex artifacts unless they are modeled and applied as corruptions, which is infeasible. To tackle this problem, we present a different approach: we propose to learn the transformation that modifies photos taken by a given camera to DSLR-quality ones. Thus, the goal is to learn a cross-distribution translation function, where the input distribution is defined by a given mobile camera sensor, and the target distribution by a DSLR sensor. To supervise the learning process, we create and leverage a dataset of images capturing the same scene with different cameras. Once the function is learned, it can be further applied to unseen photos at will.\n>我们面临的主要挑战是立即处理上述所有增强功能。即使是先进的工具也无法显着改善相机传感器丢失的图像清晰度，纹理细节或小的颜色变化，因此我们无法从现有的照片生成目标增强照片。破坏DSLR照片并在损坏的图像上训练算法也不起作用：解决方案不会推广到真实世界和非常复杂的工件，除非它们被建模并应用为损坏，这是不可行的。为了解决这个问题，我们提出了一种不同的方法：我们建议学习将给定相机拍摄的照片修改为DSLR质量的转换。因此，目标是学习交叉分布转换函数，其中输入分布由给定的移动相机传感器定义，并且目标分布由DSLR传感器定义。为了监督学习过程，我们创建并利用使用不同相机捕获相同场景的图像数据集。一旦学习了该功能，它就可以进一步应用于看不见的照片。\n\n![](https://i.imgur.com/LNYA4Zz.png)\n\nOur main contributions are:\n• A novel approach1 for the photo enhancement task based on learning a mapping function between photos from mobile devices and a DSLR camera. The target model is trained in an end-to-end fashion without using any additional supervision or handcrafted features.\n• A new large-scale dataset of over 6K photos taken synchronously by a DSLR camera and 3 low-end cameras of smartphones in a wide variety of conditions.\n• A multi-term loss function composed of color, texture and content terms, allowing an efficient image quality estimation.\n• Experiments measuring objective and subjective quality demonstrating the advantage of the enhanced photos over the originals and, at the same time, their comparable quality with the DSLR counterparts.\n>我们的主要贡献是：\n•基于学习移动设备和DSLR相机之间的映射功能的照片增强任务的新方法1。 目标模型以端到端的方式进行培训，无需使用任何额外的监督或手工制作的功能。\n•由DSLR相机和智能手机的3个低端相机在各种条件下同步拍摄的6,000多张照片的新大型数据集。\n•由颜色，纹理和内容术语组成的多项损失功能，可以进行有效的图像质量评估。\n•测量客观和主观质量的实验证明了增强的照片相对于原件的优势，同时，它们与DSLR对应物的质量相当。\n\nThe remainder of the paper is structured as follows. In Section 2 we describe the new DPED dataset. Section 3 presents our architecture and the chosen loss functions. Section 4 shows and analyzes the experimental results. Finally, Section 5 concludes the paper.\n>在本文的其余结构如下。 在第2节中，我们描述了新的DPED数据集。 第3节介绍了我们的架构和所选的损失函数。 第4节显示并分析了实验结果。 最后，第5节总结了论文。\n\n![](https://i.imgur.com/WC0hvvr.png)\n\n## 2 DSLR Photo Enhancement Dataset\nIn order to tackle the problem of image translation from poor quality images captured by smartphone cameras to superior quality images achieved by a professional DSLR camera, we introduce a large-scale real-world dataset, namely the “DSLR Photo Enhancement Dataset” (DPED)2, that can be used for the general photo quality enhancement task. DPED consists of photos taken in the wild synchronously by three smartphones and one DSLR camera. The devices used to collect the data are described in Table 1 and example quadruplets can be seen in Figure 3.\n>为了解决从智能手机相机拍摄的低质量图像到专业数码单反相机拍摄的高质量图像的图像转换问题，我们引入了大规模的真实数据集，即“DSLR照片增强数据集”（DPED） 2，可用于一般照片质量增强任务。 DPED由三部智能手机和一部数码单反相机同步拍摄的照片组成。 表1中描述了用于收集数据的设备，图3中显示了示例四联体。\n\nTo ensure that all cameras were capturing photos simultaneously, the devices were mounted on a tripod and activated remotely by a wireless control system (see Figure 2). In total, over 22K photos were collected during 3 weeks, including 4549 photos from Sony smartphone, 5727 from iPhone and 6015 photos from each Canon and BlackBerry cameras. The photos were taken during the daytime in a wide variety of places and in various illumination and weather conditions. The photos were captured in automatic mode, and we used default settings for all cameras throughout the whole collection procedure.\n>为了确保所有摄像机同时捕获照片，这些设备安装在三脚架上并通过无线控制系统远程激活（参见图2）。 总共在3周内收集了超过22,000张照片，其中包括索尼智能手机的4549张照片，iPhone的5727张照片以及佳能和黑莓相机的6015张照片。 这些照片是在白天在各种各样的地方以及各种照明和天气条件下拍摄的。 照片是在自动模式下拍摄的，我们在整个收集过程中使用了所有相机的默认设置。\n\n**Matching algorithm.** The synchronously captured images are not perfectly aligned since the cameras have different viewing angles and positions as can be seen in Figure 3. To address this,we performed additional non-linear transformations resulting in a fixed-resolution image that our network takes as an input. The algorithm goes as follows (see Fig. 4). First, for each (phoneDSLR) image pair, we compute and match SIFT keypoints [15]across the images. These are used to estimate a homography using RANSAC [21]. We then crop both images to the intersection part and downscale the DSLR image crop to the size of the phone crop.\n>**匹配算法**.同步捕获的图像没有完美对齐，因为摄像机具有不同的视角和位置，如图3所示。为了解决这个问题，我们进行了额外的非线性变换，得到了固定分辨率的图像。 我们的网络作为输入。 算法如下（见图4）。 首先，对于每个（phoneDSLR）图像对，我们在图像上计算和匹配SIFT关键点[15]。 这些用于使用RANSAC估计单应性[21]。 然后，我们将两个图像裁剪到交叉点部分，并将DSLR图像裁剪缩小到手机裁剪的大小。\n\n![](https://i.imgur.com/5JYXDrJ.png)\n>图4：匹配算法：通过SIFT描述符匹配确定重叠区域，然后是非线性变换和裁剪，从而产生表示相同场景的相同分辨率的两个图像。\n\nTraining CNN on the aligned high-resolution images is infeasible, thus patches of size 100×100px were extracted from these photos. Our preliminary experiments revealed that larger patch sizes do not lead to better performance, while requiring considerably more computational resources. We extracted patches using a non-overlapping sliding window. The window was moving in parallel along both images from each phone-DSLR image pair, and its position on the phone image was additionally adjusted by shifts and rotations based on the cross-correlation metrics. To avoid significant displacements, only patches with cross-correlation greater than 0.9 were included in the dataset. Around 100 original images were reserved for testing, the rest of the photos were used for training and validation. This procedure resulted in 139K, 160K and 162K training and 2.4-4.3K test patches for BlackBerry-Canon, iPhone-Canon and Sony-Canon pairs, respectively. It should be emphasized that both training and test patches are precisely matched, the potential shifts do not exceed 5 pixels. In the following we assume that these patches of size 3×100×100 constitute the input data to our CNNs.\n>在对齐的高分辨率图像上训练CNN是不可行的，因此从这些照片中提取尺寸为100×100px的块。我们的初步实验表明，较大的补丁大小不会带来更好的性能，同时需要更多的计算资源。我们使用非重叠滑动窗口提取补丁。窗口沿着来自每个phone-DSLR图像对的两个图像并行移动，并且其在phone图像上的位置另外通过基于互相关度量的移位和旋转来调整。为避免显着位移，数据集中仅包含互相关大于0.9的补丁。大约100张原始图像被保留用于测试，其余照片用于训练和验证。此程序分别为BlackBerry-Canon，iPhone-Canon和Sony-Canon对提供了139K，160K和162K训练以及2.4-4.3K测试补丁。应该强调的是，训练和测试补丁都是精确匹配的，潜在的移位不超过5个像素。在下文中，我们假设这些大小为3×100×100的补丁构成了CNN的输入数据。\n\n## 3 Method\nGiven a low-quality photo I<sub>s</sub> (source image), the goal of the considered enhancement task is to reproduce the image I<sub>t</sub> (target image) taken by a DSLR camera. A deep residual CNN FW parameterized by weights W is used to learn the underlying translation function. Given the training set {I<sup>j</sup><sub>s</sub>, I<sup>j</sup><sub>t</sub>}<sup>N</sup><sub>j=1</sub> consisting of N image pairs, it is trained to minimize:\n![](https://i.imgur.com/pEMUuTq.png)\nwhere L denotes a multi-term loss function we detail in section 3.1. We then define the system architecture of our solution in Section 3.2\n>给定低质量照片I<sub>s</sub>（源图像），所考虑的增强任务的目标是再现由DSLR相机拍摄的图像I <sub>t</sub>（目标图像）。 由权重W参数化的深度残余CNN FW用于学习基础翻译函数。 给定训练集{I<sup>j</sup><sub>s</sub>，I<sup>j</sup><sub>t</sub>}<sup>N</sup><sub>j=1</sub>由N个图像对组成，训练最小化：\n其中L表示多项损失函数，我们在3.1节中详细说明。 然后，我们在3.2节中定义了我们解决方案的系统架构\n\n![](https://i.imgur.com/6QX3SYC.png)\n>图5：手机（最左边两个）和DSLR（最右边两个）摄像头拍摄的原始图像和模糊图像的碎片。 模糊消除了高频并使颜色比较更容易。\n\n### 3.1 Loss function\nThe main difficulty of the image enhancement task is that input and target photos cannot be matched densely (i.e., pixelto-pixel): different optics and sensors cause specific local nonlinear distortions and aberrations, leading to a non-constant shift of pixels between each image pair even after precise alignment. Hence, the standard per-pixel losses, besides being doubtful as a perceptual quality metric, are not applicable in our case. We build our loss function under the assumption that the overall perceptual image quality can be decomposed into three independent parts: i) color quality, ii) texture quality and iii) content quality. We now define loss functions for each component, and ensure invariance to local shifts by design.\n>图像增强任务的主要困难是输入和目标照片不能密集匹配（即，像素到像素）：不同的光学器件和传感器引起特定的局部非线性失真和像差，导致每个图像之间像素的非恒定偏移 精确对齐后即成对。 因此，除了作为感知质量度量标准之外，标准的每像素损失不适用于我们的情况。 我们在假设整体感知图像质量可以分解为三个独立部分的情况下构建我们的损失函数：i）颜色质量，ii）纹理质量和iii）内容质量。 我们现在为每个组件定义损失函数，并通过设计确保局部移位的不变性。\n\n#### 3.1.1 Color loss\nTo measure the color difference between the enhanced and target images, we propose applying a Gaussian blur (see Figure 5) and computing Euclidean distance between the obtained representations. In the context of CNNs, this is equivalent to using one additional convolutional layer with a fixed Gaussian kernel followed by the mean squared error (MSE) function. Color loss\ncan be written as:\n>为了测量增强图像和目标图像之间的色差，我们建议应用高斯模糊（参见图5）并计算所获得的表示之间的欧几里德距离。 在CNN的上下文中，这相当于使用一个附加的卷积层和一个固定的高斯核，然后是均方误差（MSE）函数。 颜色损失\n可以写成：\n\n![](https://i.imgur.com/BoRYlzR.png)\nwhere X<sub>b</sub> and Y<sub>b</sub> are the blurred images of X and Y , resp.:\n>其中Xb和Yb是X和Y的模糊图像，分别为：\n\n![](https://i.imgur.com/hDUD61z.png)\nand the 2D Gaussian blur operator is given by\n>和2D高斯模糊算子由下式给出\n\n![](https://i.imgur.com/Lbug5rg.png)\nwhere we defined A = 0.053, µ<sub>x,y</sub> = 0, and σ<sub>x,y</sub> = 3.\n>我们定义A = 0.053，μx，y = 0和σx，y = 3。\n\nThe idea behind this loss is to evaluate the difference in brightness, contrast and major colors between the images while eliminating texture and content comparison. Hence, we fixed a constant σ by visual inspection as the smallest value that ensures that texture and content are dropped. The crucial property of this loss is its invariance to small distortions. Figure 6 demonstrates the MSE and Color losses for image pairs (X, Y), where Y equals X shifted in a random direction by n pixels. As one can see, color loss is nearly insensitive to small distortions (6 2 pixels). For higher shifts (3-5px), it is still about 5-10 times smaller compared to the MSE, whereas for larger displacements it demonstrates similar magnitude and behavior. As a result, color loss forces the enhanced image to have the same color distribution as the target one, while being tolerant to small mismatches.\n>这种损失背后的想法是评估图像之间的亮度，对比度和主要颜色的差异，同时消除纹理和内容比较。 因此，我们通过目视检查将常数σ固定为确保纹理和内容被丢弃的最小值。 这种损失的关键属性是它对小变形的不变性。 图6展示了图像对（X，Y）的MSE和颜色损失，其中Y等于X在随机方向上移位n个像素。 可以看出，颜色损失对小扭曲（6 2像素）几乎不敏感。 对于较高的位移（3-5px），它仍然比MSE小约5-10倍，而对于较大的位移，它表现出相似的幅度和行为。 结果，颜色损失迫使增强的图像具有与目标颜色分布相同的颜色分布，同时容忍小的不匹配。\n\n![](https://i.imgur.com/o0QoHON.png)\n>图6：MSE和颜色损失之间的比较，作为图像之间偏移幅度的函数。 结果平均超过50K图像。\n\n#### 3.1.2 Texture loss\nInstead of using a pre-defined loss function, we build upon generative adversarial networks (GANs) [5] to directly learn a suitable metric for measuring texture quality. The discriminator CNN is applied to grayscale images so that it is targeted specifically on texture processing. It observes both fake (improved) and real (target) images, and its goal is to predict whether the input image is real or not. It is trained to minimize the cross-entropy loss function, and the texture loss is defined as a standard generator objective:\n>我们不是使用预定义的损失函数，而是建立在生成对抗网络（GAN）[5]上，以直接学习用于测量纹理质量的合适度量。 鉴别器CNN应用于灰度图像，使得其特定地针对纹理处理。 它观察假（改进）和真实（目标）图像，其目标是预测输入图像是否真实。 训练最小化交叉熵损失函数，纹理损失定义为标准生成器目标：\n\n![](https://i.imgur.com/jKLESwy.png)\nwhere FW and D denote the generator and discriminator networks, respectively. The discriminator is pre-trained on the {phone, DSLR} image pairs, and then trained jointly with the proposed network as is conventional for GANs. It should be noted that this loss is shift-invariant by definition since no alignment is required in this case.\n>其中FW和D分别表示发生器和鉴别器网络。 鉴别器在{phone，DSLR}图像对上进行预训练，然后与传统的GAN一起与所提出的网络一起训练。 应该注意的是，这种损失根据定义是移位不变的，因为在这种情况下不需要对准。\n\n#### 3.1.3 Content loss\nInspired by [9, 12], we define our content loss based on the activation maps produced by the ReLU layers of the pre-trained VGG-19 network. Instead of measuring per-pixel difference between the images, this loss encourages them to have similar feature representation that comprises various aspects of their content and perceptual quality. In our case it is used to preserve image semantics since other losses don’t consider it. Let ψ<sub>j</sub> () be the feature map obtained after the j-th convolutional layer of the VGG-19 CNN, then our content loss is defined as Euclidean distance between feature representations of the enhanced and target images:\n>受[9,12]的启发，我们根据预训练的VGG-19网络的ReLU层产生的激活图来定义内容丢失。 这种损失不是测量图像之间的每像素差异，而是鼓励它们具有相似的特征表示，其包括其内容和感知质量的各个方面。 在我们的例子中，它用于保留图像语义，因为其他损失不考虑它。 设ψj（）是在VGG-19 CNN的第j个卷积层之后获得的特征图，然后我们的内容丢失被定义为增强图像和目标图像的特征表示之间的欧几里德距离：\n\n![](https://i.imgur.com/LAcT1C6.png)\nwhere C<sub>j</sub> , H<sub>j</sub> and W<sub>j</sub> denotes the number, height and width of the feature maps, and F<sub>W</sub>(I<sub>s</sub>) the enhanced image.\n>其中Cj，Hj和Wj表示特征图的数量，高度和宽度，而FW（Is）表示增强的图像\n\n#### 3.1.4 Total variation loss\nIn addition to previous losses, we add total variation (TV) loss [1] to enforce spatial smoothness of the produced image:\n>除了之前的损失，我们还增加了总变差（TV）损失[1]来强制生成图像的空间平滑度：\n\n![](https://i.imgur.com/POG2ei7.png)\nwhere C, H and W are the dimensions of the generated image F<sub>W</sub>(I<sub>s</sub>). As it is relatively lowly weighted (see Eqn. 8), it does not harm high-frequency components while it is quite effective at removing salt-and-pepper noise.\n>其中C，H和W是生成图像的尺寸FW（Is）。 由于它的重量相对较低（参见方程式8），它不会损害高频成分，同时它在去除椒盐噪声方面非常有效。\n\n#### 3.1.5 Total loss\nOur final loss is defined as a weighted sum of previous losses with the following coefficients:\n>我们的最终损失定义为先前损失的加权和，具有以下系数：\n\n![](https://i.imgur.com/BY6Bjs2.png)\nwhere the content loss is based on the features produced by the relu 5 4 layer of the VGG-19 network. The coefficients were chosen based on preliminary experiments on the DPED training data.\n>其中内容丢失基于VGG-19网络的relu 5 4层产生的特征。 基于DPED训练数据的初步实验选择系数。\n\n### 3.2 Generator and Discriminator CNNs\nFigure 7 illustrates the overall architecture of the proposed CNNs. Our image transformation network is fullyconvolutional, and starts with a 9×9 layer followed by four residual blocks. Each residual block consists of two 3×3 layers alternated with batch-normalization layers. We use two additional layers with kernels of size 3×3 and one with 9×9 kernels after the residual blocks. All layers in the transformation network have 64 channels and are followed by a ReLU activation function, except for the last one, where a scaled tanh is applied to the outputs.\n>图7示出了所提出的CNN的总体架构。 我们的图像变换网络是完全卷积的，从9×9层开始，然后是4个残余块。 每个残余块由两个3×3层组成，与批量标准化层交替。 在剩余块之后，我们使用两个额外的层，其中内核大小为3×3，一个内核具有9×9内核。 转换网络中的所有层都有64个通道，后面跟着ReLU激活功能，除了最后一个，其中缩放的tanh应用于输出。\n\nThe discriminator CNN consists of five convolutional layers each followed by a LeakyReLU nonlinearity and batch normalization. The first, second and fifth convolutional layers are strided with a step size of 4, 2 and 2, respectively. A sigmoidal activation function is applied to the outputs of the last fullyconnected layer containing 1024 neurons and produces a probability that the input image was taken by the target DSLR camera.\n>鉴别器CNN由五个卷积层组成，每个卷层后面是LeakyReLU非线性和批量归一化。 第一，第二和第五卷积层分别以步长4,2和2跨步。 S形激活函数应用于包含1024个神经元的最后一个完全连接层的输出，并产生输入图像由目标DSLR相机拍摄的概率。\n\n![](https://i.imgur.com/pyDNPBr.png)\n\n### 3.3 Training details\nThe network was trained on a NVidia Titan X GPU for 20K iterations using a batch size of 50. The parameters of the network were optimized using Adam [11] modification of stochastic gradient descent with a learning rate of 5e-4. The whole pipelineand experimental setup was identical for all cameras.\n>使用批量大小为50的网络在NVidia Titan X GPU上进行20K次迭代训练。使用Adam [11]修改随机梯度下降，学习率为5e-4，优化网络参数。 所有相机的整个管道和实验装置都是相同的。\n\n## 4 Experimen\nOur general goal to “improve image quality” is subjective and hard to evaluate quantitatively. We suggest a set of tools and methods from the literature that are most relevant to our problem. We use them, as well as our proposed method, on a set of test images taken by mobile devices and compare how close the results are to the DSRL shots.\n\nIn section 4.1, we present the methods we compare to. Then we present both objective and subjective evaluations: the former w.r.t. the ground truth reference (i.e., the DSLR images) in section 4.2, the latter with no reference subjective quality scores in section 4.3. Finally, section 4.4 analyzes the limitations of the proposed solution.\n>我们“提高图像质量”的总体目标是主观的，难以定量评估。 我们建议使用与我们的问题最相关的文献中的一套工具和方法。 我们在移动设备拍摄的一组测试图像上使用它们以及我们提出的方法，并比较结果与DSRL镜头的接近程度。\n在4.1节中，我们介绍了我们比较的方法。 然后我们提出客观和主观评价：前者w.r.t. 4.2节中的地面实况参考（即DSLR图像），后者在4.3节中没有参考主观质量分数。 最后，第4.4节分析了所提出的解决方案的局限性。\n\n### 4.1 Benchmark methods\nIn addition to our proposed photo enhancement solution, we compare with the following tools and methods.\n**Apple Photo Enhancer (APE)** is a commercial product known to generate among the best visual results, while the algorithm is unpublished. We trigger the method using the automatic Enhance function from the Photos app. It performs image improvement without taking any parameters. \n**Dong et al.[4]** is a fundamental baseline super-resolution method, thus addredding a task related to end-to-end image-toimage mapping. Hence we chose it to apply on our task and compare with. The method relies on a standard 3-layer CNN and MSE loss function and maps from low resolution / corrupted image to the restored image.\n**Johnson et al. [9]** is one of the latest state of the art in photo-realistic super-resolution and style transferring tasks. The method is based on a deep residual network (with four residual blocks, each consisting of two convolutional layers) that is trained to minimize a VGG-based loss function. \n**Manual enhancement.** We asked a graphical artist to enhance color, sharpness and general look-and-feel of 9 images using professional software (Adobe Photoshop CS6). A time limit of one workday was given, so as to simulate a realistic scenario. Figure 8 illustrates the ensemble of enhancement methods we consider for comparison in our experiments. Dong et al. [4]and Johnson et al. [9] are trained using the same train image pairs as for our solution for each of the smartphones from the DPED dataset.\n>除了我们提出的照片增强解决方案，我们还与以下工具和方法进行了比较。\nApple Photo Enhancer（APE）是一种商业产品，可以产生最佳的视觉效果，而该算法尚未发布。我们使用Photos应用程序中的自动增强功能触发该方法。它无需任何参数即可执行图像改进。\n董等人[4]是一种基本的基线超分辨率方法，因此增加了与端到端图像到图像映射相关的任务。因此我们选择它来应用我们的任务并与之进行比较。该方法依赖于标准的3层CNN和MSE丢失功能，并从低分辨率/损坏的图像映射到恢复的图像。\n约翰逊等人。 [9]是照片般逼真的超分辨率和风格转换任务的最新技术之一。该方法基于深度残余网络（具有四个残余块，每个残余块由两个卷积层组成），其被训练以最小化基于VGG的损失函数。\n手动增强。我们要求图形艺术家使用专业软件（Adobe Photoshop CS6）增强9幅图像的色彩，清晰度和一般外观。给出了一个工作日的时间限制，以模拟现实场景。图8说明了我们在实验中考虑进行比较的增强方法集合。董等人。 [4]和约翰逊等人。 [9]使用与DPED数据集中每个智能手机的解决方案相同的列车图像对进行训练。\n\n![](https://i.imgur.com/geOffNY.jpg)\n\n### 4.2 Quantitative evaluation\nWe first quantitatively compare APE, Dong et al. [4], Johnson et al. [9] and our method on the task of mapping photos from three low-end cameras to the high-quality DSLR (Canon) images and report the results in Table 3. As such, we do not evaluate global image quality but, rather, we measure resemblance to a reference(the ground truth DSLR image). We use classical distance metrics, namely PSNR and SSIM scores: the former measures signal distortion w.r.t. the reference, the latter measures structural similarity which is known to be a strong cue for perceived quality [22]. First, one can note that our method is the best in terms of SSIM, at the same time producing images that are cleaner and sharper, thus perceptually performs the best. On PSNR terms, our method competes with the state of the art: it slightly improves or worsens depending on the dataset, i.e., on the actual phone used. Alignment issues could be responsible for these minor variations, and thus we consider Johnson et al.’s method [4] and ours equivalent here, while outperforming other methods. In Fig. 8 we show visual results comparing to the source photo(iPhone) and the target DSLR photo (Canon). More results are in the supplementary material.\n>我们首先定量比较APE，Dong等。 [4]，约翰逊等人。 [9]我们的方法是将照片从三个低端相机映射到高质量的DSLR（佳能）图像，并在表3中报告结果。因此，我们不评估全球图像质量，而是我们测量与参考（地面真相DSLR图像）的相似性。我们使用经典距离度量，即PSNR和SSIM得分：前者测量信号失真w.r.t.参考，后者测量结构相似性，已知它是感知质量的强有力线索[22]。首先，我们可以注意到，我们的方法在SSIM方面是最好的，同时产生更清晰和更清晰的图像，因此在感知上表现最佳。在PSNR方面，我们的方法与现有技术竞争：它根据数据集略微改善或恶化，即在所使用的实际手机上。对齐问题可能是这些微小变化的原因，因此我们在这里考虑约翰逊等人的方法[4]和我们的方法，同时优于其他方法。在图8中，我们显示了与源照片（iPhone）和目标DSLR照片（佳能）相比的视觉效果。更多结果在补充材料中。\n\n4.3 User study\nOur goal is to produce DSLR-quality images for the end user of smartphone cameras. To measure overall quality we designed a no-reference user study where subjects are repeatedly asked to choose the better looking picture out of a displayed pair.\n>我们的目标是为智能手机相机的最终用户生产DSLR质量的图像。 为了衡量整体质量，我们设计了一个无参考用户研究，其中反复要求受试者从所显示的对中选择更好看的图片。\n\n![](https://i.imgur.com/YUo08fd.png)\n>图9：BlackBerry和Sony相机捕获的原始（顶部）与增强（底部）图像的四个示例。\n\nUsers were instructed to ignore precise picture composition errors (e.g., field of view, perspective variation, etc.). There was no time limit given to the participants, images were shown in full resolution and the users were allowed to zoom in and out at will. In this setting, we did the following pairwise comparisons (every group of experiments contains 3 classes of pictures, the users were shown all possible pairwise combinations of these classes):\n(i) Comparison between:\n• original low-end phone photos,\n• DSLR photos,\n• photos enhanced by our proposed method.\nAt every question, the user is shown two pictures from different categories (original, DSLR or enhanced). 9 scenes were used for each phone (e.g., see Fig. 11). In total, there are 27 questions for every phone, thus 81 in total.\n(ii) Additionally, we compared (iPhone images only):\n• photos enhanced by the proposed method,\n• photos enhanced manually (by a professional),\n• photos enhanced by APE.\n>指示用户忽略精确的图像合成错误（例如，视野，透视变化等）。 参与者没有时间限制，图像以全分辨率显示，用户可以随意放大和缩小。 在此设置中，我们进行了以下成对比较（每组实验包含3类图片，用户显示了这些类的所有可能的成对组合）：\n（i）比较：\n•原创的低端手机照片，\n•DSLR照片，\n•我们提出的方法增强了照片。\n在每个问题上，向用户显示来自不同类别（原始，DSLR或增强）的两张图片。 每个电话使用9个场景（例如，参见图11）。 每部手机共有27个问题，共计81个问题。\n（ii）此外，我们进行了比较（仅限iPhone图像）：\n•通过提议的方法增强照片，\n•手动增强照片（由专业人士），\n•APE增强了照片。\n\nWe again considered 9 images that resulted in 27 binary selection questions. Thus, in total the study consists of 108 binary questions. All pairs are shuffled randomly for every subject, as is the sequence of displayed images. 42 subjects unaware of the goal of this work participated. They are mostly young scientists with a computer science background.\nFigure 10 shows results: for every experiment the first 3 bars show the results of the pairwise comparison averaged over the 9 images shown, while the last bar shows the fraction of cases when the selected method was chosen over all experiments.\nThe subfigures 10a-c show the results of enhancing photos from 3 different mobile devices. It can be seen that in all cases both pictures taken with a DSLR as well as pictures enhanced by the proposed CNN are picked much more often than the original ones taken with the mobile devices. When subjects are asked to select the better picture among the DSLR-picture and our enhanced picture, the choice is almost random (see the third bar in subfigures 10a-c). This means that the quality difference is inexistent or indistinguishable, and users resort to chance.\nSubfigure 10d shows user choices among our method, human artist work, and APE. Although human enhancement turns out to be slightly preferred to the automatic APE, the images enhanced by our method are picked more often, outperforming even manual retouching.\nWe can conclude that our results are of on pair quality compared to DSLR images, while starting from low quality phone cameras. The human subjects are unable to distinguish between them – the preferences are equally distributed.\n>我们再次考虑了9个图像，产生了27个二元选择问题。因此，该研究总共包含108个二元问题。对于每个主题，所有对都随机洗牌，显示图像的序列也是如此。 42名受试者不知道这项工作的目标参加了。他们大多是具有计算机科学背景的年轻科学家。\n图10显示了结果：对于每个实验，前3个柱显示了在所示9个图像上平均的成对比较的结果，而最后的柱显示了在所有实验中选择所选方法的情况的分数。\n子图10a-c示出了增强来自3个不同移动设备的照片的结果。可以看出，在所有情况下，用DSLR拍摄的图片以及由所提出的CNN增强的图片比用移动设备拍摄的原始照片更频繁地被拍摄。当要求受试者在DSLR图片和我们的增强图片中选择更好的图片时，选择几乎是随机的（参见子图10a-c中的第三栏）。这意味着质量差异不存在或难以区分，用户诉诸机会。\n子图10d示出了我们的方法，人类艺术家作品和APE中的用户选择。尽管人体增强效果稍微优于自动APE，但是通过我们的方法增强的图像被更频繁地挑选，甚至优于手动修饰。\n我们可以得出结论，我们的结果是与DSLR图像相比的对质量，而从低质量的手机相机开始。人类受试者无法区分它们 - 偏好是平均分配的。\n\n### 4.4 Limitations\nSince the proposed enhancement process is fully-automated, some flaws are inevitable. Two typical artifacts that can appear on the processed images are color deviations (see ground/mountains in first image of Fig. 12) and too high contrast levels (second image). Although they often cause rather plausible visual effects, in some situations this can lead to content changes that may look artificial, i.e. greenish asphalt in the second image of Fig. 12. Another notable problem is noise amplification – due to the nature of GANs, they can effectively restore high frequency-components. However, high-frequency noise is emphasized too. Fig. 12 (2nd and 3rd images) shows that a high noise in the original image is amplified in the enhanced image. Note that this noise issue occurs mostly on the lowest-quality photos (i.e., from the iPhone), not on the better phone cameras.\nFinally, the need of a strong supervision in the form of matched source/target training image pairs makes the process tedious to repeat for other cameras. To overcome this, we propose a weakly-supervised approach in [7] that does not require the mentioned correspondence.\n>由于提议的增强过程是完全自动化的，因此一些缺陷是不可避免的。可以出现在处理图像上的两个典型伪像是颜色偏差（参见图12的第一图像中的地面/山脉）和太高的对比度水平（第二图像）。虽然它们经常会产生相当合理的视觉效果，但在某些情况下，这可能导致内容变化，这可能看起来是人为的，即图12的第二张图像中的绿色沥青。另一个值得注意的问题是噪音放大 - 由于GAN的性质，它们可以有效地恢复高频成分。但是，也强调了高频噪声。图12（第2和第3图像）示出了在增强图像中放大原始图像中的高噪声。请注意，此噪音问题主要发生在质量最差的照片上（即来自iPhone），而不是更好的手机相机。\n最后，需要以匹配的源/目标训练图像对的形式进行强有力的监督，这使得该过程对于其他相机重复是繁琐的。为了克服这个问题，我们在[7]中提出了一种弱监督的方法，它不需要提到的对应关系。\n\n## 5 Conclusions\nWe proposed a photo enhancement solution to effectively transform cameras from common smartphones into high quality DSLR cameras. Our end-to-end deep learning approach uses a composite perceptual error function that combines content, color and texture losses. To train and evaluate our method we introduced DPED – a large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera, and suggested an efficient way of calibrating the images so that they are suitable for image-to-image learning. Our quantitative and qualitative assessments reveal that the enhanced images demonstrate a quality comparable to DSLRtaken photos, and the method itself can be applied to cameras of various quality levels.\n>我们提出了一种照片增强解决方案，可以将相机从普通智能手机有效转换为高品质的数码单反相 我们的端到端深度学习方法使用复合感知误差函数，该函数结合了内容，颜色和纹理损失。 为了训练和评估我们的方法，我们引入了DPED--一个大型数据集，包括从三个不同的手机和一个高端反光相机拍摄的真实照片，并提出了一种校准图像的有效方法，使它们适合于图像 -  图像学习。 我们的定量和定性评估显示，增强的图像显示出与DSLRtaken照片相当的质量，并且该方法本身可以应用于各种质量水平的相机。\n\nAcknowledgments. Work supported by the ETH Zurich General Fund (OK), Toyota via the project TRACE-Zurich, the ERC grant VarCity, and an NVidia GPU grant.\n\n## References\n[1] H. A. Aly and E. Dubois. Image up-sampling using totalvariation regularization with a new observation model.\nIEEE Transactions on Image Processing, 14(10):1647–\n1659, Oct 2005. 5\n[2] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao. Dehazenet:\nAn end-to-end system for single image haze removal.\nIEEE Transactions on Image Processing, 25(11):5187–\n5198, Nov 2016. 2\n[3] Z. Cheng, Q. Yang, and B. Sheng. Deep colorization. In\nThe IEEE International Conference on Computer Vision\n(ICCV), December 2015. 2\n[4] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a Deep\nConvolutional Network for Image Super-Resolution, pages\n184–199. Springer International Publishing, Cham, 2014.\n2, 5, 6\n[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.\nGenerative adversarial nets. In Z. Ghahramani, M. Welling,C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems\n27, pages 2672–2680. Curran Associates, Inc., 2014. 4\n[6] M. Hradis, J. Kotera, P. Zem ˇ cˇ´ık, and F. Sroubek. Con- ˇ\nvolutional neural networks for direct text deblurring. In\nProceedings of BMVC 2015. The British Machine Vision\nAssociation and Society for Pattern Recognition, 2015. 2\n[7] A. Ignatov, N. Kobyshev, R. Timofte, K. Vanhoey, and\nL. Van Gool. Wespe: Weakly supervised photo enhancer\nfor digital cameras. 2017. 7\n[8] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-toimage translation with conditional adversarial networks.\narxiv, 2016. 2\n[9] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual Losses\nfor Real-Time Style Transfer and Super-Resolution, pages\n694–711. Springer International Publishing, Cham, 2016.\n2, 4, 6\n[10] J. Kim, J. K. Lee, and K. M. Lee. Accurate image superresolution using very deep convolutional networks. In 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1646–1654, June 2016. 2\n[11] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. CoRR, abs/1412.6980, 2014. 5\n[12] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and\nW. Shi. Photo-realistic single image super-resolution using\na generative adversarial network. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), July\n2017. 2, 4\n[13] J.-Y. Lee, K. Sunkavalli, Z. Lin, X. Shen, and I. So Kweon.\nAutomatic content-aware color and tone stylization. In The\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 2\n[14] Z. Ling, G. Fan, Y. Wang, and X. Lu. Learning deep transmission network for single image dehazing. In 2016 IEEE\nInternational Conference on Image Processing (ICIP),\npages 2296–2300, Sept 2016. 2\n[15] D. G. Lowe. Distinctive image features from scaleinvariant keypoints. International Journal of Computer Vision, 60(2):91–110, 2004. 3\n[16] X. Mao, C. Shen, and Y.-B. Yang. Image restoration using\nvery deep convolutional encoder-decoder networks with\nsymmetric skip connections. In D. D. Lee, M. Sugiyama,\nU. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 29, pages 2802–\n2810. Curran Associates, Inc., 2016. 2\n[17] W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao, and M.-H. Yang.\nSingle Image Dehazing via Multi-scale Convolutional Neural Networks, pages 154–169. Springer International Publishing, Cham, 2016. 2\n[18] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken,\nR. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient subpixel convolutional neural network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nJune 2016. 2\n[19] P. Svoboda, M. Hradis, D. Barina, and P. Zemc´ık. Compression artifacts removal using convolutional neural networks. CoRR, abs/1605.00366, 2016. 2\n[20] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted Anchored Neighborhood Regression for Fast SuperResolution, pages 111–126. Springer International Publishing, Cham, 2015. 2\n[21] A. Vedaldi and B. Fulkerson. VLFeat: An open and\nportable library of computer vision algorithms, 2008. 3\n[22] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.\nImage quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing,\n13(4):600–612, April 2004. 6\n[23] Z. Yan, H. Zhang, B. Wang, S. Paris, and Y. Yu. Automatic\nphoto adjustment using deep neural networks. ACM Trans.\nGraph., 35(2):11:1–11:15, Feb. 2016. 2\n[24] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan.\nJoint rain detection and removal via iterative region dependent multi-task learning. CoRR, abs/1609.07769, 2016. 2\n[25] L. Yuan and J. Sun. Automatic Exposure Correction of\nConsumer Photographs, pages 771–785. Springer Berlin\nHeidelberg, Berlin, Heidelberg, 2012. 2\n[26] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a gaussian denoiser: Residual learning of deep CNN\nfor image denoising. IEEE Transactions on Image Processing, 2017. 2\n[27] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. ECCV, 2016. 2\n[28] X. Zhang and R. Wu. Fast depth image denoising and enhancement using a deep convolutional network. In 2016\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 2499–2503, March\n2016. 2\n[29] E. Zhou, H. Fan, Z. Cao, Y. Jiang, and Q. Yin. Learning face hallucination in the wild. In Proceedings of the\nTwenty-Ninth AAAI Conference on Artificial Intelligence,\nAAAI’15, pages 3871–3877. AAAI Press, 2015. 2\n\n','<p><img src=\"https://i.imgur.com/FirxWRj.png\" alt=\"\"></p>\n<h2 class=\"my-blog-head\" id=\"my-blog-head20\">Abstract</h2><p>Despite a rapid rise in the quality of built-in smartphone cameras, their physical limitations — small sensor size, compact lenses and the lack of specific hardware, — impede them to achieve the quality results of DSLR cameras. In this work we present an end-to-end deep learning approach that bridges this gap by translating ordinary photos into DSLR-quality images.We propose learning the translation function using a residual convolutional neural network that improves both color rendition and image sharpness. Since the standard mean squared loss is not well suited for measuring perceptual image quality, we introduce a composite perceptual error function that combines content, color and texture losses. The first two losses are defined analytically, while the texture loss is learned in an adversarial fashion. We also present DPED, a large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera. Our quantitative and qualitative assessments reveal that the enhanced image quality is comparable to that of DSLR-taken photos, while the methodology is generalized to any type of digital camera.</p>\n<blockquote>\n<p>尽管内置智能手机相机的质量迅速提升，但它们的物理限制 - 小尺寸传感器，小巧镜头和缺乏特定硬件 - 阻碍了它们实现数码单反相机的高质量效果。在这项工作中，我们提出了一种端到端的深度学习方法，通过将普通照片转换为DSLR质量的图像来弥补这一差距。我们建议使用残余卷积神经网络来学习翻译功能，从而改善色彩再现和图像清晰度。由于标准均方损失不适合测量感知图像质量，我们引入了一种复合感知误差函数，它结合了内容，颜色和纹理损失。前两个损失是分析定义的，而纹理损失是以对抗方式学习的。我们还展示了DPED，这是一个大型数据集，包括从三个不同手机和一个高端反光相机拍摄的真实照片。我们的定量和定性评估表明，增强的图像质量可与DSLR拍摄的照片相媲美，而该方法可推广到任何类型的数码相机。</p>\n</blockquote>\n<h2 class=\"my-blog-head\" id=\"my-blog-head21\">1 Introduction</h2><p>During the last several years there has been a significant improvement in compact camera sensors quality, which has brought mobile photography to a substantially new level. Even low-end devices are now able to take reasonably good photos in appropriate lighting conditions, thanks to their advanced software and hardware tools for post-processing. However, when it comes to artistic quality, mobile devices still fall behind their DSLR counterparts. Larger sensors and high-aperture optics yield better photo resolution, color rendition and less noise,whereas their additional sensors help to fine-tune shooting parameters. These physical differences result in strong obstacles, making DSLR camera quality unattainable for compact mobile devices.</p>\n<blockquote>\n<p>在过去几年中，紧凑型相机传感器质量得到了显着改善，这使得移动摄影技术达到了一个新的水平。 由于其先进的软件和硬件工具可用于后期处理，即使是低端设备现在也能够在适当的照明条件下拍摄相当不错的照片。 然而，在艺术品质方面，移动设备仍然落后于数码单反相机。 较大的传感器和高光圈光学系统可提供更好的照片分辨率，色彩还原和更低的噪点，而其附加传感器有助于微调拍摄参数。 这些物理差异导致了强大的障碍，使得紧凑型移动设备无法实现DSLR相机质量。</p>\n</blockquote>\n<p>While a number of photographer tools for automatic image enhancement exist, they are usually focused on adjusting only global parameters such as contrast or brightness, without improving texture quality or taking image semantics into account.Besides that, they are usually based on a pre-defined set of rules that do not always consider the specifics of a particular device. Therefore, the dominant approach to photo post-processing is still based on manual image correction using specialized retouching software.</p>\n<blockquote>\n<p>虽然存在许多用于自动图像增强的摄影师工具，但它们通常专注于仅调整对比度或亮度等全局参数，而不会改善纹理质量或考虑图像语义。此外，它们通常基于预定义 一组规则并不总是考虑特定设备的细节。 因此，照片后处理的主要方法仍然是基于使用专门的修饰软件的手动图像校正。</p>\n</blockquote>\n<h3 class=\"my-blog-head\" id=\"my-blog-head22\">1.1 Related work</h3><p>The problem of automatic image quality enhancement has not been addressed in its entirety in the area of computer vision,though a number of sub-tasks and related problems have been already successfully solved using deep learning techniques. Such tasks are usually dealing with image-to-image translation problems, and their common property is that they are targeted at removing artificially added artifacts to the original images. Among the related problems are the following:<br><strong>Image super-resolution</strong> aims at restoring the original image from its downscaled version. In [4] a CNN architecture and MSE loss are used for directly learning low to high resolution mapping. It is the first CNN-based solution to achieve top performance in single image super-resolution, comparable with non-CNN methods [20]. The subsequent works developed deeper and more complex CNN architectures (e.g., [10, 18,16]). Currently, the best photo-realistic results on this task are achieved using a VGG-based loss function [9] and adversarial networks [12] that turned out to be efficient at recovering plausible high-frequency components.<br><strong>Image deblurring/dehazing</strong> tries to remove artificially added haze or blur from the images. Usually, MSE is used as a target loss function and the proposed CNN architectures consist of 3 to 15 convolutional layers [14, 2, 6] or are bi-channel CNNs [17].<br><strong>Image denoising/sparse</strong> inpainting similarly targets removal of noise and artifacts from the pictures. In [28] the authors proposed weighted MSE together with a 3-layer CNN, while in [19]it was shown that an 8-layer residual CNN performs better when using a standard mean square error. Among other solutions are a bi-channel CNN [29], a 17-layer CNN [26] and a recurrent CNN [24] that was reapplied several times to the produced results.<br><strong>Image colorization</strong>. Here the goal is to recover colors that were removed from the original image. The baseline approach for this problem is to predict new values for each pixel based on its local description that consists of various hand-crafted features [3]. Considerably better performance on this task was obtained using generative adversarial networks [8] or a 16-layer CNN with a multinomial cross-entropy loss function [27].<br><strong>Image adjustment</strong>. A few works considered the problem of image color/contrast/exposure adjustment. In [25] the authors proposed an algorithm for automatic exposure correction using hand-designed features and predefined rules. In [23], a more general algorithm was proposed that – similarly to [3] – uses local description of image pixels for reproducing various photographic styles. A different approach was considered in [13], where images with similar content are retrieved from a database and their styles are applied to the target picture. All of these adjustments are implicitly included in our end-to-end transformation learning approach by design.</p>\n<blockquote>\n<p>虽然已经使用深度学习技术成功地解决了许多子任务和相关问题，但是在计算机视觉领域尚未完全解决自动图像质量增强的问题。这些任务通常涉及图像到图像的转换问题，它们的共同特性是它们的目标是将人为添加的伪像移除到原始图像。其中相关问题如下：<br><strong>图像超级分辨率</strong>旨在从缩小版本恢复原始图像。在[4]中，CNN架构和MSE损耗用于直接学习低分辨率到高分辨率的映射。这是第一个在单图像超分辨率下实现最佳性能的基于CNN的解决方案，与非CNN方法相当[20]。随后的工作开发了更深入和更复杂的CNN架构（例如，[10,18,16]）。目前，使用基于VGG的损失函数[9]和对抗性网络[12]实现了这项任务的最佳照片般逼真的结果，结果证明这种结果能够有效地恢复似乎合理的高频成分。<br><strong>图像去模糊/去雾</strong>试图从图像中去除人为添加的雾霾或模糊。通常，MSE用作目标损耗函数，并且所提出的CNN架构由3到15个卷积层[14,1,2]组成，或者是双通道CNN [17]。<br><strong>图像去噪/稀疏</strong>修复类似地用于从图片中去除噪声和伪影。在[28]中，作者提出了加权MSE和3层CNN，而在[19]中显示，当使用标准均方误差时，8层残余CNN表现更好。其他解决方案包括双通道CNN [29]，17层CNN [26]和复发CNN [24]，它们多次重新应用于产生的结果。<br><strong>图像着色</strong>。这里的目标是恢复从原始图像中删除的颜色。此问题的基线方法是根据由各种手工制作的特征组成的本地描述来预测每个像素的新值[3]。使用生成对抗网络[8]或具有多项交叉熵损失函数的16层CNN，可以获得相当好的性能[27]。<br><strong>图像调整</strong>。一些作品考虑了图像颜色/对比度/曝光调整的问题。在[25]中，作者提出了一种使用手工设计的特征和预定义规则进行自动曝光校正的算法。在[23]中，提出了一种更通用的算法 - 类似于[3]  - 使用图像像素的局部描述来再现各种摄影风格。在[13]中考虑了一种不同的方法，其中从数据库中检索具有相似内容的图像，并将它们的样式应用于目标图像。所有这些调整都隐含在我们的端到端转型学习方法中。</p>\n</blockquote>\n<h3 class=\"my-blog-head\" id=\"my-blog-head23\">1.2 Contributions</h3><p>The key challenge we face is dealing with all the aforementioned enhancements at once. Even advanced tools cannot notably improve image sharpness, texture details or small color variations that were lost by the camera sensor, thus we can not generate target enhanced photos from the existing ones. Corrupting DSLR photos and training an algorithm on the corrupted images does not work either: the solution would not generalize to real-world and very complex artifacts unless they are modeled and applied as corruptions, which is infeasible. To tackle this problem, we present a different approach: we propose to learn the transformation that modifies photos taken by a given camera to DSLR-quality ones. Thus, the goal is to learn a cross-distribution translation function, where the input distribution is defined by a given mobile camera sensor, and the target distribution by a DSLR sensor. To supervise the learning process, we create and leverage a dataset of images capturing the same scene with different cameras. Once the function is learned, it can be further applied to unseen photos at will.</p>\n<blockquote>\n<p>我们面临的主要挑战是立即处理上述所有增强功能。即使是先进的工具也无法显着改善相机传感器丢失的图像清晰度，纹理细节或小的颜色变化，因此我们无法从现有的照片生成目标增强照片。破坏DSLR照片并在损坏的图像上训练算法也不起作用：解决方案不会推广到真实世界和非常复杂的工件，除非它们被建模并应用为损坏，这是不可行的。为了解决这个问题，我们提出了一种不同的方法：我们建议学习将给定相机拍摄的照片修改为DSLR质量的转换。因此，目标是学习交叉分布转换函数，其中输入分布由给定的移动相机传感器定义，并且目标分布由DSLR传感器定义。为了监督学习过程，我们创建并利用使用不同相机捕获相同场景的图像数据集。一旦学习了该功能，它就可以进一步应用于看不见的照片。</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/LNYA4Zz.png\" alt=\"\"></p>\n<p>Our main contributions are:<br>• A novel approach1 for the photo enhancement task based on learning a mapping function between photos from mobile devices and a DSLR camera. The target model is trained in an end-to-end fashion without using any additional supervision or handcrafted features.<br>• A new large-scale dataset of over 6K photos taken synchronously by a DSLR camera and 3 low-end cameras of smartphones in a wide variety of conditions.<br>• A multi-term loss function composed of color, texture and content terms, allowing an efficient image quality estimation.<br>• Experiments measuring objective and subjective quality demonstrating the advantage of the enhanced photos over the originals and, at the same time, their comparable quality with the DSLR counterparts.</p>\n<blockquote>\n<p>我们的主要贡献是：<br>•基于学习移动设备和DSLR相机之间的映射功能的照片增强任务的新方法1。 目标模型以端到端的方式进行培训，无需使用任何额外的监督或手工制作的功能。<br>•由DSLR相机和智能手机的3个低端相机在各种条件下同步拍摄的6,000多张照片的新大型数据集。<br>•由颜色，纹理和内容术语组成的多项损失功能，可以进行有效的图像质量评估。<br>•测量客观和主观质量的实验证明了增强的照片相对于原件的优势，同时，它们与DSLR对应物的质量相当。</p>\n</blockquote>\n<p>The remainder of the paper is structured as follows. In Section 2 we describe the new DPED dataset. Section 3 presents our architecture and the chosen loss functions. Section 4 shows and analyzes the experimental results. Finally, Section 5 concludes the paper.</p>\n<blockquote>\n<p>在本文的其余结构如下。 在第2节中，我们描述了新的DPED数据集。 第3节介绍了我们的架构和所选的损失函数。 第4节显示并分析了实验结果。 最后，第5节总结了论文。</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/WC0hvvr.png\" alt=\"\"></p>\n<h2 class=\"my-blog-head\" id=\"my-blog-head24\">2 DSLR Photo Enhancement Dataset</h2><p>In order to tackle the problem of image translation from poor quality images captured by smartphone cameras to superior quality images achieved by a professional DSLR camera, we introduce a large-scale real-world dataset, namely the “DSLR Photo Enhancement Dataset” (DPED)2, that can be used for the general photo quality enhancement task. DPED consists of photos taken in the wild synchronously by three smartphones and one DSLR camera. The devices used to collect the data are described in Table 1 and example quadruplets can be seen in Figure 3.</p>\n<blockquote>\n<p>为了解决从智能手机相机拍摄的低质量图像到专业数码单反相机拍摄的高质量图像的图像转换问题，我们引入了大规模的真实数据集，即“DSLR照片增强数据集”（DPED） 2，可用于一般照片质量增强任务。 DPED由三部智能手机和一部数码单反相机同步拍摄的照片组成。 表1中描述了用于收集数据的设备，图3中显示了示例四联体。</p>\n</blockquote>\n<p>To ensure that all cameras were capturing photos simultaneously, the devices were mounted on a tripod and activated remotely by a wireless control system (see Figure 2). In total, over 22K photos were collected during 3 weeks, including 4549 photos from Sony smartphone, 5727 from iPhone and 6015 photos from each Canon and BlackBerry cameras. The photos were taken during the daytime in a wide variety of places and in various illumination and weather conditions. The photos were captured in automatic mode, and we used default settings for all cameras throughout the whole collection procedure.</p>\n<blockquote>\n<p>为了确保所有摄像机同时捕获照片，这些设备安装在三脚架上并通过无线控制系统远程激活（参见图2）。 总共在3周内收集了超过22,000张照片，其中包括索尼智能手机的4549张照片，iPhone的5727张照片以及佳能和黑莓相机的6015张照片。 这些照片是在白天在各种各样的地方以及各种照明和天气条件下拍摄的。 照片是在自动模式下拍摄的，我们在整个收集过程中使用了所有相机的默认设置。</p>\n</blockquote>\n<p><strong>Matching algorithm.</strong> The synchronously captured images are not perfectly aligned since the cameras have different viewing angles and positions as can be seen in Figure 3. To address this,we performed additional non-linear transformations resulting in a fixed-resolution image that our network takes as an input. The algorithm goes as follows (see Fig. 4). First, for each (phoneDSLR) image pair, we compute and match SIFT keypoints [15]across the images. These are used to estimate a homography using RANSAC [21]. We then crop both images to the intersection part and downscale the DSLR image crop to the size of the phone crop.</p>\n<blockquote>\n<p><strong>匹配算法</strong>.同步捕获的图像没有完美对齐，因为摄像机具有不同的视角和位置，如图3所示。为了解决这个问题，我们进行了额外的非线性变换，得到了固定分辨率的图像。 我们的网络作为输入。 算法如下（见图4）。 首先，对于每个（phoneDSLR）图像对，我们在图像上计算和匹配SIFT关键点[15]。 这些用于使用RANSAC估计单应性[21]。 然后，我们将两个图像裁剪到交叉点部分，并将DSLR图像裁剪缩小到手机裁剪的大小。</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/5JYXDrJ.png\" alt=\"\"></p>\n<blockquote>\n<p>图4：匹配算法：通过SIFT描述符匹配确定重叠区域，然后是非线性变换和裁剪，从而产生表示相同场景的相同分辨率的两个图像。</p>\n</blockquote>\n<p>Training CNN on the aligned high-resolution images is infeasible, thus patches of size 100×100px were extracted from these photos. Our preliminary experiments revealed that larger patch sizes do not lead to better performance, while requiring considerably more computational resources. We extracted patches using a non-overlapping sliding window. The window was moving in parallel along both images from each phone-DSLR image pair, and its position on the phone image was additionally adjusted by shifts and rotations based on the cross-correlation metrics. To avoid significant displacements, only patches with cross-correlation greater than 0.9 were included in the dataset. Around 100 original images were reserved for testing, the rest of the photos were used for training and validation. This procedure resulted in 139K, 160K and 162K training and 2.4-4.3K test patches for BlackBerry-Canon, iPhone-Canon and Sony-Canon pairs, respectively. It should be emphasized that both training and test patches are precisely matched, the potential shifts do not exceed 5 pixels. In the following we assume that these patches of size 3×100×100 constitute the input data to our CNNs.</p>\n<blockquote>\n<p>在对齐的高分辨率图像上训练CNN是不可行的，因此从这些照片中提取尺寸为100×100px的块。我们的初步实验表明，较大的补丁大小不会带来更好的性能，同时需要更多的计算资源。我们使用非重叠滑动窗口提取补丁。窗口沿着来自每个phone-DSLR图像对的两个图像并行移动，并且其在phone图像上的位置另外通过基于互相关度量的移位和旋转来调整。为避免显着位移，数据集中仅包含互相关大于0.9的补丁。大约100张原始图像被保留用于测试，其余照片用于训练和验证。此程序分别为BlackBerry-Canon，iPhone-Canon和Sony-Canon对提供了139K，160K和162K训练以及2.4-4.3K测试补丁。应该强调的是，训练和测试补丁都是精确匹配的，潜在的移位不超过5个像素。在下文中，我们假设这些大小为3×100×100的补丁构成了CNN的输入数据。</p>\n</blockquote>\n<h2 class=\"my-blog-head\" id=\"my-blog-head25\">3 Method</h2><p>Given a low-quality photo I<sub>s</sub> (source image), the goal of the considered enhancement task is to reproduce the image I<sub>t</sub> (target image) taken by a DSLR camera. A deep residual CNN FW parameterized by weights W is used to learn the underlying translation function. Given the training set {I<sup>j</sup><sub>s</sub>, I<sup>j</sup><sub>t</sub>}<sup>N</sup><sub>j=1</sub> consisting of N image pairs, it is trained to minimize:<br><img src=\"https://i.imgur.com/pEMUuTq.png\" alt=\"\"><br>where L denotes a multi-term loss function we detail in section 3.1. We then define the system architecture of our solution in Section 3.2</p>\n<blockquote>\n<p>给定低质量照片I<sub>s</sub>（源图像），所考虑的增强任务的目标是再现由DSLR相机拍摄的图像I <sub>t</sub>（目标图像）。 由权重W参数化的深度残余CNN FW用于学习基础翻译函数。 给定训练集{I<sup>j</sup><sub>s</sub>，I<sup>j</sup><sub>t</sub>}<sup>N</sup><sub>j=1</sub>由N个图像对组成，训练最小化：<br>其中L表示多项损失函数，我们在3.1节中详细说明。 然后，我们在3.2节中定义了我们解决方案的系统架构</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/6QX3SYC.png\" alt=\"\"></p>\n<blockquote>\n<p>图5：手机（最左边两个）和DSLR（最右边两个）摄像头拍摄的原始图像和模糊图像的碎片。 模糊消除了高频并使颜色比较更容易。</p>\n</blockquote>\n<h3 class=\"my-blog-head\" id=\"my-blog-head26\">3.1 Loss function</h3><p>The main difficulty of the image enhancement task is that input and target photos cannot be matched densely (i.e., pixelto-pixel): different optics and sensors cause specific local nonlinear distortions and aberrations, leading to a non-constant shift of pixels between each image pair even after precise alignment. Hence, the standard per-pixel losses, besides being doubtful as a perceptual quality metric, are not applicable in our case. We build our loss function under the assumption that the overall perceptual image quality can be decomposed into three independent parts: i) color quality, ii) texture quality and iii) content quality. We now define loss functions for each component, and ensure invariance to local shifts by design.</p>\n<blockquote>\n<p>图像增强任务的主要困难是输入和目标照片不能密集匹配（即，像素到像素）：不同的光学器件和传感器引起特定的局部非线性失真和像差，导致每个图像之间像素的非恒定偏移 精确对齐后即成对。 因此，除了作为感知质量度量标准之外，标准的每像素损失不适用于我们的情况。 我们在假设整体感知图像质量可以分解为三个独立部分的情况下构建我们的损失函数：i）颜色质量，ii）纹理质量和iii）内容质量。 我们现在为每个组件定义损失函数，并通过设计确保局部移位的不变性。</p>\n</blockquote>\n<h4 class=\"my-blog-head\" id=\"my-blog-head27\">3.1.1 Color loss</h4><p>To measure the color difference between the enhanced and target images, we propose applying a Gaussian blur (see Figure 5) and computing Euclidean distance between the obtained representations. In the context of CNNs, this is equivalent to using one additional convolutional layer with a fixed Gaussian kernel followed by the mean squared error (MSE) function. Color loss<br>can be written as:</p>\n<blockquote>\n<p>为了测量增强图像和目标图像之间的色差，我们建议应用高斯模糊（参见图5）并计算所获得的表示之间的欧几里德距离。 在CNN的上下文中，这相当于使用一个附加的卷积层和一个固定的高斯核，然后是均方误差（MSE）函数。 颜色损失<br>可以写成：</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/BoRYlzR.png\" alt=\"\"><br>where X<sub>b</sub> and Y<sub>b</sub> are the blurred images of X and Y , resp.:</p>\n<blockquote>\n<p>其中Xb和Yb是X和Y的模糊图像，分别为：</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/hDUD61z.png\" alt=\"\"><br>and the 2D Gaussian blur operator is given by</p>\n<blockquote>\n<p>和2D高斯模糊算子由下式给出</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/Lbug5rg.png\" alt=\"\"><br>where we defined A = 0.053, µ<sub>x,y</sub> = 0, and σ<sub>x,y</sub> = 3.</p>\n<blockquote>\n<p>我们定义A = 0.053，μx，y = 0和σx，y = 3。</p>\n</blockquote>\n<p>The idea behind this loss is to evaluate the difference in brightness, contrast and major colors between the images while eliminating texture and content comparison. Hence, we fixed a constant σ by visual inspection as the smallest value that ensures that texture and content are dropped. The crucial property of this loss is its invariance to small distortions. Figure 6 demonstrates the MSE and Color losses for image pairs (X, Y), where Y equals X shifted in a random direction by n pixels. As one can see, color loss is nearly insensitive to small distortions (6 2 pixels). For higher shifts (3-5px), it is still about 5-10 times smaller compared to the MSE, whereas for larger displacements it demonstrates similar magnitude and behavior. As a result, color loss forces the enhanced image to have the same color distribution as the target one, while being tolerant to small mismatches.</p>\n<blockquote>\n<p>这种损失背后的想法是评估图像之间的亮度，对比度和主要颜色的差异，同时消除纹理和内容比较。 因此，我们通过目视检查将常数σ固定为确保纹理和内容被丢弃的最小值。 这种损失的关键属性是它对小变形的不变性。 图6展示了图像对（X，Y）的MSE和颜色损失，其中Y等于X在随机方向上移位n个像素。 可以看出，颜色损失对小扭曲（6 2像素）几乎不敏感。 对于较高的位移（3-5px），它仍然比MSE小约5-10倍，而对于较大的位移，它表现出相似的幅度和行为。 结果，颜色损失迫使增强的图像具有与目标颜色分布相同的颜色分布，同时容忍小的不匹配。</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/o0QoHON.png\" alt=\"\"></p>\n<blockquote>\n<p>图6：MSE和颜色损失之间的比较，作为图像之间偏移幅度的函数。 结果平均超过50K图像。</p>\n</blockquote>\n<h4 class=\"my-blog-head\" id=\"my-blog-head28\">3.1.2 Texture loss</h4><p>Instead of using a pre-defined loss function, we build upon generative adversarial networks (GANs) [5] to directly learn a suitable metric for measuring texture quality. The discriminator CNN is applied to grayscale images so that it is targeted specifically on texture processing. It observes both fake (improved) and real (target) images, and its goal is to predict whether the input image is real or not. It is trained to minimize the cross-entropy loss function, and the texture loss is defined as a standard generator objective:</p>\n<blockquote>\n<p>我们不是使用预定义的损失函数，而是建立在生成对抗网络（GAN）[5]上，以直接学习用于测量纹理质量的合适度量。 鉴别器CNN应用于灰度图像，使得其特定地针对纹理处理。 它观察假（改进）和真实（目标）图像，其目标是预测输入图像是否真实。 训练最小化交叉熵损失函数，纹理损失定义为标准生成器目标：</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/jKLESwy.png\" alt=\"\"><br>where FW and D denote the generator and discriminator networks, respectively. The discriminator is pre-trained on the {phone, DSLR} image pairs, and then trained jointly with the proposed network as is conventional for GANs. It should be noted that this loss is shift-invariant by definition since no alignment is required in this case.</p>\n<blockquote>\n<p>其中FW和D分别表示发生器和鉴别器网络。 鉴别器在{phone，DSLR}图像对上进行预训练，然后与传统的GAN一起与所提出的网络一起训练。 应该注意的是，这种损失根据定义是移位不变的，因为在这种情况下不需要对准。</p>\n</blockquote>\n<h4 class=\"my-blog-head\" id=\"my-blog-head29\">3.1.3 Content loss</h4><p>Inspired by [9, 12], we define our content loss based on the activation maps produced by the ReLU layers of the pre-trained VGG-19 network. Instead of measuring per-pixel difference between the images, this loss encourages them to have similar feature representation that comprises various aspects of their content and perceptual quality. In our case it is used to preserve image semantics since other losses don’t consider it. Let ψ<sub>j</sub> () be the feature map obtained after the j-th convolutional layer of the VGG-19 CNN, then our content loss is defined as Euclidean distance between feature representations of the enhanced and target images:</p>\n<blockquote>\n<p>受[9,12]的启发，我们根据预训练的VGG-19网络的ReLU层产生的激活图来定义内容丢失。 这种损失不是测量图像之间的每像素差异，而是鼓励它们具有相似的特征表示，其包括其内容和感知质量的各个方面。 在我们的例子中，它用于保留图像语义，因为其他损失不考虑它。 设ψj（）是在VGG-19 CNN的第j个卷积层之后获得的特征图，然后我们的内容丢失被定义为增强图像和目标图像的特征表示之间的欧几里德距离：</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/LAcT1C6.png\" alt=\"\"><br>where C<sub>j</sub> , H<sub>j</sub> and W<sub>j</sub> denotes the number, height and width of the feature maps, and F<sub>W</sub>(I<sub>s</sub>) the enhanced image.</p>\n<blockquote>\n<p>其中Cj，Hj和Wj表示特征图的数量，高度和宽度，而FW（Is）表示增强的图像</p>\n</blockquote>\n<h4 class=\"my-blog-head\" id=\"my-blog-head30\">3.1.4 Total variation loss</h4><p>In addition to previous losses, we add total variation (TV) loss [1] to enforce spatial smoothness of the produced image:</p>\n<blockquote>\n<p>除了之前的损失，我们还增加了总变差（TV）损失[1]来强制生成图像的空间平滑度：</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/POG2ei7.png\" alt=\"\"><br>where C, H and W are the dimensions of the generated image F<sub>W</sub>(I<sub>s</sub>). As it is relatively lowly weighted (see Eqn. 8), it does not harm high-frequency components while it is quite effective at removing salt-and-pepper noise.</p>\n<blockquote>\n<p>其中C，H和W是生成图像的尺寸FW（Is）。 由于它的重量相对较低（参见方程式8），它不会损害高频成分，同时它在去除椒盐噪声方面非常有效。</p>\n</blockquote>\n<h4 class=\"my-blog-head\" id=\"my-blog-head31\">3.1.5 Total loss</h4><p>Our final loss is defined as a weighted sum of previous losses with the following coefficients:</p>\n<blockquote>\n<p>我们的最终损失定义为先前损失的加权和，具有以下系数：</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/BY6Bjs2.png\" alt=\"\"><br>where the content loss is based on the features produced by the relu 5 4 layer of the VGG-19 network. The coefficients were chosen based on preliminary experiments on the DPED training data.</p>\n<blockquote>\n<p>其中内容丢失基于VGG-19网络的relu 5 4层产生的特征。 基于DPED训练数据的初步实验选择系数。</p>\n</blockquote>\n<h3 class=\"my-blog-head\" id=\"my-blog-head32\">3.2 Generator and Discriminator CNNs</h3><p>Figure 7 illustrates the overall architecture of the proposed CNNs. Our image transformation network is fullyconvolutional, and starts with a 9×9 layer followed by four residual blocks. Each residual block consists of two 3×3 layers alternated with batch-normalization layers. We use two additional layers with kernels of size 3×3 and one with 9×9 kernels after the residual blocks. All layers in the transformation network have 64 channels and are followed by a ReLU activation function, except for the last one, where a scaled tanh is applied to the outputs.</p>\n<blockquote>\n<p>图7示出了所提出的CNN的总体架构。 我们的图像变换网络是完全卷积的，从9×9层开始，然后是4个残余块。 每个残余块由两个3×3层组成，与批量标准化层交替。 在剩余块之后，我们使用两个额外的层，其中内核大小为3×3，一个内核具有9×9内核。 转换网络中的所有层都有64个通道，后面跟着ReLU激活功能，除了最后一个，其中缩放的tanh应用于输出。</p>\n</blockquote>\n<p>The discriminator CNN consists of five convolutional layers each followed by a LeakyReLU nonlinearity and batch normalization. The first, second and fifth convolutional layers are strided with a step size of 4, 2 and 2, respectively. A sigmoidal activation function is applied to the outputs of the last fullyconnected layer containing 1024 neurons and produces a probability that the input image was taken by the target DSLR camera.</p>\n<blockquote>\n<p>鉴别器CNN由五个卷积层组成，每个卷层后面是LeakyReLU非线性和批量归一化。 第一，第二和第五卷积层分别以步长4,2和2跨步。 S形激活函数应用于包含1024个神经元的最后一个完全连接层的输出，并产生输入图像由目标DSLR相机拍摄的概率。</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/pyDNPBr.png\" alt=\"\"></p>\n<h3 class=\"my-blog-head\" id=\"my-blog-head33\">3.3 Training details</h3><p>The network was trained on a NVidia Titan X GPU for 20K iterations using a batch size of 50. The parameters of the network were optimized using Adam [11] modification of stochastic gradient descent with a learning rate of 5e-4. The whole pipelineand experimental setup was identical for all cameras.</p>\n<blockquote>\n<p>使用批量大小为50的网络在NVidia Titan X GPU上进行20K次迭代训练。使用Adam [11]修改随机梯度下降，学习率为5e-4，优化网络参数。 所有相机的整个管道和实验装置都是相同的。</p>\n</blockquote>\n<h2 class=\"my-blog-head\" id=\"my-blog-head34\">4 Experimen</h2><p>Our general goal to “improve image quality” is subjective and hard to evaluate quantitatively. We suggest a set of tools and methods from the literature that are most relevant to our problem. We use them, as well as our proposed method, on a set of test images taken by mobile devices and compare how close the results are to the DSRL shots.</p>\n<p>In section 4.1, we present the methods we compare to. Then we present both objective and subjective evaluations: the former w.r.t. the ground truth reference (i.e., the DSLR images) in section 4.2, the latter with no reference subjective quality scores in section 4.3. Finally, section 4.4 analyzes the limitations of the proposed solution.</p>\n<blockquote>\n<p>我们“提高图像质量”的总体目标是主观的，难以定量评估。 我们建议使用与我们的问题最相关的文献中的一套工具和方法。 我们在移动设备拍摄的一组测试图像上使用它们以及我们提出的方法，并比较结果与DSRL镜头的接近程度。<br>在4.1节中，我们介绍了我们比较的方法。 然后我们提出客观和主观评价：前者w.r.t. 4.2节中的地面实况参考（即DSLR图像），后者在4.3节中没有参考主观质量分数。 最后，第4.4节分析了所提出的解决方案的局限性。</p>\n</blockquote>\n<h3 class=\"my-blog-head\" id=\"my-blog-head35\">4.1 Benchmark methods</h3><p>In addition to our proposed photo enhancement solution, we compare with the following tools and methods.<br><strong>Apple Photo Enhancer (APE)</strong> is a commercial product known to generate among the best visual results, while the algorithm is unpublished. We trigger the method using the automatic Enhance function from the Photos app. It performs image improvement without taking any parameters.<br><strong>Dong et al.[4]</strong> is a fundamental baseline super-resolution method, thus addredding a task related to end-to-end image-toimage mapping. Hence we chose it to apply on our task and compare with. The method relies on a standard 3-layer CNN and MSE loss function and maps from low resolution / corrupted image to the restored image.<br><strong>Johnson et al. [9]</strong> is one of the latest state of the art in photo-realistic super-resolution and style transferring tasks. The method is based on a deep residual network (with four residual blocks, each consisting of two convolutional layers) that is trained to minimize a VGG-based loss function.<br><strong>Manual enhancement.</strong> We asked a graphical artist to enhance color, sharpness and general look-and-feel of 9 images using professional software (Adobe Photoshop CS6). A time limit of one workday was given, so as to simulate a realistic scenario. Figure 8 illustrates the ensemble of enhancement methods we consider for comparison in our experiments. Dong et al. [4]and Johnson et al. [9] are trained using the same train image pairs as for our solution for each of the smartphones from the DPED dataset.</p>\n<blockquote>\n<p>除了我们提出的照片增强解决方案，我们还与以下工具和方法进行了比较。<br>Apple Photo Enhancer（APE）是一种商业产品，可以产生最佳的视觉效果，而该算法尚未发布。我们使用Photos应用程序中的自动增强功能触发该方法。它无需任何参数即可执行图像改进。<br>董等人[4]是一种基本的基线超分辨率方法，因此增加了与端到端图像到图像映射相关的任务。因此我们选择它来应用我们的任务并与之进行比较。该方法依赖于标准的3层CNN和MSE丢失功能，并从低分辨率/损坏的图像映射到恢复的图像。<br>约翰逊等人。 [9]是照片般逼真的超分辨率和风格转换任务的最新技术之一。该方法基于深度残余网络（具有四个残余块，每个残余块由两个卷积层组成），其被训练以最小化基于VGG的损失函数。<br>手动增强。我们要求图形艺术家使用专业软件（Adobe Photoshop CS6）增强9幅图像的色彩，清晰度和一般外观。给出了一个工作日的时间限制，以模拟现实场景。图8说明了我们在实验中考虑进行比较的增强方法集合。董等人。 [4]和约翰逊等人。 [9]使用与DPED数据集中每个智能手机的解决方案相同的列车图像对进行训练。</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/geOffNY.jpg\" alt=\"\"></p>\n<h3 class=\"my-blog-head\" id=\"my-blog-head36\">4.2 Quantitative evaluation</h3><p>We first quantitatively compare APE, Dong et al. [4], Johnson et al. [9] and our method on the task of mapping photos from three low-end cameras to the high-quality DSLR (Canon) images and report the results in Table 3. As such, we do not evaluate global image quality but, rather, we measure resemblance to a reference(the ground truth DSLR image). We use classical distance metrics, namely PSNR and SSIM scores: the former measures signal distortion w.r.t. the reference, the latter measures structural similarity which is known to be a strong cue for perceived quality [22]. First, one can note that our method is the best in terms of SSIM, at the same time producing images that are cleaner and sharper, thus perceptually performs the best. On PSNR terms, our method competes with the state of the art: it slightly improves or worsens depending on the dataset, i.e., on the actual phone used. Alignment issues could be responsible for these minor variations, and thus we consider Johnson et al.’s method [4] and ours equivalent here, while outperforming other methods. In Fig. 8 we show visual results comparing to the source photo(iPhone) and the target DSLR photo (Canon). More results are in the supplementary material.</p>\n<blockquote>\n<p>我们首先定量比较APE，Dong等。 [4]，约翰逊等人。 [9]我们的方法是将照片从三个低端相机映射到高质量的DSLR（佳能）图像，并在表3中报告结果。因此，我们不评估全球图像质量，而是我们测量与参考（地面真相DSLR图像）的相似性。我们使用经典距离度量，即PSNR和SSIM得分：前者测量信号失真w.r.t.参考，后者测量结构相似性，已知它是感知质量的强有力线索[22]。首先，我们可以注意到，我们的方法在SSIM方面是最好的，同时产生更清晰和更清晰的图像，因此在感知上表现最佳。在PSNR方面，我们的方法与现有技术竞争：它根据数据集略微改善或恶化，即在所使用的实际手机上。对齐问题可能是这些微小变化的原因，因此我们在这里考虑约翰逊等人的方法[4]和我们的方法，同时优于其他方法。在图8中，我们显示了与源照片（iPhone）和目标DSLR照片（佳能）相比的视觉效果。更多结果在补充材料中。</p>\n</blockquote>\n<p>4.3 User study<br>Our goal is to produce DSLR-quality images for the end user of smartphone cameras. To measure overall quality we designed a no-reference user study where subjects are repeatedly asked to choose the better looking picture out of a displayed pair.</p>\n<blockquote>\n<p>我们的目标是为智能手机相机的最终用户生产DSLR质量的图像。 为了衡量整体质量，我们设计了一个无参考用户研究，其中反复要求受试者从所显示的对中选择更好看的图片。</p>\n</blockquote>\n<p><img src=\"https://i.imgur.com/YUo08fd.png\" alt=\"\"></p>\n<blockquote>\n<p>图9：BlackBerry和Sony相机捕获的原始（顶部）与增强（底部）图像的四个示例。</p>\n</blockquote>\n<p>Users were instructed to ignore precise picture composition errors (e.g., field of view, perspective variation, etc.). There was no time limit given to the participants, images were shown in full resolution and the users were allowed to zoom in and out at will. In this setting, we did the following pairwise comparisons (every group of experiments contains 3 classes of pictures, the users were shown all possible pairwise combinations of these classes):<br>(i) Comparison between:<br>• original low-end phone photos,<br>• DSLR photos,<br>• photos enhanced by our proposed method.<br>At every question, the user is shown two pictures from different categories (original, DSLR or enhanced). 9 scenes were used for each phone (e.g., see Fig. 11). In total, there are 27 questions for every phone, thus 81 in total.<br>(ii) Additionally, we compared (iPhone images only):<br>• photos enhanced by the proposed method,<br>• photos enhanced manually (by a professional),<br>• photos enhanced by APE.</p>\n<blockquote>\n<p>指示用户忽略精确的图像合成错误（例如，视野，透视变化等）。 参与者没有时间限制，图像以全分辨率显示，用户可以随意放大和缩小。 在此设置中，我们进行了以下成对比较（每组实验包含3类图片，用户显示了这些类的所有可能的成对组合）：<br>（i）比较：<br>•原创的低端手机照片，<br>•DSLR照片，<br>•我们提出的方法增强了照片。<br>在每个问题上，向用户显示来自不同类别（原始，DSLR或增强）的两张图片。 每个电话使用9个场景（例如，参见图11）。 每部手机共有27个问题，共计81个问题。<br>（ii）此外，我们进行了比较（仅限iPhone图像）：<br>•通过提议的方法增强照片，<br>•手动增强照片（由专业人士），<br>•APE增强了照片。</p>\n</blockquote>\n<p>We again considered 9 images that resulted in 27 binary selection questions. Thus, in total the study consists of 108 binary questions. All pairs are shuffled randomly for every subject, as is the sequence of displayed images. 42 subjects unaware of the goal of this work participated. They are mostly young scientists with a computer science background.<br>Figure 10 shows results: for every experiment the first 3 bars show the results of the pairwise comparison averaged over the 9 images shown, while the last bar shows the fraction of cases when the selected method was chosen over all experiments.<br>The subfigures 10a-c show the results of enhancing photos from 3 different mobile devices. It can be seen that in all cases both pictures taken with a DSLR as well as pictures enhanced by the proposed CNN are picked much more often than the original ones taken with the mobile devices. When subjects are asked to select the better picture among the DSLR-picture and our enhanced picture, the choice is almost random (see the third bar in subfigures 10a-c). This means that the quality difference is inexistent or indistinguishable, and users resort to chance.<br>Subfigure 10d shows user choices among our method, human artist work, and APE. Although human enhancement turns out to be slightly preferred to the automatic APE, the images enhanced by our method are picked more often, outperforming even manual retouching.<br>We can conclude that our results are of on pair quality compared to DSLR images, while starting from low quality phone cameras. The human subjects are unable to distinguish between them – the preferences are equally distributed.</p>\n<blockquote>\n<p>我们再次考虑了9个图像，产生了27个二元选择问题。因此，该研究总共包含108个二元问题。对于每个主题，所有对都随机洗牌，显示图像的序列也是如此。 42名受试者不知道这项工作的目标参加了。他们大多是具有计算机科学背景的年轻科学家。<br>图10显示了结果：对于每个实验，前3个柱显示了在所示9个图像上平均的成对比较的结果，而最后的柱显示了在所有实验中选择所选方法的情况的分数。<br>子图10a-c示出了增强来自3个不同移动设备的照片的结果。可以看出，在所有情况下，用DSLR拍摄的图片以及由所提出的CNN增强的图片比用移动设备拍摄的原始照片更频繁地被拍摄。当要求受试者在DSLR图片和我们的增强图片中选择更好的图片时，选择几乎是随机的（参见子图10a-c中的第三栏）。这意味着质量差异不存在或难以区分，用户诉诸机会。<br>子图10d示出了我们的方法，人类艺术家作品和APE中的用户选择。尽管人体增强效果稍微优于自动APE，但是通过我们的方法增强的图像被更频繁地挑选，甚至优于手动修饰。<br>我们可以得出结论，我们的结果是与DSLR图像相比的对质量，而从低质量的手机相机开始。人类受试者无法区分它们 - 偏好是平均分配的。</p>\n</blockquote>\n<h3 class=\"my-blog-head\" id=\"my-blog-head37\">4.4 Limitations</h3><p>Since the proposed enhancement process is fully-automated, some flaws are inevitable. Two typical artifacts that can appear on the processed images are color deviations (see ground/mountains in first image of Fig. 12) and too high contrast levels (second image). Although they often cause rather plausible visual effects, in some situations this can lead to content changes that may look artificial, i.e. greenish asphalt in the second image of Fig. 12. Another notable problem is noise amplification – due to the nature of GANs, they can effectively restore high frequency-components. However, high-frequency noise is emphasized too. Fig. 12 (2nd and 3rd images) shows that a high noise in the original image is amplified in the enhanced image. Note that this noise issue occurs mostly on the lowest-quality photos (i.e., from the iPhone), not on the better phone cameras.<br>Finally, the need of a strong supervision in the form of matched source/target training image pairs makes the process tedious to repeat for other cameras. To overcome this, we propose a weakly-supervised approach in [7] that does not require the mentioned correspondence.</p>\n<blockquote>\n<p>由于提议的增强过程是完全自动化的，因此一些缺陷是不可避免的。可以出现在处理图像上的两个典型伪像是颜色偏差（参见图12的第一图像中的地面/山脉）和太高的对比度水平（第二图像）。虽然它们经常会产生相当合理的视觉效果，但在某些情况下，这可能导致内容变化，这可能看起来是人为的，即图12的第二张图像中的绿色沥青。另一个值得注意的问题是噪音放大 - 由于GAN的性质，它们可以有效地恢复高频成分。但是，也强调了高频噪声。图12（第2和第3图像）示出了在增强图像中放大原始图像中的高噪声。请注意，此噪音问题主要发生在质量最差的照片上（即来自iPhone），而不是更好的手机相机。<br>最后，需要以匹配的源/目标训练图像对的形式进行强有力的监督，这使得该过程对于其他相机重复是繁琐的。为了克服这个问题，我们在[7]中提出了一种弱监督的方法，它不需要提到的对应关系。</p>\n</blockquote>\n<h2 class=\"my-blog-head\" id=\"my-blog-head38\">5 Conclusions</h2><p>We proposed a photo enhancement solution to effectively transform cameras from common smartphones into high quality DSLR cameras. Our end-to-end deep learning approach uses a composite perceptual error function that combines content, color and texture losses. To train and evaluate our method we introduced DPED – a large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera, and suggested an efficient way of calibrating the images so that they are suitable for image-to-image learning. Our quantitative and qualitative assessments reveal that the enhanced images demonstrate a quality comparable to DSLRtaken photos, and the method itself can be applied to cameras of various quality levels.</p>\n<blockquote>\n<p>我们提出了一种照片增强解决方案，可以将相机从普通智能手机有效转换为高品质的数码单反相 我们的端到端深度学习方法使用复合感知误差函数，该函数结合了内容，颜色和纹理损失。 为了训练和评估我们的方法，我们引入了DPED–一个大型数据集，包括从三个不同的手机和一个高端反光相机拍摄的真实照片，并提出了一种校准图像的有效方法，使它们适合于图像 -  图像学习。 我们的定量和定性评估显示，增强的图像显示出与DSLRtaken照片相当的质量，并且该方法本身可以应用于各种质量水平的相机。</p>\n</blockquote>\n<p>Acknowledgments. Work supported by the ETH Zurich General Fund (OK), Toyota via the project TRACE-Zurich, the ERC grant VarCity, and an NVidia GPU grant.</p>\n<h2 class=\"my-blog-head\" id=\"my-blog-head39\">References</h2><p>[1] H. A. Aly and E. Dubois. Image up-sampling using totalvariation regularization with a new observation model.<br>IEEE Transactions on Image Processing, 14(10):1647–<br>1659, Oct 2005. 5<br>[2] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao. Dehazenet:<br>An end-to-end system for single image haze removal.<br>IEEE Transactions on Image Processing, 25(11):5187–<br>5198, Nov 2016. 2<br>[3] Z. Cheng, Q. Yang, and B. Sheng. Deep colorization. In<br>The IEEE International Conference on Computer Vision<br>(ICCV), December 2015. 2<br>[4] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a Deep<br>Convolutional Network for Image Super-Resolution, pages<br>184–199. Springer International Publishing, Cham, 2014.<br>2, 5, 6<br>[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,<br>D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.<br>Generative adversarial nets. In Z. Ghahramani, M. Welling,C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems<br>27, pages 2672–2680. Curran Associates, Inc., 2014. 4<br>[6] M. Hradis, J. Kotera, P. Zem ˇ cˇ´ık, and F. Sroubek. Con- ˇ<br>volutional neural networks for direct text deblurring. In<br>Proceedings of BMVC 2015. The British Machine Vision<br>Association and Society for Pattern Recognition, 2015. 2<br>[7] A. Ignatov, N. Kobyshev, R. Timofte, K. Vanhoey, and<br>L. Van Gool. Wespe: Weakly supervised photo enhancer<br>for digital cameras. 2017. 7<br>[8] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-toimage translation with conditional adversarial networks.<br>arxiv, 2016. 2<br>[9] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual Losses<br>for Real-Time Style Transfer and Super-Resolution, pages<br>694–711. Springer International Publishing, Cham, 2016.<br>2, 4, 6<br>[10] J. Kim, J. K. Lee, and K. M. Lee. Accurate image superresolution using very deep convolutional networks. In 2016<br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1646–1654, June 2016. 2<br>[11] D. P. Kingma and J. Ba. Adam: A method for stochastic<br>optimization. CoRR, abs/1412.6980, 2014. 5<br>[12] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and<br>W. Shi. Photo-realistic single image super-resolution using<br>a generative adversarial network. In The IEEE Conference<br>on Computer Vision and Pattern Recognition (CVPR), July</p>\n<ol start=\"2017\">\n<li>2, 4<br>[13] J.-Y. Lee, K. Sunkavalli, Z. Lin, X. Shen, and I. So Kweon.<br>Automatic content-aware color and tone stylization. In The<br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 2<br>[14] Z. Ling, G. Fan, Y. Wang, and X. Lu. Learning deep transmission network for single image dehazing. In 2016 IEEE<br>International Conference on Image Processing (ICIP),<br>pages 2296–2300, Sept 2016. 2<br>[15] D. G. Lowe. Distinctive image features from scaleinvariant keypoints. International Journal of Computer Vision, 60(2):91–110, 2004. 3<br>[16] X. Mao, C. Shen, and Y.-B. Yang. Image restoration using<br>very deep convolutional encoder-decoder networks with<br>symmetric skip connections. In D. D. Lee, M. Sugiyama,<br>U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances<br>in Neural Information Processing Systems 29, pages 2802–</li>\n<li>Curran Associates, Inc., 2016. 2<br>[17] W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao, and M.-H. Yang.<br>Single Image Dehazing via Multi-scale Convolutional Neural Networks, pages 154–169. Springer International Publishing, Cham, 2016. 2<br>[18] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken,<br>R. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient subpixel convolutional neural network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),<br>June 2016. 2<br>[19] P. Svoboda, M. Hradis, D. Barina, and P. Zemc´ık. Compression artifacts removal using convolutional neural networks. CoRR, abs/1605.00366, 2016. 2<br>[20] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted Anchored Neighborhood Regression for Fast SuperResolution, pages 111–126. Springer International Publishing, Cham, 2015. 2<br>[21] A. Vedaldi and B. Fulkerson. VLFeat: An open and<br>portable library of computer vision algorithms, 2008. 3<br>[22] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.<br>Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing,<br>13(4):600–612, April 2004. 6<br>[23] Z. Yan, H. Zhang, B. Wang, S. Paris, and Y. Yu. Automatic<br>photo adjustment using deep neural networks. ACM Trans.<br>Graph., 35(2):11:1–11:15, Feb. 2016. 2<br>[24] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan.<br>Joint rain detection and removal via iterative region dependent multi-task learning. CoRR, abs/1609.07769, 2016. 2<br>[25] L. Yuan and J. Sun. Automatic Exposure Correction of<br>Consumer Photographs, pages 771–785. Springer Berlin<br>Heidelberg, Berlin, Heidelberg, 2012. 2<br>[26] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a gaussian denoiser: Residual learning of deep CNN<br>for image denoising. IEEE Transactions on Image Processing, 2017. 2<br>[27] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. ECCV, 2016. 2<br>[28] X. Zhang and R. Wu. Fast depth image denoising and enhancement using a deep convolutional network. In 2016<br>IEEE International Conference on Acoustics, Speech and<br>Signal Processing (ICASSP), pages 2499–2503, March</li>\n<li>2<br>[29] E. Zhou, H. Fan, Z. Cao, Y. Jiang, and Q. Yin. Learning face hallucination in the wild. In Proceedings of the<br>Twenty-Ninth AAAI Conference on Artificial Intelligence,<br>AAAI’15, pages 3871–3877. AAAI Press, 2015. 2</li>\n</ol>\n','http://www.yuhaochen.top:8888/imgs/2019/02/fd3ba1c07e607be1.jpg','DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks',7,_binary '\0'),(4,'6D3i0FJnnvTtKPJJ8ByTlm','Linux、Spark、IDEA集成开发环境配置(单机版)','61m7ZT6t49NNWlcDX9AAtQ',1550837242,NULL,1550837353,1550837353,0,'## 环境\n1、Linux(ubuntu、mint等)\n2、IDEA\n3、jdk-1.8\n4、Scala-2.10.4\n5、Spark-1.6.3\n注意:Scala和Spark版本之间具有依赖关系,其他版本可以去尝试\n\n\n## 1、安装jdk\n1.从http://www.oracle.com/technetwork/java/javase/downloads页面下载JDK 1.8安装包，此处选择的是jdk-8u73-linux-x64.tar.gz\n2.解压到软件希望安装的目录下\n3.修改环境变量： sudo gedit /etc/profile\n```shell=\nexport JAVA_HOME=/usr/local/java/jdk1.8.0_73\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\nexport PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin:$PATH\n```\n4.重新载入profile文件\n```shell=\nsource /etc/profile\n```\n5.测试\n```shell=\njava -version\n```\n\n## 2、安装Scala\n1.从http://www.scala-lang.org/download/2.10.4.html页面下载scala-2.10.4安装包\n![](https://i.imgur.com/t0o8658.png)\n2.解压到软件希望安装的目录下\n3.修改环境变量: sudo gedit /etc/profile\n```shell=\nexport SCALA_HOME=/usr/local/scala/scala-2.10.4\nexport PATH=${SCALA_HOME}/bin:$PATH\n```\n4.重新载入profile文件：\n```shell=\nsource /etc/profile\n```\n5.测试：\n```shell=\nscala -version\n```\n\n## 3、安装Spark\n1.从http://spark.apache.org/downloads.html页面下载spark安装包，这里我选择的是1.6.3版本的Pre-build for Hadoop2.6 and later.\n![](https://i.imgur.com/NbX5vTj.png)\n2.解压到软件希望安装的目录下\n3.修改环境变量: sudo gedit /etc/profile\n```shell=\nexport SPARK_HOME=/usr/local/spark/spark-1.6.3-bin-hadoop2.6\nexport PATH=${SPARK_HOME}/bin:$PATH\n```\n4.重新载入profile文件\n```shell=\nsource /etc/profile\n```\n\n## 4、安装IDEA\n1.从http://www.jetbrains.com/idea/download/#section=linux页面下载IntelliJ IDEA安装包\n2.解压到软件希望安装的目录下\n3.下载插件\n首先启动intelliJ IDEA:在命令行终端中，进入$IDEA_HOME/bin目录，输入sudo ./idea.sh进行启动,进入如下界面，然后选择右下角“plugins”\n![](https://i.imgur.com/xNPThw8.png)\n![](https://i.imgur.com/ibvcBYv.png)\n安装插件后，在启动界面中选择创建新项目，弹出的界面中将会出现\"Scala\"类型项目，如下图\n![](https://i.imgur.com/RP6jI5J.png)\n点击next，project name自己随便起的名字，把自己安装的scala和jdk选中，按照上面的安装过程，此处选的为jdk 1.8.0_73， scala-2.10.4！完成后，点击Finish\n![](https://i.imgur.com/QgsLJUU.png)\n然后在IDE中File -> project Structure -> Libraries ->“+”，然后进入你安装spark时候解压的 spark-XXX-bin-hadoopXX下，在lib目录下，选择spark-assembly-XXX-hadoopXX.jar,结果如下图所示，然后点击Apply，最后点击ok\n![](https://i.imgur.com/IhMHDB1.png)\n\n现在我们就可以在src下创建一个包，然后创建一个Scala Object，如下图，然后就可以用scala来编写代码了。\n![](https://i.imgur.com/vNhWh1e.png)\n\n## 5、测试集成环境\n下面是统计单词的小程序\n```scala=\npackage graphTest\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n  * Created by xxx\n  */\nobject myFirstScalaObject {\n  def main(args: Array[String]) {\n    val conf = new SparkConf()\n    conf.setAppName(\"world\")\n    conf.setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val lines = sc.textFile(\"word\")   //数据路径\n    val words = lines.flatMap{line => line.split(\" \")}\n    val pairs = words.map{ word => (word,1)}\n    val wordCounts = pairs.reduceByKey(_+_)\n    wordCounts.foreach(wordNumberPair => println(wordNumberPair._1 + \":\" + wordNumberPair._2))\n    sc.stop()\n  }\n}\n```\n\n然后点击Run即可运行了。\n\n此处运行时如果出现如下的报错信息，则表明Spark版本和Scala版本不兼容，需要更改scala的版本。但是在本文介绍的scala-2.10.4版本与spark 1.6.2版本是兼容的。\n```\nException in thread \"main\" java.lang.NoSuchMethodError: scala.collection.immutable.HashSet$.empty()Lscala/collection/immutable/HashSet;\n```\n![](https://i.imgur.com/f2wPsAt.png)\n\n\n\n\n\n\n\n\n','<h2 class=\"my-blog-head\" id=\"my-blog-head18\">环境</h2><p>1、Linux(ubuntu、mint等)<br>2、IDEA<br>3、jdk-1.8<br>4、Scala-2.10.4<br>5、Spark-1.6.3<br>注意:Scala和Spark版本之间具有依赖关系,其他版本可以去尝试</p>\n<h2 class=\"my-blog-head\" id=\"my-blog-head19\">1、安装jdk</h2><p>1.从<a href=\"http://www.oracle.com/technetwork/java/javase/downloads%E9%A1%B5%E9%9D%A2%E4%B8%8B%E8%BD%BDJDK\">http://www.oracle.com/technetwork/java/javase/downloads页面下载JDK</a> 1.8安装包，此处选择的是jdk-8u73-linux-x64.tar.gz<br>2.解压到软件希望安装的目录下<br>3.修改环境变量： sudo gedit /etc/profile</p>\n<pre><code class=\"language-shell=\">export JAVA_HOME=/usr/local/java/jdk1.8.0_73\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\nexport PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin:$PATH</code></pre>\n<p>4.重新载入profile文件</p>\n<pre><code class=\"language-shell=\">source /etc/profile</code></pre>\n<p>5.测试</p>\n<pre><code class=\"language-shell=\">java -version</code></pre>\n<h2 class=\"my-blog-head\" id=\"my-blog-head20\">2、安装Scala</h2><p>1.从<a href=\"http://www.scala-lang.org/download/2.10.4.html%E9%A1%B5%E9%9D%A2%E4%B8%8B%E8%BD%BDscala-2.10.4%E5%AE%89%E8%A3%85%E5%8C%85\">http://www.scala-lang.org/download/2.10.4.html页面下载scala-2.10.4安装包</a><br><img src=\"https://i.imgur.com/t0o8658.png\" alt=\"\"><br>2.解压到软件希望安装的目录下<br>3.修改环境变量: sudo gedit /etc/profile</p>\n<pre><code class=\"language-shell=\">export SCALA_HOME=/usr/local/scala/scala-2.10.4\nexport PATH=${SCALA_HOME}/bin:$PATH</code></pre>\n<p>4.重新载入profile文件：</p>\n<pre><code class=\"language-shell=\">source /etc/profile</code></pre>\n<p>5.测试：</p>\n<pre><code class=\"language-shell=\">scala -version</code></pre>\n<h2 class=\"my-blog-head\" id=\"my-blog-head21\">3、安装Spark</h2><p>1.从<a href=\"http://spark.apache.org/downloads.html%E9%A1%B5%E9%9D%A2%E4%B8%8B%E8%BD%BDspark%E5%AE%89%E8%A3%85%E5%8C%85%EF%BC%8C%E8%BF%99%E9%87%8C%E6%88%91%E9%80%89%E6%8B%A9%E7%9A%84%E6%98%AF1.6.3%E7%89%88%E6%9C%AC%E7%9A%84Pre-build\">http://spark.apache.org/downloads.html页面下载spark安装包，这里我选择的是1.6.3版本的Pre-build</a> for Hadoop2.6 and later.<br><img src=\"https://i.imgur.com/NbX5vTj.png\" alt=\"\"><br>2.解压到软件希望安装的目录下<br>3.修改环境变量: sudo gedit /etc/profile</p>\n<pre><code class=\"language-shell=\">export SPARK_HOME=/usr/local/spark/spark-1.6.3-bin-hadoop2.6\nexport PATH=${SPARK_HOME}/bin:$PATH</code></pre>\n<p>4.重新载入profile文件</p>\n<pre><code class=\"language-shell=\">source /etc/profile</code></pre>\n<h2 class=\"my-blog-head\" id=\"my-blog-head22\">4、安装IDEA</h2><p>1.从<a href=\"http://www.jetbrains.com/idea/download/#section=linux%E9%A1%B5%E9%9D%A2%E4%B8%8B%E8%BD%BDIntelliJ\">http://www.jetbrains.com/idea/download/#section=linux页面下载IntelliJ</a> IDEA安装包<br>2.解压到软件希望安装的目录下<br>3.下载插件<br>首先启动intelliJ IDEA:在命令行终端中，进入$IDEA_HOME/bin目录，输入sudo ./idea.sh进行启动,进入如下界面，然后选择右下角“plugins”<br><img src=\"https://i.imgur.com/xNPThw8.png\" alt=\"\"><br><img src=\"https://i.imgur.com/ibvcBYv.png\" alt=\"\"><br>安装插件后，在启动界面中选择创建新项目，弹出的界面中将会出现”Scala”类型项目，如下图<br><img src=\"https://i.imgur.com/RP6jI5J.png\" alt=\"\"><br>点击next，project name自己随便起的名字，把自己安装的scala和jdk选中，按照上面的安装过程，此处选的为jdk 1.8.0_73， scala-2.10.4！完成后，点击Finish<br><img src=\"https://i.imgur.com/QgsLJUU.png\" alt=\"\"><br>然后在IDE中File -&gt; project Structure -&gt; Libraries -&gt;“+”，然后进入你安装spark时候解压的 spark-XXX-bin-hadoopXX下，在lib目录下，选择spark-assembly-XXX-hadoopXX.jar,结果如下图所示，然后点击Apply，最后点击ok<br><img src=\"https://i.imgur.com/IhMHDB1.png\" alt=\"\"></p>\n<p>现在我们就可以在src下创建一个包，然后创建一个Scala Object，如下图，然后就可以用scala来编写代码了。<br><img src=\"https://i.imgur.com/vNhWh1e.png\" alt=\"\"></p>\n<h2 class=\"my-blog-head\" id=\"my-blog-head23\">5、测试集成环境</h2><p>下面是统计单词的小程序</p>\n<pre><code class=\"language-scala=\">package graphTest\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n  * Created by xxx\n  */\nobject myFirstScalaObject {\n  def main(args: Array[String]) {\n    val conf = new SparkConf()\n    conf.setAppName(&quot;world&quot;)\n    conf.setMaster(&quot;local&quot;)\n    val sc = new SparkContext(conf)\n    val lines = sc.textFile(&quot;word&quot;)   //数据路径\n    val words = lines.flatMap{line =&gt; line.split(&quot; &quot;)}\n    val pairs = words.map{ word =&gt; (word,1)}\n    val wordCounts = pairs.reduceByKey(_+_)\n    wordCounts.foreach(wordNumberPair =&gt; println(wordNumberPair._1 + &quot;:&quot; + wordNumberPair._2))\n    sc.stop()\n  }\n}</code></pre>\n<p>然后点击Run即可运行了。</p>\n<p>此处运行时如果出现如下的报错信息，则表明Spark版本和Scala版本不兼容，需要更改scala的版本。但是在本文介绍的scala-2.10.4版本与spark 1.6.2版本是兼容的。</p>\n<pre><code>Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: scala.collection.immutable.HashSet$.empty()Lscala/collection/immutable/HashSet;</code></pre><p><img src=\"https://i.imgur.com/f2wPsAt.png\" alt=\"\"></p>\n','http://www.yuhaochen.top:8888/imgs/2019/02/1b3603c19d32ff3c.jpg','单机版spark',0,_binary '\0'),(5,'382zLecONoQTT1Vjqa9sVa','PyTorch实战指南','5DKFCekkrGH981BYKRrqt8',1550837538,NULL,1550837609,1550837538,0,'\n\n[TOC]\n\n在学习某个深度学习框架时，掌握其基本知识和接口固然重要，但如何合理组织代码，使得代码具有良好的可读性和可扩展性也必不可少。本文不会深入讲解过多知识性的东西，更多的则是传授一些经验，这些内容可能有些争议，因其受我个人喜好和coding风格影响较大，**你可以将这部分当成是一种参考或提议，而不是作为必须遵循的准则**。归根到底，都是希望你能以一种更为合理的方式组织自己的程序。\n\n在做深度学习实验或项目时，为了得到最优的模型结果，中间往往需要很多次的尝试和修改。而合理的文件组织结构，以及一些小技巧可以极大地提高代码的易读易用性。根据我的个人经验，在从事大多数深度学习研究时，程序都需要实现以下几个功能：\n\n- 模型定义\n- 数据处理和加载\n- 训练模型（Train&Validate）\n- 训练过程的可视化\n- 测试（Test/Inference）\n\n另外程序还应该满足以下几个要求：\n\n- 模型需具有高度可配置性，便于修改参数、修改模型，反复实验\n- 代码应具有良好的组织结构，使人一目了然\n- 代码应具有良好的说明，使其他人能够理解\n\n在本文我将应用这些内容，并结合实际的例子，来讲解如何用PyTorch完成Kaggle上的经典比赛：Dogs vs. Cats[^1]。本文所有示例程序均在github上开源 https://github.com/chenyuntc/pytorch-best-practice 。\n\n[^1]: https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition\n\n### 6.1.1 比赛介绍\n\nDogs vs. Cats是一个传统的二分类问题，其训练集包含25000张图片，均放置在同一文件夹下，命名格式为`<category>.<num>.jpg`, 如`cat.10000.jpg`、`dog.100.jpg`，测试集包含12500张图片，命名为`<num>.jpg`，如`1000.jpg`。参赛者需根据训练集的图片训练模型，并在测试集上进行预测，输出它是狗的概率。最后提交的csv文件如下，第一列是图片的`<num>`，第二列是图片为狗的概率。\n\n```\nid,label\n10001,0.889\n10002,0.01\n...\n```\n\n### 6.1.2 文件组织架构\n\n前面提到过，程序主要包含以下功能：\n\n- 模型定义\n- 数据加载\n- 训练和测试\n\n首先来看程序文件的组织结构：\n\n```\n├── checkpoints/\n├── data/\n│   ├── __init__.py\n│   ├── dataset.py\n│   └── get_data.sh\n├── models/\n│   ├── __init__.py\n│   ├── AlexNet.py\n│   ├── BasicModule.py\n│   └── ResNet34.py\n└── utils/\n│   ├── __init__.py\n│   └── visualize.py\n├── config.py\n├── main.py\n├── requirements.txt\n├── README.md\n```\n\n其中：\n\n- `checkpoints/`： 用于保存训练好的模型，可使程序在异常退出后仍能重新载入模型，恢复训练\n- `data/`：数据相关操作，包括数据预处理、dataset实现等\n- `models/`：模型定义，可以有多个模型，例如上面的AlexNet和ResNet34，一个模型对应一个文件\n- `utils/`：可能用到的工具函数，在本次实验中主要是封装了可视化工具\n- `config.py`：配置文件，所有可配置的变量都集中在此，并提供默认值\n- `main.py`：主文件，训练和测试程序的入口，可通过不同的命令来指定不同的操作和参数\n- `requirements.txt`：程序依赖的第三方库\n- `README.md`：提供程序的必要说明\n\n### 6.1.3 关于`__init__.py`\n\n可以看到，几乎每个文件夹下都有`__init__.py`，一个目录如果包含了`__init__.py` 文件，那么它就变成了一个包（package）。`__init__.py`可以为空，也可以定义包的属性和方法，但其必须存在，其它程序才能从这个目录中导入相应的模块或函数。例如在`data/`文件夹下有`__init__.py`，则在`main.py` 中就可以`from data.dataset import DogCat`。而如果在`__init__.py`中写入`from .dataset import DogCat`，则在main.py中就可以直接写为：`from data import DogCat`，或者`import data; dataset = data.DogCat`，相比于`from data.dataset import DogCat`更加便捷。\n\n### 6.1.4 数据加载\n\n数据的相关处理主要保存在`data/dataset.py`中。关于数据加载的相关操作，在上一章中我们已经提到过，其基本原理就是使用`Dataset`提供数据集的封装，再使用`Dataloader`实现数据并行加载。Kaggle提供的数据包括训练集和测试集，而我们在实际使用中，还需专门从训练集中取出一部分作为验证集。对于这三类数据集，其相应操作也不太一样，而如果专门写三个`Dataset`，则稍显复杂和冗余，因此这里通过加一些判断来区分。对于训练集，我们希望做一些数据增强处理，如随机裁剪、随机翻转、加噪声等，而验证集和测试集则不需要。下面看`dataset.py`的代码：\n\n```python\nimport os\nfrom PIL import Image\nfrom torch.utils import data\nimport numpy as np\nfrom torchvision import transforms as T\n\nclass DogCat(data.Dataset):\n    \n    def __init__(self, root, transforms=None, train=True, test=False):\n        \'\'\'\n        目标：获取所有图片地址，并根据训练、验证、测试划分数据\n        \'\'\'\n        self.test = test\n        imgs = [os.path.join(root, img) for img in os.listdir(root)] \n\n        # test1: data/test1/8973.jpg\n        # train: data/train/cat.10004.jpg \n        if self.test:\n            imgs = sorted(imgs, key=lambda x: int(x.split(\'.\')[-2].split(\'/\')[-1]))\n        else:\n            imgs = sorted(imgs, key=lambda x: int(x.split(\'.\')[-2]))\n            \n        imgs_num = len(imgs)\n        \n        # 划分训练、验证集，验证:训练 = 3:7\n        if self.test:\n            self.imgs = imgs\n        elif train:\n            self.imgs = imgs[:int(0.7*imgs_num)]\n        else :\n            self.imgs = imgs[int(0.7*imgs_num):]            \n    \n        if transforms is None:\n        \n            # 数据转换操作，测试验证和训练的数据转换有所区别\n	        \n            normalize = T.Normalize(mean = [0.485, 0.456, 0.406], \n                                     std = [0.229, 0.224, 0.225])\n\n            # 测试集和验证集\n            if self.test or not train: \n                self.transforms = T.Compose([\n                    T.Scale(224),\n                    T.CenterCrop(224),\n                    T.ToTensor(),\n                    normalize\n                ]) \n            # 训练集\n            else :\n                self.transforms = T.Compose([\n                    T.Scale(256),\n                    T.RandomSizedCrop(224),\n                    T.RandomHorizontalFlip(),\n                    T.ToTensor(),\n                    normalize\n                ]) \n                \n        \n    def __getitem__(self, index):\n        \'\'\'\n        返回一张图片的数据\n        对于测试集，没有label，返回图片id，如1000.jpg返回1000\n        \'\'\'\n        img_path = self.imgs[index]\n        if self.test: \n             label = int(self.imgs[index].split(\'.\')[-2].split(\'/\')[-1])\n        else: \n             label = 1 if \'dog\' in img_path.split(\'/\')[-1] else 0\n        data = Image.open(img_path)\n        data = self.transforms(data)\n        return data, label\n    \n    def __len__(self):\n        \'\'\'\n        返回数据集中所有图片的个数\n        \'\'\'\n        return len(self.imgs)\n```\n\n关于数据集使用的注意事项，在上一章中已经提到，将文件读取等费时操作放在`__getitem__`函数中，利用多进程加速。避免一次性将所有图片都读进内存，不仅费时也会占用较大内存，而且不易进行数据增强等操作。另外在这里，我们将训练集中的30%作为验证集，可用来检查模型的训练效果，避免过拟合。在使用时，我们可通过dataloader加载数据。\n\n```\ntrain_dataset = DogCat(opt.train_data_root, train=True)\ntrainloader = DataLoader(train_dataset,\n                        batch_size = opt.batch_size,\n                        shuffle = True,\n                        num_workers = opt.num_workers)\n                  \nfor ii, (data, label) in enumerate(trainloader):\n	train()\n```\n\n### 6.1.5 模型定义\n\n模型的定义主要保存在`models/`目录下，其中`BasicModule`是对`nn.Module`的简易封装，提供快速加载和保存模型的接口。\n\n```python\nclass BasicModule(t.nn.Module):\n    \'\'\'\n    封装了nn.Module，主要提供save和load两个方法\n    \'\'\'\n\n    def __init__(self):\n        super(BasicModule,self).__init__()\n        self.model_name = str(type(self)) # 模型的默认名字\n\n    def load(self, path):\n        \'\'\'\n        可加载指定路径的模型\n        \'\'\'\n        self.load_state_dict(t.load(path))\n\n    def save(self, name=None):\n        \'\'\'\n        保存模型，默认使用“模型名字+时间”作为文件名，\n        如AlexNet_0710_23:57:29.pth\n        \'\'\'\n        if name is None:\n            prefix = \'checkpoints/\' + self.model_name + \'_\'\n            name = time.strftime(prefix + \'%m%d_%H:%M:%S.pth\')\n        t.save(self.state_dict(), name)\n        return name\n```\n\n在实际使用中，直接调用`model.save()`及`model.load(opt.load_path)`即可。\n\n其它自定义模型一般继承`BasicModule`，然后实现自己的模型。其中`AlexNet.py`实现了AlexNet，`ResNet34`实现了ResNet34。在`models/__init__py`中，代码如下：\n\n```python\nfrom .AlexNet import AlexNet\nfrom .ResNet34 import ResNet34\n```\n\n这样在主函数中就可以写成：\n\n```python\nfrom models import AlexNet\n或\nimport models\nmodel = models.AlexNet()\n或\nimport models\nmodel = getattr(\'models\', \'AlexNet\')()\n```\n\n其中最后一种写法最为关键，这意味着我们可以通过字符串直接指定使用的模型，而不必使用判断语句，也不必在每次新增加模型后都修改代码。新增模型后只需要在`models/__init__.py`中加上`from .new_module import new_module`即可。\n\n其它关于模型定义的注意事项，在上一章中已详细讲解，这里就不再赘述，总结起来就是：\n\n- 尽量使用`nn.Sequential`（比如AlexNet）\n- 将经常使用的结构封装成子Module（比如GoogLeNet的Inception结构，ResNet的Residual Block结构）\n- 将重复且有规律性的结构，用函数生成（比如VGG的多种变体，ResNet多种变体都是由多个重复卷积层组成）\n\n### 6.1.6 工具函数\n\n在项目中，我们可能会用到一些helper方法，这些方法可以统一放在`utils/`文件夹下，需要使用时再引入。在本例中主要是封装了可视化工具visdom的一些操作，其代码如下，在本次实验中只会用到`plot`方法，用来统计损失信息。\n\n```python\n#coding:utf8\nimport visdom\nimport time\nimport numpy as np\n\nclass Visualizer(object):\n    \'\'\'\n    封装了visdom的基本操作，但是你仍然可以通过`self.vis.function`\n    或者`self.function`调用原生的visdom接口\n    比如 \n    self.text(\'hello visdom\')\n    self.histogram(t.randn(1000))\n    self.line(t.arange(0, 10),t.arange(1, 11))\n    \'\'\'\n\n    def __init__(self, env=\'default\', **kwargs):\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        \n        # 画的第几个数，相当于横坐标\n        # 保存（’loss\',23） 即loss的第23个点\n        self.index = {} \n        self.log_text = \'\'\n    def reinit(self, env=\'default\', **kwargs):\n        \'\'\'\n        修改visdom的配置\n        \'\'\'\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        return self\n\n    def plot_many(self, d):\n        \'\'\'\n        一次plot多个\n        @params d: dict (name, value) i.e. (\'loss\', 0.11)\n        \'\'\'\n        for k, v in d.iteritems():\n            self.plot(k, v)\n\n    def img_many(self, d):\n        for k, v in d.iteritems():\n            self.img(k, v)\n\n    def plot(self, name, y, **kwargs):\n        \'\'\'\n        self.plot(\'loss\', 1.00)\n        \'\'\'\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=unicode(name),\n                      opts=dict(title=name),\n                      update=None if x == 0 else \'append\',\n                      **kwargs\n                      )\n        self.index[name] = x + 1\n\n    def img(self, name, img_, **kwargs):\n        \'\'\'\n        self.img(\'input_img\', t.Tensor(64, 64))\n        self.img(\'input_imgs\', t.Tensor(3, 64, 64))\n        self.img(\'input_imgs\', t.Tensor(100, 1, 64, 64))\n        self.img(\'input_imgs\', t.Tensor(100, 3, 64, 64), nrows=10)\n\n        !!! don\'t ~~self.img(\'input_imgs\', t.Tensor(100, 64, 64), nrows=10)~~ !!!\n        \'\'\'\n        self.vis.images(img_.cpu().numpy(),\n                       win=unicode(name),\n                       opts=dict(title=name),\n                       **kwargs\n                       )\n\n    def log(self, info, win=\'log_text\'):\n        \'\'\'\n        self.log({\'loss\':1, \'lr\':0.0001})\n        \'\'\'\n\n        self.log_text += (\'[{time}] {info} <br>\'.format(\n                            time=time.strftime(\'%m%d_%H%M%S\'),\\\n                            info=info)) \n        self.vis.text(self.log_text, win)   \n\n    def __getattr__(self, name):\n        \'\'\'\n        自定义的plot,image,log,plot_many等除外\n        self.function 等价于self.vis.function\n        \'\'\'\n        return getattr(self.vis, name)\n```\n\n### 6.1.6 配置文件\n\n在模型定义、数据处理和训练等过程都有很多变量，这些变量应提供默认值，并统一放置在配置文件中，这样在后期调试、修改代码或迁移程序时会比较方便，在这里我们将所有可配置项放在`config.py`中。\n\n```python\nclass DefaultConfig(object):\n    env = \'default\' # visdom 环境\n    model = \'AlexNet\' # 使用的模型，名字必须与models/__init__.py中的名字一致\n    \n    train_data_root = \'./data/train/\' # 训练集存放路径\n    test_data_root = \'./data/test1\' # 测试集存放路径\n    load_model_path = \'checkpoints/model.pth\' # 加载预训练的模型的路径，为None代表不加载\n\n    batch_size = 128 # batch size\n    use_gpu = True # use GPU or not\n    num_workers = 4 # how many workers for loading data\n    print_freq = 20 # print info every N batch\n\n    debug_file = \'/tmp/debug\' # if os.path.exists(debug_file): enter ipdb\n    result_file = \'result.csv\'\n      \n    max_epoch = 10\n    lr = 0.1 # initial learning rate\n    lr_decay = 0.95 # when val_loss increase, lr = lr*lr_decay\n    weight_decay = 1e-4 # 损失函数\n```\n\n可配置的参数主要包括：\n\n- 数据集参数（文件路径、batch_size等）\n- 训练参数（学习率、训练epoch等）\n- 模型参数\n\n这样我们在程序中就可以这样使用：\n\n```\nimport models\nfrom config import DefaultConfig\n\nopt = DefaultConfig()\nlr = opt.lr\nmodel = getattr(models, opt.model)\ndataset = DogCat(opt.train_data_root)\n```\n\n这些都只是默认参数，在这里还提供了更新函数，根据字典更新配置参数。\n\n```\ndef parse(self, kwargs):\n        \'\'\'\n        根据字典kwargs 更新 config参数\n        \'\'\'\n        # 更新配置参数\n        for k, v in kwargs.iteritems():\n            if not hasattr(self, k):\n                # 警告还是报错，取决于你个人的喜好\n                warnings.warn(\"Warning: opt has not attribut %s\" %k)\n            setattr(self, k, v)\n            \n        # 打印配置信息	\n        print(\'user config:\')\n        for k, v in self.__class__.__dict__.iteritems():\n            if not k.startswith(\'__\'):\n                print(k, getattr(self, k))\n```\n\n这样我们在实际使用时，并不需要每次都修改`config.py`，只需要通过命令行传入所需参数，覆盖默认配置即可。\n\n例如：\n\n```\nopt = DefaultConfig()\nnew_config = {\'lr\':0.1,\'use_gpu\':False}\nopt.parse(new_config)\nopt.lr == 0.1\n```\n\n### 6.1.7 main.py\n\n在讲解主程序`main.py`之前，我们先来看看2017年3月谷歌开源的一个命令行工具`fire`[^3] ，通过`pip install fire`即可安装。下面来看看`fire`的基础用法，假设`example.py`文件内容如下：\n\n```python\nimport fire\n\ndef add(x, y):\n  return x + y\n  \ndef mul(**kwargs):\n    a = kwargs[\'a\']\n    b = kwargs[\'b\']\n    return a * b\n\nif __name__ == \'__main__\':\n  fire.Fire()\n```\n\n那么我们可以使用：\n\n```bash\npython example.py add 1 2 # 执行add(1, 2)\npython example.py mul --a=1 --b=2 # 执行mul(a=1, b=2), kwargs={\'a\':1, \'b\':2}\npython example.py add --x=1 --y==2 # 执行add(x=1, y=2)\n```\n\n可见，只要在程序中运行`fire.Fire()`，即可使用命令行参数`python file <function> [args,] {--kwargs,}`。fire还支持更多的高级功能，具体请参考官方指南[^4] 。\n\n[^3]: https://github.com/google/python-fire\n[^4]: https://github.com/google/python-fire/blob/master/doc/guide.md\n\n在主程序`main.py`中，主要包含四个函数，其中三个需要命令行执行，`main.py`的代码组织结构如下：\n\n```python\ndef train(**kwargs):\n    \'\'\'\n    训练\n    \'\'\'\n    pass\n	 \ndef val(model, dataloader):\n    \'\'\'\n    计算模型在验证集上的准确率等信息，用以辅助训练\n    \'\'\'\n    pass\n\ndef test(**kwargs):\n    \'\'\'\n    测试（inference）\n    \'\'\'\n    pass\n\ndef help():\n    \'\'\'\n    打印帮助的信息 \n    \'\'\'\n    print(\'help\')\n\nif __name__==\'__main__\':\n    import fire\n    fire.Fire()\n```\n\n根据fire的使用方法，可通过`python main.py <function> --args=xx`的方式来执行训练或者测试。\n\n#### 训练\n\n训练的主要步骤如下：\n\n- 定义网络\n- 定义数据\n- 定义损失函数和优化器\n- 计算重要指标\n- 开始训练\n  - 训练网络\n  - 可视化各种指标\n  - 计算在验证集上的指标\n\n训练函数的代码如下：\n\n```python\ndef train(**kwargs):\n    \n    # 根据命令行参数更新配置\n    opt.parse(kwargs)\n    vis = Visualizer(opt.env)\n    \n    # step1: 模型\n    model = getattr(models, opt.model)()\n    if opt.load_model_path:\n        model.load(opt.load_model_path)\n    if opt.use_gpu: model.cuda()\n\n    # step2: 数据\n    train_data = DogCat(opt.train_data_root,train=True)\n    val_data = DogCat(opt.train_data_root,train=False)\n    train_dataloader = DataLoader(train_data,opt.batch_size,\n                        shuffle=True,\n                        num_workers=opt.num_workers)\n    val_dataloader = DataLoader(val_data,opt.batch_size,\n                        shuffle=False,\n                        num_workers=opt.num_workers)\n    \n    # step3: 目标函数和优化器\n    criterion = t.nn.CrossEntropyLoss()\n    lr = opt.lr\n    optimizer = t.optim.Adam(model.parameters(),\n                            lr = lr,\n                            weight_decay = opt.weight_decay)\n        \n    # step4: 统计指标：平滑处理之后的损失，还有混淆矩阵\n    loss_meter = meter.AverageValueMeter()\n    confusion_matrix = meter.ConfusionMeter(2)\n    previous_loss = 1e100\n\n    # 训练\n    for epoch in range(opt.max_epoch):\n        \n        loss_meter.reset()\n        confusion_matrix.reset()\n\n        for ii,(data,label) in enumerate(train_dataloader):\n\n            # 训练模型参数 \n            input = Variable(data)\n            target = Variable(label)\n            if opt.use_gpu:\n                input = input.cuda()\n                target = target.cuda()\n            optimizer.zero_grad()\n            score = model(input)\n            loss = criterion(score,target)\n            loss.backward()\n            optimizer.step()\n            \n            # 更新统计指标以及可视化\n            loss_meter.add(loss.data[0])\n            confusion_matrix.add(score.data, target.data)\n\n            if ii%opt.print_freq==opt.print_freq-1:\n                vis.plot(\'loss\', loss_meter.value()[0])\n                \n                # 如果需要的话，进入debug模式\n                if os.path.exists(opt.debug_file):\n                    import ipdb;\n                    ipdb.set_trace()\n\n        model.save()\n\n        # 计算验证集上的指标及可视化\n        val_cm,val_accuracy = val(model,val_dataloader)\n        vis.plot(\'val_accuracy\',val_accuracy)\n        vis.log(\"epoch:{epoch},lr:{lr},loss:{loss},train_cm:{train_cm},val_cm:{val_cm}\"\n        .format(\n                    epoch = epoch,\n                    loss = loss_meter.value()[0],\n                    val_cm = str(val_cm.value()),\n                    train_cm=str(confusion_matrix.value()),\n                    lr=lr))\n        \n        # 如果损失不再下降，则降低学习率\n        if loss_meter.value()[0] > previous_loss:          \n            lr = lr * opt.lr_decay\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = lr\n                \n        previous_loss = loss_meter.value()[0]\n```\n\n这里用到了PyTorchNet[^5]里面的一个工具: meter。meter提供了一些轻量级的工具，用于帮助用户快速统计训练过程中的一些指标。`AverageValueMeter`能够计算所有数的平均值和标准差，这里用来统计一个epoch中损失的平均值。`confusionmeter`用来统计分类问题中的分类情况，是一个比准确率更详细的统计指标。例如对于表格6-1，共有50张狗的图片，其中有35张被正确分类成了狗，还有15张被误判成猫；共有100张猫的图片，其中有91张被正确判为了猫，剩下9张被误判成狗。相比于准确率等统计信息，混淆矩阵更能体现分类的结果，尤其是在样本比例不均衡的情况下。\n\n表6-1 混淆矩阵\n\n| 样本   | 判为狗  | 判为猫  |\n| ---- | ---- | ---- |\n| 实际是狗 | 35   | 15   |\n| 实际是猫 | 9    | 91   |\n\nPyTorchNet从TorchNet[^6]迁移而来，提供了很多有用的工具，但其目前开发和文档都还不是很完善，本书不做过多的讲解。\n\n[^5]: https://github.com/pytorch/tnt\n[^6]: https://github.com/torchnet/torchnet\n\n#### 验证\n\n验证相对来说比较简单，但要注意需将模型置于验证模式(`model.eval()`)，验证完成后还需要将其置回为训练模式(`model.train()`)，这两句代码会影响`BatchNorm`和`Dropout`等层的运行模式。验证模型准确率的代码如下。\n\n```python\ndef val(model,dataloader):\n    \'\'\'\n    计算模型在验证集上的准确率等信息\n    \'\'\'\n\n    # 把模型设为验证模式\n    model.eval()\n    \n    confusion_matrix = meter.ConfusionMeter(2)\n    for ii, data in enumerate(dataloader):\n        input, label = data\n        val_input = Variable(input, volatile=True)\n        val_label = Variable(label.long(), volatile=True)\n        if opt.use_gpu:\n            val_input = val_input.cuda()\n            val_label = val_label.cuda()\n        score = model(val_input)\n        confusion_matrix.add(score.data.squeeze(), label.long())\n\n    # 把模型恢复为训练模式\n    model.train()\n    \n    cm_value = confusion_matrix.value()\n    accuracy = 100. * (cm_value[0][0] + cm_value[1][1]) /\\\n			     (cm_value.sum())\n    return confusion_matrix, accuracy\n```\n\n#### 测试\n\n测试时，需要计算每个样本属于狗的概率，并将结果保存成csv文件。测试的代码与验证比较相似，但需要自己加载模型和数据。\n\n```python\ndef test(**kwargs):\n    opt.parse(kwargs)\n    \n    # 模型\n    model = getattr(models, opt.model)().eval()\n    if opt.load_model_path:\n        model.load(opt.load_model_path)\n    if opt.use_gpu: model.cuda()\n\n    # 数据\n    train_data = DogCat(opt.test_data_root,test=True)\n    test_dataloader = DataLoader(train_data,\\\n							    batch_size=opt.batch_size,\\\n							    shuffle=False,\\\n							    num_workers=opt.num_workers)\n    \n    results = []\n    for ii,(data,path) in enumerate(test_dataloader):\n        input = t.autograd.Variable(data,volatile = True)\n        if opt.use_gpu: input = input.cuda()\n        score = model(input)\n        probability = t.nn.functional.softmax\\\n	        (score)[:,1].data.tolist()      \n        batch_results = [(path_,probability_) \\\n	        for path_,probability_ in zip(path,probability) ]\n        results += batch_results\n    write_csv(results,opt.result_file)\n    return results\n```\n\n#### 帮助函数\n\n为了方便他人使用, 程序中还应当提供一个帮助函数，用于说明函数是如何使用。程序的命令行接口中有众多参数，如果手动用字符串表示不仅复杂，而且后期修改config文件时，还需要修改对应的帮助信息，十分不便。这里使用了Python标准库中的inspect方法，可以自动获取config的源代码。help的代码如下:\n\n```python\ndef help():\n    \'\'\'\n    打印帮助的信息： python file.py help\n    \'\'\'\n    \n    print(\'\'\'\n    usage : python {0} <function> [--args=value,]\n    <function> := train | test | help\n    example: \n            python {0} train --env=\'env0701\' --lr=0.01\n            python {0} test --dataset=\'path/to/dataset/root/\'\n            python {0} help\n    avaiable args:\'\'\'.format(__file__))\n\n    from inspect import getsource\n    source = (getsource(opt.__class__))\n    print(source)\n```\n\n当用户执行`python main.py help`的时候，会打印如下帮助信息：\n\n```bash\n    usage : python main.py <function> [--args=value,]\n    <function> := train | test | help\n    example: \n            python main.py train --env=\'env0701\' --lr=0.01\n            python main.py test --dataset=\'path/to/dataset/\'\n            python main.py help\n    avaiable args:\nclass DefaultConfig(object):\n    env = \'default\' # visdom 环境\n    model = \'AlexNet\' # 使用的模型\n    \n    train_data_root = \'./data/train/\' # 训练集存放路径\n    test_data_root = \'./data/test1\' # 测试集存放路径\n    load_model_path = \'checkpoints/model.pth\' # 加载预训练的模型\n\n    batch_size = 128 # batch size\n    use_gpu = True # user GPU or not\n    num_workers = 4 # how many workers for loading data\n    print_freq = 20 # print info every N batch\n\n    debug_file = \'/tmp/debug\' \n    result_file = \'result.csv\' # 结果文件\n      \n    max_epoch = 10\n    lr = 0.1 # initial learning rate\n    lr_decay = 0.95 # when val_loss increase, lr = lr*lr_decay\n    weight_decay = 1e-4 # 损失函数\n```\n\n### 6.1.8 使用\n\n正如`help`函数的打印信息所述，可以通过命令行参数指定变量名.下面是三个使用例子，fire会将包含`-`的命令行参数自动转层下划线`_`，也会将非数值的值转成字符串。所以`--train-data-root=data/train`和`--train_data_root=\'data/train\'`是等价的。\n\n```\n# 训练模型\npython main.py train \n        --train-data-root=data/train/ \n        --load-model-path=\'checkpoints/resnet34_16:53:00.pth\' \n        --lr=0.005 \n        --batch-size=32 \n        --model=\'ResNet34\'  \n        --max-epoch = 20\n\n# 测试模型\npython main.py test\n       --test-data-root=data/test1 \n       --load-model-path=\'checkpoints/resnet34_00:23:05.pth\' \n       --batch-size=128 \n       --model=\'ResNet34\' \n       --num-workers=12\n\n# 打印帮助信息\npython main.py help\n```\n\n### 6.1.9 争议\n\n以上的程序设计规范带有作者强烈的个人喜好，并不想作为一个标准，而是作为一个提议和一种参考。上述设计在很多地方还有待商榷，例如对于训练过程是否应该封装成一个`trainer`对象，或者直接封装到`BaiscModule`的`train`方法之中。对命令行参数的处理也有不少值得讨论之处。因此不要将本文中的观点作为一个必须遵守的规范，而应该看作一个参考。\n\n本章中的设计可能会引起不少争议，其中比较值得商榷的部分主要有以下两个方面：\n\n- 命令行参数的设置。目前大多数程序都是使用Python标准库中的`argparse`来处理命令行参数，也有些使用比较轻量级的`click`。这种处理相对来说对命令行的支持更完备，但根据作者的经验来看，这种做法不够直观，并且代码量相对来说也较多。比如`argparse`，每次增加一个命令行参数，都必须写如下代码：\n\n```python\nparser.add_argument(\'-save-interval\', type=int, default=500, help=\'how many steps to wait before saving [default:500]\')\n```\n\n在读者眼中，这种实现方式远不如一个专门的`config.py`来的直观和易用。尤其是对于使用Jupyter notebook或IPython等交互式调试的用户来说，`argparse`较难使用。\n\n- 模型训练。有不少人喜欢将模型的训练过程集成于模型的定义之中，代码结构如下所示：\n\n```python\n  class MyModel(nn.Module):\n  	\n      def __init__(self,opt):\n          self.dataloader = Dataloader(opt)\n          self.optimizer  = optim.Adam(self.parameters(),lr=0.001)\n          self.lr = opt.lr\n          self.model = make_model()\n      \n      def forward(self,input):\n          pass\n      \n      def train_(self):\n          # 训练模型\n          for epoch in range(opt.max_epoch)\n          	for ii,data in enumerate(self.dataloader):\n              	train_epoch()\n              \n          	model.save()\n  	\n      def train_epoch(self):\n          pass\n```\n\n抑或是专门设计一个`Trainer`对象，形如：\n\n```python\n    \'\'\'\n  code simplified from:\n  https://github.com/pytorch/pytorch/blob/master/torch/utils/trainer/trainer.py\n  \'\'\'\n  import heapq\n  from torch.autograd import Variable\n\n  class Trainer(object):\n\n      def __init__(self, model=None, criterion=None, optimizer=None, dataset=None):\n          self.model = model\n          self.criterion = criterion\n          self.optimizer = optimizer\n          self.dataset = dataset\n          self.iterations = 0\n\n      def run(self, epochs=1):\n          for i in range(1, epochs + 1):\n              self.train()\n\n      def train(self):\n          for i, data in enumerate(self.dataset, self.iterations + 1):\n              batch_input, batch_target = data\n              self.call_plugins(\'batch\', i, batch_input, batch_target)\n              input_var = Variable(batch_input)\n              target_var = Variable(batch_target)\n    \n              plugin_data = [None, None]\n    \n              def closure():\n                  batch_output = self.model(input_var)\n                  loss = self.criterion(batch_output, target_var)\n                  loss.backward()\n                  if plugin_data[0] is None:\n                      plugin_data[0] = batch_output.data\n                      plugin_data[1] = loss.data\n                  return loss\n    \n              self.optimizer.zero_grad()\n              self.optimizer.step(closure)\n    \n          self.iterations += i\n```\n\n还有一些人喜欢模仿keras和scikit-learn的设计，设计一个`fit`接口。对读者来说，这些处理方式很难说哪个更好或更差，找到最适合自己的方法才是最好的。\n\n- `BasicModule` 的封装，可多可少。训练过程中的很多操作都可以移到`BasicModule`之中，比如`get_optimizer`方法用来获取优化器，比如`train_step`用来执行单歩训练。对于不同的模型，如果对应的优化器定义不一样，或者是训练方法不一样，可以复写这些函数自定义相应的方法，取决于自己的喜好和项目的实际需求。   ','<p>[TOC]</p>\n<p>在学习某个深度学习框架时，掌握其基本知识和接口固然重要，但如何合理组织代码，使得代码具有良好的可读性和可扩展性也必不可少。本文不会深入讲解过多知识性的东西，更多的则是传授一些经验，这些内容可能有些争议，因其受我个人喜好和coding风格影响较大，<strong>你可以将这部分当成是一种参考或提议，而不是作为必须遵循的准则</strong>。归根到底，都是希望你能以一种更为合理的方式组织自己的程序。</p>\n<p>在做深度学习实验或项目时，为了得到最优的模型结果，中间往往需要很多次的尝试和修改。而合理的文件组织结构，以及一些小技巧可以极大地提高代码的易读易用性。根据我的个人经验，在从事大多数深度学习研究时，程序都需要实现以下几个功能：</p>\n<ul>\n<li>模型定义</li>\n<li>数据处理和加载</li>\n<li>训练模型（Train&amp;Validate）</li>\n<li>训练过程的可视化</li>\n<li>测试（Test/Inference）</li>\n</ul>\n<p>另外程序还应该满足以下几个要求：</p>\n<ul>\n<li>模型需具有高度可配置性，便于修改参数、修改模型，反复实验</li>\n<li>代码应具有良好的组织结构，使人一目了然</li>\n<li>代码应具有良好的说明，使其他人能够理解</li>\n</ul>\n<p>在本文我将应用这些内容，并结合实际的例子，来讲解如何用PyTorch完成Kaggle上的经典比赛：Dogs vs. Cats<a href=\"https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition\">^1</a>。本文所有示例程序均在github上开源 <a href=\"https://github.com/chenyuntc/pytorch-best-practice\">https://github.com/chenyuntc/pytorch-best-practice</a> 。</p>\n<h3 class=\"my-blog-head\" id=\"my-blog-head14\">6.1.1 比赛介绍</h3><p>Dogs vs. Cats是一个传统的二分类问题，其训练集包含25000张图片，均放置在同一文件夹下，命名格式为<code>&lt;category&gt;.&lt;num&gt;.jpg</code>, 如<code>cat.10000.jpg</code>、<code>dog.100.jpg</code>，测试集包含12500张图片，命名为<code>&lt;num&gt;.jpg</code>，如<code>1000.jpg</code>。参赛者需根据训练集的图片训练模型，并在测试集上进行预测，输出它是狗的概率。最后提交的csv文件如下，第一列是图片的<code>&lt;num&gt;</code>，第二列是图片为狗的概率。</p>\n<pre><code>id,label\n10001,0.889\n10002,0.01\n...</code></pre><h3 class=\"my-blog-head\" id=\"my-blog-head15\">6.1.2 文件组织架构</h3><p>前面提到过，程序主要包含以下功能：</p>\n<ul>\n<li>模型定义</li>\n<li>数据加载</li>\n<li>训练和测试</li>\n</ul>\n<p>首先来看程序文件的组织结构：</p>\n<pre><code>├── checkpoints/\n├── data/\n│   ├── __init__.py\n│   ├── dataset.py\n│   └── get_data.sh\n├── models/\n│   ├── __init__.py\n│   ├── AlexNet.py\n│   ├── BasicModule.py\n│   └── ResNet34.py\n└── utils/\n│   ├── __init__.py\n│   └── visualize.py\n├── config.py\n├── main.py\n├── requirements.txt\n├── README.md</code></pre><p>其中：</p>\n<ul>\n<li><code>checkpoints/</code>： 用于保存训练好的模型，可使程序在异常退出后仍能重新载入模型，恢复训练</li>\n<li><code>data/</code>：数据相关操作，包括数据预处理、dataset实现等</li>\n<li><code>models/</code>：模型定义，可以有多个模型，例如上面的AlexNet和ResNet34，一个模型对应一个文件</li>\n<li><code>utils/</code>：可能用到的工具函数，在本次实验中主要是封装了可视化工具</li>\n<li><code>config.py</code>：配置文件，所有可配置的变量都集中在此，并提供默认值</li>\n<li><code>main.py</code>：主文件，训练和测试程序的入口，可通过不同的命令来指定不同的操作和参数</li>\n<li><code>requirements.txt</code>：程序依赖的第三方库</li>\n<li><code>README.md</code>：提供程序的必要说明</li>\n</ul>\n<h3 class=\"my-blog-head\" id=\"my-blog-head16\">6.1.3 关于<code>__init__.py</code></h3><p>可以看到，几乎每个文件夹下都有<code>__init__.py</code>，一个目录如果包含了<code>__init__.py</code> 文件，那么它就变成了一个包（package）。<code>__init__.py</code>可以为空，也可以定义包的属性和方法，但其必须存在，其它程序才能从这个目录中导入相应的模块或函数。例如在<code>data/</code>文件夹下有<code>__init__.py</code>，则在<code>main.py</code> 中就可以<code>from data.dataset import DogCat</code>。而如果在<code>__init__.py</code>中写入<code>from .dataset import DogCat</code>，则在main.py中就可以直接写为：<code>from data import DogCat</code>，或者<code>import data; dataset = data.DogCat</code>，相比于<code>from data.dataset import DogCat</code>更加便捷。</p>\n<h3 class=\"my-blog-head\" id=\"my-blog-head17\">6.1.4 数据加载</h3><p>数据的相关处理主要保存在<code>data/dataset.py</code>中。关于数据加载的相关操作，在上一章中我们已经提到过，其基本原理就是使用<code>Dataset</code>提供数据集的封装，再使用<code>Dataloader</code>实现数据并行加载。Kaggle提供的数据包括训练集和测试集，而我们在实际使用中，还需专门从训练集中取出一部分作为验证集。对于这三类数据集，其相应操作也不太一样，而如果专门写三个<code>Dataset</code>，则稍显复杂和冗余，因此这里通过加一些判断来区分。对于训练集，我们希望做一些数据增强处理，如随机裁剪、随机翻转、加噪声等，而验证集和测试集则不需要。下面看<code>dataset.py</code>的代码：</p>\n<pre><code class=\"language-python\">import os\nfrom PIL import Image\nfrom torch.utils import data\nimport numpy as np\nfrom torchvision import transforms as T\n\nclass DogCat(data.Dataset):\n\n    def __init__(self, root, transforms=None, train=True, test=False):\n        &#39;&#39;&#39;\n        目标：获取所有图片地址，并根据训练、验证、测试划分数据\n        &#39;&#39;&#39;\n        self.test = test\n        imgs = [os.path.join(root, img) for img in os.listdir(root)] \n\n        # test1: data/test1/8973.jpg\n        # train: data/train/cat.10004.jpg \n        if self.test:\n            imgs = sorted(imgs, key=lambda x: int(x.split(&#39;.&#39;)[-2].split(&#39;/&#39;)[-1]))\n        else:\n            imgs = sorted(imgs, key=lambda x: int(x.split(&#39;.&#39;)[-2]))\n\n        imgs_num = len(imgs)\n\n        # 划分训练、验证集，验证:训练 = 3:7\n        if self.test:\n            self.imgs = imgs\n        elif train:\n            self.imgs = imgs[:int(0.7*imgs_num)]\n        else :\n            self.imgs = imgs[int(0.7*imgs_num):]            \n\n        if transforms is None:\n\n            # 数据转换操作，测试验证和训练的数据转换有所区别\n\n            normalize = T.Normalize(mean = [0.485, 0.456, 0.406], \n                                     std = [0.229, 0.224, 0.225])\n\n            # 测试集和验证集\n            if self.test or not train: \n                self.transforms = T.Compose([\n                    T.Scale(224),\n                    T.CenterCrop(224),\n                    T.ToTensor(),\n                    normalize\n                ]) \n            # 训练集\n            else :\n                self.transforms = T.Compose([\n                    T.Scale(256),\n                    T.RandomSizedCrop(224),\n                    T.RandomHorizontalFlip(),\n                    T.ToTensor(),\n                    normalize\n                ]) \n\n\n    def __getitem__(self, index):\n        &#39;&#39;&#39;\n        返回一张图片的数据\n        对于测试集，没有label，返回图片id，如1000.jpg返回1000\n        &#39;&#39;&#39;\n        img_path = self.imgs[index]\n        if self.test: \n             label = int(self.imgs[index].split(&#39;.&#39;)[-2].split(&#39;/&#39;)[-1])\n        else: \n             label = 1 if &#39;dog&#39; in img_path.split(&#39;/&#39;)[-1] else 0\n        data = Image.open(img_path)\n        data = self.transforms(data)\n        return data, label\n\n    def __len__(self):\n        &#39;&#39;&#39;\n        返回数据集中所有图片的个数\n        &#39;&#39;&#39;\n        return len(self.imgs)</code></pre>\n<p>关于数据集使用的注意事项，在上一章中已经提到，将文件读取等费时操作放在<code>__getitem__</code>函数中，利用多进程加速。避免一次性将所有图片都读进内存，不仅费时也会占用较大内存，而且不易进行数据增强等操作。另外在这里，我们将训练集中的30%作为验证集，可用来检查模型的训练效果，避免过拟合。在使用时，我们可通过dataloader加载数据。</p>\n<pre><code>train_dataset = DogCat(opt.train_data_root, train=True)\ntrainloader = DataLoader(train_dataset,\n                        batch_size = opt.batch_size,\n                        shuffle = True,\n                        num_workers = opt.num_workers)\n\nfor ii, (data, label) in enumerate(trainloader):\n    train()</code></pre><h3 class=\"my-blog-head\" id=\"my-blog-head18\">6.1.5 模型定义</h3><p>模型的定义主要保存在<code>models/</code>目录下，其中<code>BasicModule</code>是对<code>nn.Module</code>的简易封装，提供快速加载和保存模型的接口。</p>\n<pre><code class=\"language-python\">class BasicModule(t.nn.Module):\n    &#39;&#39;&#39;\n    封装了nn.Module，主要提供save和load两个方法\n    &#39;&#39;&#39;\n\n    def __init__(self):\n        super(BasicModule,self).__init__()\n        self.model_name = str(type(self)) # 模型的默认名字\n\n    def load(self, path):\n        &#39;&#39;&#39;\n        可加载指定路径的模型\n        &#39;&#39;&#39;\n        self.load_state_dict(t.load(path))\n\n    def save(self, name=None):\n        &#39;&#39;&#39;\n        保存模型，默认使用“模型名字+时间”作为文件名，\n        如AlexNet_0710_23:57:29.pth\n        &#39;&#39;&#39;\n        if name is None:\n            prefix = &#39;checkpoints/&#39; + self.model_name + &#39;_&#39;\n            name = time.strftime(prefix + &#39;%m%d_%H:%M:%S.pth&#39;)\n        t.save(self.state_dict(), name)\n        return name</code></pre>\n<p>在实际使用中，直接调用<code>model.save()</code>及<code>model.load(opt.load_path)</code>即可。</p>\n<p>其它自定义模型一般继承<code>BasicModule</code>，然后实现自己的模型。其中<code>AlexNet.py</code>实现了AlexNet，<code>ResNet34</code>实现了ResNet34。在<code>models/__init__py</code>中，代码如下：</p>\n<pre><code class=\"language-python\">from .AlexNet import AlexNet\nfrom .ResNet34 import ResNet34</code></pre>\n<p>这样在主函数中就可以写成：</p>\n<pre><code class=\"language-python\">from models import AlexNet\n或\nimport models\nmodel = models.AlexNet()\n或\nimport models\nmodel = getattr(&#39;models&#39;, &#39;AlexNet&#39;)()</code></pre>\n<p>其中最后一种写法最为关键，这意味着我们可以通过字符串直接指定使用的模型，而不必使用判断语句，也不必在每次新增加模型后都修改代码。新增模型后只需要在<code>models/__init__.py</code>中加上<code>from .new_module import new_module</code>即可。</p>\n<p>其它关于模型定义的注意事项，在上一章中已详细讲解，这里就不再赘述，总结起来就是：</p>\n<ul>\n<li>尽量使用<code>nn.Sequential</code>（比如AlexNet）</li>\n<li>将经常使用的结构封装成子Module（比如GoogLeNet的Inception结构，ResNet的Residual Block结构）</li>\n<li>将重复且有规律性的结构，用函数生成（比如VGG的多种变体，ResNet多种变体都是由多个重复卷积层组成）</li>\n</ul>\n<h3 class=\"my-blog-head\" id=\"my-blog-head19\">6.1.6 工具函数</h3><p>在项目中，我们可能会用到一些helper方法，这些方法可以统一放在<code>utils/</code>文件夹下，需要使用时再引入。在本例中主要是封装了可视化工具visdom的一些操作，其代码如下，在本次实验中只会用到<code>plot</code>方法，用来统计损失信息。</p>\n<pre><code class=\"language-python\">#coding:utf8\nimport visdom\nimport time\nimport numpy as np\n\nclass Visualizer(object):\n    &#39;&#39;&#39;\n    封装了visdom的基本操作，但是你仍然可以通过`self.vis.function`\n    或者`self.function`调用原生的visdom接口\n    比如 \n    self.text(&#39;hello visdom&#39;)\n    self.histogram(t.randn(1000))\n    self.line(t.arange(0, 10),t.arange(1, 11))\n    &#39;&#39;&#39;\n\n    def __init__(self, env=&#39;default&#39;, **kwargs):\n        self.vis = visdom.Visdom(env=env, **kwargs)\n\n        # 画的第几个数，相当于横坐标\n        # 保存（’loss&#39;,23） 即loss的第23个点\n        self.index = {} \n        self.log_text = &#39;&#39;\n    def reinit(self, env=&#39;default&#39;, **kwargs):\n        &#39;&#39;&#39;\n        修改visdom的配置\n        &#39;&#39;&#39;\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        return self\n\n    def plot_many(self, d):\n        &#39;&#39;&#39;\n        一次plot多个\n        @params d: dict (name, value) i.e. (&#39;loss&#39;, 0.11)\n        &#39;&#39;&#39;\n        for k, v in d.iteritems():\n            self.plot(k, v)\n\n    def img_many(self, d):\n        for k, v in d.iteritems():\n            self.img(k, v)\n\n    def plot(self, name, y, **kwargs):\n        &#39;&#39;&#39;\n        self.plot(&#39;loss&#39;, 1.00)\n        &#39;&#39;&#39;\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=unicode(name),\n                      opts=dict(title=name),\n                      update=None if x == 0 else &#39;append&#39;,\n                      **kwargs\n                      )\n        self.index[name] = x + 1\n\n    def img(self, name, img_, **kwargs):\n        &#39;&#39;&#39;\n        self.img(&#39;input_img&#39;, t.Tensor(64, 64))\n        self.img(&#39;input_imgs&#39;, t.Tensor(3, 64, 64))\n        self.img(&#39;input_imgs&#39;, t.Tensor(100, 1, 64, 64))\n        self.img(&#39;input_imgs&#39;, t.Tensor(100, 3, 64, 64), nrows=10)\n\n        !!! don&#39;t ~~self.img(&#39;input_imgs&#39;, t.Tensor(100, 64, 64), nrows=10)~~ !!!\n        &#39;&#39;&#39;\n        self.vis.images(img_.cpu().numpy(),\n                       win=unicode(name),\n                       opts=dict(title=name),\n                       **kwargs\n                       )\n\n    def log(self, info, win=&#39;log_text&#39;):\n        &#39;&#39;&#39;\n        self.log({&#39;loss&#39;:1, &#39;lr&#39;:0.0001})\n        &#39;&#39;&#39;\n\n        self.log_text += (&#39;[{time}] {info} &lt;br&gt;&#39;.format(\n                            time=time.strftime(&#39;%m%d_%H%M%S&#39;),\\\n                            info=info)) \n        self.vis.text(self.log_text, win)   \n\n    def __getattr__(self, name):\n        &#39;&#39;&#39;\n        自定义的plot,image,log,plot_many等除外\n        self.function 等价于self.vis.function\n        &#39;&#39;&#39;\n        return getattr(self.vis, name)</code></pre>\n<h3 class=\"my-blog-head\" id=\"my-blog-head20\">6.1.6 配置文件</h3><p>在模型定义、数据处理和训练等过程都有很多变量，这些变量应提供默认值，并统一放置在配置文件中，这样在后期调试、修改代码或迁移程序时会比较方便，在这里我们将所有可配置项放在<code>config.py</code>中。</p>\n<pre><code class=\"language-python\">class DefaultConfig(object):\n    env = &#39;default&#39; # visdom 环境\n    model = &#39;AlexNet&#39; # 使用的模型，名字必须与models/__init__.py中的名字一致\n\n    train_data_root = &#39;./data/train/&#39; # 训练集存放路径\n    test_data_root = &#39;./data/test1&#39; # 测试集存放路径\n    load_model_path = &#39;checkpoints/model.pth&#39; # 加载预训练的模型的路径，为None代表不加载\n\n    batch_size = 128 # batch size\n    use_gpu = True # use GPU or not\n    num_workers = 4 # how many workers for loading data\n    print_freq = 20 # print info every N batch\n\n    debug_file = &#39;/tmp/debug&#39; # if os.path.exists(debug_file): enter ipdb\n    result_file = &#39;result.csv&#39;\n\n    max_epoch = 10\n    lr = 0.1 # initial learning rate\n    lr_decay = 0.95 # when val_loss increase, lr = lr*lr_decay\n    weight_decay = 1e-4 # 损失函数</code></pre>\n<p>可配置的参数主要包括：</p>\n<ul>\n<li>数据集参数（文件路径、batch_size等）</li>\n<li>训练参数（学习率、训练epoch等）</li>\n<li>模型参数</li>\n</ul>\n<p>这样我们在程序中就可以这样使用：</p>\n<pre><code>import models\nfrom config import DefaultConfig\n\nopt = DefaultConfig()\nlr = opt.lr\nmodel = getattr(models, opt.model)\ndataset = DogCat(opt.train_data_root)</code></pre><p>这些都只是默认参数，在这里还提供了更新函数，根据字典更新配置参数。</p>\n<pre><code>def parse(self, kwargs):\n        &#39;&#39;&#39;\n        根据字典kwargs 更新 config参数\n        &#39;&#39;&#39;\n        # 更新配置参数\n        for k, v in kwargs.iteritems():\n            if not hasattr(self, k):\n                # 警告还是报错，取决于你个人的喜好\n                warnings.warn(&quot;Warning: opt has not attribut %s&quot; %k)\n            setattr(self, k, v)\n\n        # 打印配置信息    \n        print(&#39;user config:&#39;)\n        for k, v in self.__class__.__dict__.iteritems():\n            if not k.startswith(&#39;__&#39;):\n                print(k, getattr(self, k))</code></pre><p>这样我们在实际使用时，并不需要每次都修改<code>config.py</code>，只需要通过命令行传入所需参数，覆盖默认配置即可。</p>\n<p>例如：</p>\n<pre><code>opt = DefaultConfig()\nnew_config = {&#39;lr&#39;:0.1,&#39;use_gpu&#39;:False}\nopt.parse(new_config)\nopt.lr == 0.1</code></pre><h3 class=\"my-blog-head\" id=\"my-blog-head21\">6.1.7 main.py</h3><p>在讲解主程序<code>main.py</code>之前，我们先来看看2017年3月谷歌开源的一个命令行工具<code>fire</code><a href=\"https://github.com/google/python-fire\">^3</a> ，通过<code>pip install fire</code>即可安装。下面来看看<code>fire</code>的基础用法，假设<code>example.py</code>文件内容如下：</p>\n<pre><code class=\"language-python\">import fire\n\ndef add(x, y):\n  return x + y\n\ndef mul(**kwargs):\n    a = kwargs[&#39;a&#39;]\n    b = kwargs[&#39;b&#39;]\n    return a * b\n\nif __name__ == &#39;__main__&#39;:\n  fire.Fire()</code></pre>\n<p>那么我们可以使用：</p>\n<pre><code class=\"language-bash\">python example.py add 1 2 # 执行add(1, 2)\npython example.py mul --a=1 --b=2 # 执行mul(a=1, b=2), kwargs={&#39;a&#39;:1, &#39;b&#39;:2}\npython example.py add --x=1 --y==2 # 执行add(x=1, y=2)</code></pre>\n<p>可见，只要在程序中运行<code>fire.Fire()</code>，即可使用命令行参数<code>python file &lt;function&gt; [args,] {--kwargs,}</code>。fire还支持更多的高级功能，具体请参考官方指南<a href=\"https://github.com/google/python-fire/blob/master/doc/guide.md\">^4</a> 。</p>\n<p>在主程序<code>main.py</code>中，主要包含四个函数，其中三个需要命令行执行，<code>main.py</code>的代码组织结构如下：</p>\n<pre><code class=\"language-python\">def train(**kwargs):\n    &#39;&#39;&#39;\n    训练\n    &#39;&#39;&#39;\n    pass\n\ndef val(model, dataloader):\n    &#39;&#39;&#39;\n    计算模型在验证集上的准确率等信息，用以辅助训练\n    &#39;&#39;&#39;\n    pass\n\ndef test(**kwargs):\n    &#39;&#39;&#39;\n    测试（inference）\n    &#39;&#39;&#39;\n    pass\n\ndef help():\n    &#39;&#39;&#39;\n    打印帮助的信息 \n    &#39;&#39;&#39;\n    print(&#39;help&#39;)\n\nif __name__==&#39;__main__&#39;:\n    import fire\n    fire.Fire()</code></pre>\n<p>根据fire的使用方法，可通过<code>python main.py &lt;function&gt; --args=xx</code>的方式来执行训练或者测试。</p>\n<h4 class=\"my-blog-head\" id=\"my-blog-head22\">训练</h4><p>训练的主要步骤如下：</p>\n<ul>\n<li>定义网络</li>\n<li>定义数据</li>\n<li>定义损失函数和优化器</li>\n<li>计算重要指标</li>\n<li>开始训练<ul>\n<li>训练网络</li>\n<li>可视化各种指标</li>\n<li>计算在验证集上的指标</li>\n</ul>\n</li>\n</ul>\n<p>训练函数的代码如下：</p>\n<pre><code class=\"language-python\">def train(**kwargs):\n\n    # 根据命令行参数更新配置\n    opt.parse(kwargs)\n    vis = Visualizer(opt.env)\n\n    # step1: 模型\n    model = getattr(models, opt.model)()\n    if opt.load_model_path:\n        model.load(opt.load_model_path)\n    if opt.use_gpu: model.cuda()\n\n    # step2: 数据\n    train_data = DogCat(opt.train_data_root,train=True)\n    val_data = DogCat(opt.train_data_root,train=False)\n    train_dataloader = DataLoader(train_data,opt.batch_size,\n                        shuffle=True,\n                        num_workers=opt.num_workers)\n    val_dataloader = DataLoader(val_data,opt.batch_size,\n                        shuffle=False,\n                        num_workers=opt.num_workers)\n\n    # step3: 目标函数和优化器\n    criterion = t.nn.CrossEntropyLoss()\n    lr = opt.lr\n    optimizer = t.optim.Adam(model.parameters(),\n                            lr = lr,\n                            weight_decay = opt.weight_decay)\n\n    # step4: 统计指标：平滑处理之后的损失，还有混淆矩阵\n    loss_meter = meter.AverageValueMeter()\n    confusion_matrix = meter.ConfusionMeter(2)\n    previous_loss = 1e100\n\n    # 训练\n    for epoch in range(opt.max_epoch):\n\n        loss_meter.reset()\n        confusion_matrix.reset()\n\n        for ii,(data,label) in enumerate(train_dataloader):\n\n            # 训练模型参数 \n            input = Variable(data)\n            target = Variable(label)\n            if opt.use_gpu:\n                input = input.cuda()\n                target = target.cuda()\n            optimizer.zero_grad()\n            score = model(input)\n            loss = criterion(score,target)\n            loss.backward()\n            optimizer.step()\n\n            # 更新统计指标以及可视化\n            loss_meter.add(loss.data[0])\n            confusion_matrix.add(score.data, target.data)\n\n            if ii%opt.print_freq==opt.print_freq-1:\n                vis.plot(&#39;loss&#39;, loss_meter.value()[0])\n\n                # 如果需要的话，进入debug模式\n                if os.path.exists(opt.debug_file):\n                    import ipdb;\n                    ipdb.set_trace()\n\n        model.save()\n\n        # 计算验证集上的指标及可视化\n        val_cm,val_accuracy = val(model,val_dataloader)\n        vis.plot(&#39;val_accuracy&#39;,val_accuracy)\n        vis.log(&quot;epoch:{epoch},lr:{lr},loss:{loss},train_cm:{train_cm},val_cm:{val_cm}&quot;\n        .format(\n                    epoch = epoch,\n                    loss = loss_meter.value()[0],\n                    val_cm = str(val_cm.value()),\n                    train_cm=str(confusion_matrix.value()),\n                    lr=lr))\n\n        # 如果损失不再下降，则降低学习率\n        if loss_meter.value()[0] &gt; previous_loss:          \n            lr = lr * opt.lr_decay\n            for param_group in optimizer.param_groups:\n                param_group[&#39;lr&#39;] = lr\n\n        previous_loss = loss_meter.value()[0]</code></pre>\n<p>这里用到了PyTorchNet<a href=\"https://github.com/pytorch/tnt\">^5</a>里面的一个工具: meter。meter提供了一些轻量级的工具，用于帮助用户快速统计训练过程中的一些指标。<code>AverageValueMeter</code>能够计算所有数的平均值和标准差，这里用来统计一个epoch中损失的平均值。<code>confusionmeter</code>用来统计分类问题中的分类情况，是一个比准确率更详细的统计指标。例如对于表格6-1，共有50张狗的图片，其中有35张被正确分类成了狗，还有15张被误判成猫；共有100张猫的图片，其中有91张被正确判为了猫，剩下9张被误判成狗。相比于准确率等统计信息，混淆矩阵更能体现分类的结果，尤其是在样本比例不均衡的情况下。</p>\n<p>表6-1 混淆矩阵</p>\n<table>\n<thead>\n<tr>\n<th>样本</th>\n<th>判为狗</th>\n<th>判为猫</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>实际是狗</td>\n<td>35</td>\n<td>15</td>\n</tr>\n<tr>\n<td>实际是猫</td>\n<td>9</td>\n<td>91</td>\n</tr>\n</tbody></table>\n<p>PyTorchNet从TorchNet<a href=\"https://github.com/torchnet/torchnet\">^6</a>迁移而来，提供了很多有用的工具，但其目前开发和文档都还不是很完善，本书不做过多的讲解。</p>\n<h4 class=\"my-blog-head\" id=\"my-blog-head23\">验证</h4><p>验证相对来说比较简单，但要注意需将模型置于验证模式(<code>model.eval()</code>)，验证完成后还需要将其置回为训练模式(<code>model.train()</code>)，这两句代码会影响<code>BatchNorm</code>和<code>Dropout</code>等层的运行模式。验证模型准确率的代码如下。</p>\n<pre><code class=\"language-python\">def val(model,dataloader):\n    &#39;&#39;&#39;\n    计算模型在验证集上的准确率等信息\n    &#39;&#39;&#39;\n\n    # 把模型设为验证模式\n    model.eval()\n\n    confusion_matrix = meter.ConfusionMeter(2)\n    for ii, data in enumerate(dataloader):\n        input, label = data\n        val_input = Variable(input, volatile=True)\n        val_label = Variable(label.long(), volatile=True)\n        if opt.use_gpu:\n            val_input = val_input.cuda()\n            val_label = val_label.cuda()\n        score = model(val_input)\n        confusion_matrix.add(score.data.squeeze(), label.long())\n\n    # 把模型恢复为训练模式\n    model.train()\n\n    cm_value = confusion_matrix.value()\n    accuracy = 100. * (cm_value[0][0] + cm_value[1][1]) /\\\n                 (cm_value.sum())\n    return confusion_matrix, accuracy</code></pre>\n<h4 class=\"my-blog-head\" id=\"my-blog-head24\">测试</h4><p>测试时，需要计算每个样本属于狗的概率，并将结果保存成csv文件。测试的代码与验证比较相似，但需要自己加载模型和数据。</p>\n<pre><code class=\"language-python\">def test(**kwargs):\n    opt.parse(kwargs)\n\n    # 模型\n    model = getattr(models, opt.model)().eval()\n    if opt.load_model_path:\n        model.load(opt.load_model_path)\n    if opt.use_gpu: model.cuda()\n\n    # 数据\n    train_data = DogCat(opt.test_data_root,test=True)\n    test_dataloader = DataLoader(train_data,\\\n                                batch_size=opt.batch_size,\\\n                                shuffle=False,\\\n                                num_workers=opt.num_workers)\n\n    results = []\n    for ii,(data,path) in enumerate(test_dataloader):\n        input = t.autograd.Variable(data,volatile = True)\n        if opt.use_gpu: input = input.cuda()\n        score = model(input)\n        probability = t.nn.functional.softmax\\\n            (score)[:,1].data.tolist()      \n        batch_results = [(path_,probability_) \\\n            for path_,probability_ in zip(path,probability) ]\n        results += batch_results\n    write_csv(results,opt.result_file)\n    return results</code></pre>\n<h4 class=\"my-blog-head\" id=\"my-blog-head25\">帮助函数</h4><p>为了方便他人使用, 程序中还应当提供一个帮助函数，用于说明函数是如何使用。程序的命令行接口中有众多参数，如果手动用字符串表示不仅复杂，而且后期修改config文件时，还需要修改对应的帮助信息，十分不便。这里使用了Python标准库中的inspect方法，可以自动获取config的源代码。help的代码如下:</p>\n<pre><code class=\"language-python\">def help():\n    &#39;&#39;&#39;\n    打印帮助的信息： python file.py help\n    &#39;&#39;&#39;\n\n    print(&#39;&#39;&#39;\n    usage : python {0} &lt;function&gt; [--args=value,]\n    &lt;function&gt; := train | test | help\n    example: \n            python {0} train --env=&#39;env0701&#39; --lr=0.01\n            python {0} test --dataset=&#39;path/to/dataset/root/&#39;\n            python {0} help\n    avaiable args:&#39;&#39;&#39;.format(__file__))\n\n    from inspect import getsource\n    source = (getsource(opt.__class__))\n    print(source)</code></pre>\n<p>当用户执行<code>python main.py help</code>的时候，会打印如下帮助信息：</p>\n<pre><code class=\"language-bash\">    usage : python main.py &lt;function&gt; [--args=value,]\n    &lt;function&gt; := train | test | help\n    example: \n            python main.py train --env=&#39;env0701&#39; --lr=0.01\n            python main.py test --dataset=&#39;path/to/dataset/&#39;\n            python main.py help\n    avaiable args:\nclass DefaultConfig(object):\n    env = &#39;default&#39; # visdom 环境\n    model = &#39;AlexNet&#39; # 使用的模型\n\n    train_data_root = &#39;./data/train/&#39; # 训练集存放路径\n    test_data_root = &#39;./data/test1&#39; # 测试集存放路径\n    load_model_path = &#39;checkpoints/model.pth&#39; # 加载预训练的模型\n\n    batch_size = 128 # batch size\n    use_gpu = True # user GPU or not\n    num_workers = 4 # how many workers for loading data\n    print_freq = 20 # print info every N batch\n\n    debug_file = &#39;/tmp/debug&#39; \n    result_file = &#39;result.csv&#39; # 结果文件\n\n    max_epoch = 10\n    lr = 0.1 # initial learning rate\n    lr_decay = 0.95 # when val_loss increase, lr = lr*lr_decay\n    weight_decay = 1e-4 # 损失函数</code></pre>\n<h3 class=\"my-blog-head\" id=\"my-blog-head26\">6.1.8 使用</h3><p>正如<code>help</code>函数的打印信息所述，可以通过命令行参数指定变量名.下面是三个使用例子，fire会将包含<code>-</code>的命令行参数自动转层下划线<code>_</code>，也会将非数值的值转成字符串。所以<code>--train-data-root=data/train</code>和<code>--train_data_root=&#39;data/train&#39;</code>是等价的。</p>\n<pre><code># 训练模型\npython main.py train \n        --train-data-root=data/train/ \n        --load-model-path=&#39;checkpoints/resnet34_16:53:00.pth&#39; \n        --lr=0.005 \n        --batch-size=32 \n        --model=&#39;ResNet34&#39;  \n        --max-epoch = 20\n\n# 测试模型\npython main.py test\n       --test-data-root=data/test1 \n       --load-model-path=&#39;checkpoints/resnet34_00:23:05.pth&#39; \n       --batch-size=128 \n       --model=&#39;ResNet34&#39; \n       --num-workers=12\n\n# 打印帮助信息\npython main.py help</code></pre><h3 class=\"my-blog-head\" id=\"my-blog-head27\">6.1.9 争议</h3><p>以上的程序设计规范带有作者强烈的个人喜好，并不想作为一个标准，而是作为一个提议和一种参考。上述设计在很多地方还有待商榷，例如对于训练过程是否应该封装成一个<code>trainer</code>对象，或者直接封装到<code>BaiscModule</code>的<code>train</code>方法之中。对命令行参数的处理也有不少值得讨论之处。因此不要将本文中的观点作为一个必须遵守的规范，而应该看作一个参考。</p>\n<p>本章中的设计可能会引起不少争议，其中比较值得商榷的部分主要有以下两个方面：</p>\n<ul>\n<li>命令行参数的设置。目前大多数程序都是使用Python标准库中的<code>argparse</code>来处理命令行参数，也有些使用比较轻量级的<code>click</code>。这种处理相对来说对命令行的支持更完备，但根据作者的经验来看，这种做法不够直观，并且代码量相对来说也较多。比如<code>argparse</code>，每次增加一个命令行参数，都必须写如下代码：</li>\n</ul>\n<pre><code class=\"language-python\">parser.add_argument(&#39;-save-interval&#39;, type=int, default=500, help=&#39;how many steps to wait before saving [default:500]&#39;)</code></pre>\n<p>在读者眼中，这种实现方式远不如一个专门的<code>config.py</code>来的直观和易用。尤其是对于使用Jupyter notebook或IPython等交互式调试的用户来说，<code>argparse</code>较难使用。</p>\n<ul>\n<li>模型训练。有不少人喜欢将模型的训练过程集成于模型的定义之中，代码结构如下所示：</li>\n</ul>\n<pre><code class=\"language-python\">  class MyModel(nn.Module):\n\n      def __init__(self,opt):\n          self.dataloader = Dataloader(opt)\n          self.optimizer  = optim.Adam(self.parameters(),lr=0.001)\n          self.lr = opt.lr\n          self.model = make_model()\n\n      def forward(self,input):\n          pass\n\n      def train_(self):\n          # 训练模型\n          for epoch in range(opt.max_epoch)\n              for ii,data in enumerate(self.dataloader):\n                  train_epoch()\n\n              model.save()\n\n      def train_epoch(self):\n          pass</code></pre>\n<p>抑或是专门设计一个<code>Trainer</code>对象，形如：</p>\n<pre><code class=\"language-python\">    &#39;&#39;&#39;\n  code simplified from:\n  https://github.com/pytorch/pytorch/blob/master/torch/utils/trainer/trainer.py\n  &#39;&#39;&#39;\n  import heapq\n  from torch.autograd import Variable\n\n  class Trainer(object):\n\n      def __init__(self, model=None, criterion=None, optimizer=None, dataset=None):\n          self.model = model\n          self.criterion = criterion\n          self.optimizer = optimizer\n          self.dataset = dataset\n          self.iterations = 0\n\n      def run(self, epochs=1):\n          for i in range(1, epochs + 1):\n              self.train()\n\n      def train(self):\n          for i, data in enumerate(self.dataset, self.iterations + 1):\n              batch_input, batch_target = data\n              self.call_plugins(&#39;batch&#39;, i, batch_input, batch_target)\n              input_var = Variable(batch_input)\n              target_var = Variable(batch_target)\n\n              plugin_data = [None, None]\n\n              def closure():\n                  batch_output = self.model(input_var)\n                  loss = self.criterion(batch_output, target_var)\n                  loss.backward()\n                  if plugin_data[0] is None:\n                      plugin_data[0] = batch_output.data\n                      plugin_data[1] = loss.data\n                  return loss\n\n              self.optimizer.zero_grad()\n              self.optimizer.step(closure)\n\n          self.iterations += i</code></pre>\n<p>还有一些人喜欢模仿keras和scikit-learn的设计，设计一个<code>fit</code>接口。对读者来说，这些处理方式很难说哪个更好或更差，找到最适合自己的方法才是最好的。</p>\n<ul>\n<li><code>BasicModule</code> 的封装，可多可少。训练过程中的很多操作都可以移到<code>BasicModule</code>之中，比如<code>get_optimizer</code>方法用来获取优化器，比如<code>train_step</code>用来执行单歩训练。对于不同的模型，如果对应的优化器定义不一样，或者是训练方法不一样，可以复写这些函数自定义相应的方法，取决于自己的喜好和项目的实际需求。   </li>\n</ul>\n','','pytorch入门资料',0,_binary '\0'),(6,'4fDo0bfqLHlXvT90PdVqNq','如何优美的将操作系统装进移动硬盘','bkU6nGaHJrggRmNfUkQuU',1550837695,NULL,1550837785,1550837695,0,'## 1、Windows10 装进移动硬盘\n    \n软件公司在Windows8的时候发布了Windows to go简称wtg,能够方便的将Windows操作系统安装进可移动介质，包括移动硬盘，U盘和移动固态硬盘。但是这些移动存储介质必须是有微软认证过的厂商生产的设备，认证过的设备价格上还是比较高的，所以需要使用普通的移动硬盘安装wtg可以使用一些三方的wtg安装工具，例如:[WTG辅助工具 v4.8.2](https://bbs.luobotou.org/thread-761-1-1.html)\n    \n**WTG辅助工具**\n1、准备Windows iso镜像、wtg辅助工具\n2、选择好镜像、可移动磁盘、引导类别，点击创建就可以了\n![](https://i.imgur.com/F5SVlYe.png)\n3、打开电脑的boot选择界面选择移动硬盘的引导就可以进入系统了，第一次进去需要安装系统，安装完成就可以正常使用了。\n    \n**另一种工具dism++**\n需要制作WTG系统映像\nUltraISO等虚拟光盘映像软件\n方法/步骤\n1、将系统映像加载的虚拟光驱中\n![](https://i.imgur.com/E3CfWpB.png)\n2、打开dism++,进入常用工具下的工具箱，单机系统还原，跳出对话框如下\n![](https://i.imgur.com/yKk4kWh.png)\n![](https://i.imgur.com/d4m33iO.png)\n\n3、在目标镜像下第一个浏览中选择系统镜像下的sources>install.wim,然后再目标映像下选择相应的操作系统。在第二个浏览选择中选择想要制造WTG的U盘，然后勾选WTG,添加引导和格式化，最后确认。\n![](https://i.imgur.com/Q3Hk5Rz.png)\n![](https://i.imgur.com/jVl32qd.png)\n![](https://i.imgur.com/2cBFdr9.png)\n\n4、最后等待拷贝进度，释放完成后重启电脑，按F12，选择U盘启动，进入WTG恭喜制造成功。\n    \n\n## 2、ubuntu/Linux mint to go\n1、准备工具\nVMware Workstation Pro、ubuntu或者Linux mint镜像，其他Linux镜像可能也行，我没有尝试过\n2、使用VMware创建自定义一个虚拟机\n![](https://i.imgur.com/ktUcfOm.png)\n选择稍后安装操作系统\n![](https://i.imgur.com/N2sWoGV.png)\n选择Linux 64位\n![](https://i.imgur.com/9stuFGD.png)\n建议内存2G\n![](https://i.imgur.com/WoZ4a6Q.png)\n网络类型建议选择桥接，如果桥接不能连上网那就试试nat\n![](https://i.imgur.com/q7IrkOg.png)\n\n接下来的磁盘设置就比较重要了。这是决定你的系统安装位置的设置项。I/O 控制器种类选择 LSI Logic，磁盘类型选择 SCSI，磁盘选择「使用物理磁盘」。然后要选择物理设备。这里必须小心谨慎一点，不要像我一样一不小心就把我笔记本的板载存储给格了。这里我的U盘是被识别为了PhysicalDrive 1，在你的电脑上可能会有所不同。使用情况选择「使用整个磁盘」。\n![](https://i.imgur.com/soacaVQ.png)\n![](https://i.imgur.com/eYEdfTv.png)\n\n在虚拟机即将创建完成，确定虚拟机配置的时候，点击「自定义配置」，在「新 CD/DVD」配置项中选择「使用 ISO 映像文件」并选择你下载好的 Ubuntu 系统安装镜像。\n![](https://i.imgur.com/iftJe6I.png)\n\n\n3、启动虚拟机，选择进入电源固件，进入BIOS调整启动顺序，把光驱调到硬盘之前。\n![](https://i.imgur.com/UoTfd7r.png)\n![](https://i.imgur.com/GsHaKaz.png)\nF10 保存退出。重启后即可看到熟悉的 Ubuntu 安装界面。\n![](https://i.imgur.com/tYaM49Y.png)\n\n这个时候注意下，因为待会儿要搞事情，所以这里要选择「试用 Ubuntu」，在 LiveCD 中操作。\n\n进入 LiveCD 后先点击右上角，选择「系统设置」，然后「显示」，把分辨率调整到一个合适的大小，否则因为 VMware 这蛋疼的监视器特性，安装时你八成不会看到完整的分区界面。选择合适的分辨率后拖动窗口，右下角的按钮就是「应用」。\n![](https://i.imgur.com/SCtJdbE.png)\n\n然后就可以双击桌面上的「安装 Ubuntu」启动安装向导了，网不好的可以不勾选更新。\n![](https://i.imgur.com/fdysTeW.png)\n\n一路 Next 下去，不过在选择安装类型的时候要选「其他选项」。在分区的地方停顿一下。分区还是比较重要的。\n\n:::info\n分区说明\n一般来说全新的移动硬盘都会有一个 FAT32/NTFS/exFAT 分区，把这个分区删掉。在磁盘前部建立一个大小为 200MB 的 FAT32 分区作为 EFI 分区，必须是主分区，挂载点为 /boot/efi。然后建立根分区（挂载点为 /），这个分区也必须是主分区。你可以按你的喜好来配置根分区，我的建议是分区大小不小于 10GB，分区格式为 EXT4。最好还分配一个BIOS分区，大小100M就好，你也可以按照个人喜好给 /usr，/home，/var，/tmp 等目录单独划分分区。如果你的硬盘盘容量比较大，可以在磁盘后部留下一点空闲的空间，安装完成后把这部分空间单独划分出一个 NTFS 分区来，你的 Ubuntu To Go 系统盘还能继续当移动硬盘使,不理解的可以查一下Linux分区方案。\n:::\n\n在这停顿！在点击「现在安装」之前，你必须先确认一下你的分区配置是否无误。虽然后面还有 3 个步骤，但是你不能返回重新配置。\n如果你确定你做好了分区，那么就点击「现在安装」进行下一步的配置。一般来说也是一路 Next 下去，设置一下个人信息就好了。\n![](https://i.imgur.com/E6PARvi.png)\n\n![](https://i.imgur.com/sNIcHKU.png)\n\n\n在点击「继续」前，你还有最后一次检查设置的机会。如果确认设置无误，点击「继续」即可正式开始安装。因为 U 盘的读写性能比较低而且虚拟机有性能损耗，安装过程会比较长，不过一般都会在一个小时内结束。\n\n![](https://i.imgur.com/sQJWjz0.png)\n\n\n注意一下，因为要调整虚拟机设置，所以安装完成后不要点击「现在重启」，而是点击「继续试用」然后通过右上角关机。\n\n关机后打开虚拟机设置，将「选项」选项卡中「高级」设置项中的「通过 EFI 而非 BIOS 引导」选中，然后点击「确定」。\n![](https://i.imgur.com/P4knOpg.png)\n\n\n再次打开虚拟机进入 LiveCD。这次不是直接进入 GUI 选择引导了，而是有一个引导菜单。选中「Try Ubuntu without installing」并按下回车即可进入 LiveCD。然后打开终端，输入以下指令获得 root 权限。\n\nsudo -s\n然后输入以下指令查看分区情况。\n\nfdisk -l\n从下图可以看到在我这 /dev/sda1 是格式为 FAT32 的 EFI 分区，/dev/sda2 是格式为 EXT4 的根分区。\n![](https://i.imgur.com/EdNP9Ge.png)\n\n\n执行如下指令挂载分区。\n\n我这里的分区情况和你那儿的可能不同，请根据你的实际分区情况操作。挂载分区时，应先挂载根分区，再挂载其他分区。\n```\nmount /dev/sda2 /mnt\nmount /dev/sda1 /mnt/boot/efi\n#如果你有划分其他分区，也需要使用 mount 指令挂载它们。如下指令为挂载 /dev/sda3 为 /home。\n#mount /dev/sda3 /mnt/home\n```\n\n\n然后是著名的 dev，proc，sys，run 四部曲。\n```\nmount --o bind /dev /mnt/dev\nmount --o bind /proc /mnt/proc\nmount --o bind /sys /mnt/sys\nmount --o bind /run /mnt/run\n```\n\n都挂载完成后，执行如下指令进入 chroot 环境。\n\n```\nchroot /mnt\n```\n因为我们在一开始安装 Ubuntu 的时候使用的是 Legacy BIOS 引导而非 EFI，所以现在的 To Go 系统盘里的 Ubuntu 里的 grub 是不支持 EFI 启动的。执行如下指令安装 grub-efi。\n\n```\napt-get install grub-efi -y\n```\n\ngrub-efi 安装完成后，使用如下指令安装引导。\n```\ngrub-install --target=x86_64-efi --root-directory=/boot/efi --efi-directory=/boot/efi\n```\n会出现一些你可能看不懂的奇♂怪的东西，但是管他呢，只要最后不报错就好。\n\n结束之后，执行如下指令完成安装并退出 chroot 环境。\n```\numount /proc\numount /dev\numount /sys\numount /run\nsync\nexit\n```\n终端先不要关，事情还没搞完。执行如下指令以避免安装完成后出现一些问题。\n\n```\ncd /mnt/boot\ncp grub/grub.cfg efi/boot/grub/\nmv efi/EFI/ubuntu efi/EFI/boot\nmv efi/EFI/boot/grubx64.efi efi/EFI/boot/bootx64.efi\n```\n\n重启即可看到，已经引导到 Ubuntu To Go 启动而不是光驱了。你可以再次关机，更改虚拟机设置中的「通过 EFI 而非 BIOS 引导」设置项来分别测试在 Legacy BIOS 和 EFI 下的运行情况。你也可以关闭物理机，然后在 BIOS 中将第一启动设备设为 Ubuntu To Go 系统盘，即可在实体机上运行 Ubuntu To Go 系统盘中的 Ubuntu。\n\n\n\n\n\n\n\n\n\n\n\n\n\n','<h2 class=\"my-blog-head\" id=\"my-blog-head30\">1、Windows10 装进移动硬盘</h2><p>软件公司在Windows8的时候发布了Windows to go简称wtg,能够方便的将Windows操作系统安装进可移动介质，包括移动硬盘，U盘和移动固态硬盘。但是这些移动存储介质必须是有微软认证过的厂商生产的设备，认证过的设备价格上还是比较高的，所以需要使用普通的移动硬盘安装wtg可以使用一些三方的wtg安装工具，例如:<a href=\"https://bbs.luobotou.org/thread-761-1-1.html\">WTG辅助工具 v4.8.2</a></p>\n<p><strong>WTG辅助工具</strong><br>1、准备Windows iso镜像、wtg辅助工具<br>2、选择好镜像、可移动磁盘、引导类别，点击创建就可以了<br><img src=\"https://i.imgur.com/F5SVlYe.png\" alt=\"\"><br>3、打开电脑的boot选择界面选择移动硬盘的引导就可以进入系统了，第一次进去需要安装系统，安装完成就可以正常使用了。</p>\n<p><strong>另一种工具dism++</strong><br>需要制作WTG系统映像<br>UltraISO等虚拟光盘映像软件<br>方法/步骤<br>1、将系统映像加载的虚拟光驱中<br><img src=\"https://i.imgur.com/E3CfWpB.png\" alt=\"\"><br>2、打开dism++,进入常用工具下的工具箱，单机系统还原，跳出对话框如下<br><img src=\"https://i.imgur.com/yKk4kWh.png\" alt=\"\"><br><img src=\"https://i.imgur.com/d4m33iO.png\" alt=\"\"></p>\n<p>3、在目标镜像下第一个浏览中选择系统镜像下的sources&gt;install.wim,然后再目标映像下选择相应的操作系统。在第二个浏览选择中选择想要制造WTG的U盘，然后勾选WTG,添加引导和格式化，最后确认。<br><img src=\"https://i.imgur.com/Q3Hk5Rz.png\" alt=\"\"><br><img src=\"https://i.imgur.com/jVl32qd.png\" alt=\"\"><br><img src=\"https://i.imgur.com/2cBFdr9.png\" alt=\"\"></p>\n<p>4、最后等待拷贝进度，释放完成后重启电脑，按F12，选择U盘启动，进入WTG恭喜制造成功。</p>\n<h2 class=\"my-blog-head\" id=\"my-blog-head31\">2、ubuntu/Linux mint to go</h2><p>1、准备工具<br>VMware Workstation Pro、ubuntu或者Linux mint镜像，其他Linux镜像可能也行，我没有尝试过<br>2、使用VMware创建自定义一个虚拟机<br><img src=\"https://i.imgur.com/ktUcfOm.png\" alt=\"\"><br>选择稍后安装操作系统<br><img src=\"https://i.imgur.com/N2sWoGV.png\" alt=\"\"><br>选择Linux 64位<br><img src=\"https://i.imgur.com/9stuFGD.png\" alt=\"\"><br>建议内存2G<br><img src=\"https://i.imgur.com/WoZ4a6Q.png\" alt=\"\"><br>网络类型建议选择桥接，如果桥接不能连上网那就试试nat<br><img src=\"https://i.imgur.com/q7IrkOg.png\" alt=\"\"></p>\n<p>接下来的磁盘设置就比较重要了。这是决定你的系统安装位置的设置项。I/O 控制器种类选择 LSI Logic，磁盘类型选择 SCSI，磁盘选择「使用物理磁盘」。然后要选择物理设备。这里必须小心谨慎一点，不要像我一样一不小心就把我笔记本的板载存储给格了。这里我的U盘是被识别为了PhysicalDrive 1，在你的电脑上可能会有所不同。使用情况选择「使用整个磁盘」。<br><img src=\"https://i.imgur.com/soacaVQ.png\" alt=\"\"><br><img src=\"https://i.imgur.com/eYEdfTv.png\" alt=\"\"></p>\n<p>在虚拟机即将创建完成，确定虚拟机配置的时候，点击「自定义配置」，在「新 CD/DVD」配置项中选择「使用 ISO 映像文件」并选择你下载好的 Ubuntu 系统安装镜像。<br><img src=\"https://i.imgur.com/iftJe6I.png\" alt=\"\"></p>\n<p>3、启动虚拟机，选择进入电源固件，进入BIOS调整启动顺序，把光驱调到硬盘之前。<br><img src=\"https://i.imgur.com/UoTfd7r.png\" alt=\"\"><br><img src=\"https://i.imgur.com/GsHaKaz.png\" alt=\"\"><br>F10 保存退出。重启后即可看到熟悉的 Ubuntu 安装界面。<br><img src=\"https://i.imgur.com/tYaM49Y.png\" alt=\"\"></p>\n<p>这个时候注意下，因为待会儿要搞事情，所以这里要选择「试用 Ubuntu」，在 LiveCD 中操作。</p>\n<p>进入 LiveCD 后先点击右上角，选择「系统设置」，然后「显示」，把分辨率调整到一个合适的大小，否则因为 VMware 这蛋疼的监视器特性，安装时你八成不会看到完整的分区界面。选择合适的分辨率后拖动窗口，右下角的按钮就是「应用」。<br><img src=\"https://i.imgur.com/SCtJdbE.png\" alt=\"\"></p>\n<p>然后就可以双击桌面上的「安装 Ubuntu」启动安装向导了，网不好的可以不勾选更新。<br><img src=\"https://i.imgur.com/fdysTeW.png\" alt=\"\"></p>\n<p>一路 Next 下去，不过在选择安装类型的时候要选「其他选项」。在分区的地方停顿一下。分区还是比较重要的。</p>\n<p>:::info<br>分区说明<br>一般来说全新的移动硬盘都会有一个 FAT32/NTFS/exFAT 分区，把这个分区删掉。在磁盘前部建立一个大小为 200MB 的 FAT32 分区作为 EFI 分区，必须是主分区，挂载点为 /boot/efi。然后建立根分区（挂载点为 /），这个分区也必须是主分区。你可以按你的喜好来配置根分区，我的建议是分区大小不小于 10GB，分区格式为 EXT4。最好还分配一个BIOS分区，大小100M就好，你也可以按照个人喜好给 /usr，/home，/var，/tmp 等目录单独划分分区。如果你的硬盘盘容量比较大，可以在磁盘后部留下一点空闲的空间，安装完成后把这部分空间单独划分出一个 NTFS 分区来，你的 Ubuntu To Go 系统盘还能继续当移动硬盘使,不理解的可以查一下Linux分区方案。<br>:::</p>\n<p>在这停顿！在点击「现在安装」之前，你必须先确认一下你的分区配置是否无误。虽然后面还有 3 个步骤，但是你不能返回重新配置。<br>如果你确定你做好了分区，那么就点击「现在安装」进行下一步的配置。一般来说也是一路 Next 下去，设置一下个人信息就好了。<br><img src=\"https://i.imgur.com/E6PARvi.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/sNIcHKU.png\" alt=\"\"></p>\n<p>在点击「继续」前，你还有最后一次检查设置的机会。如果确认设置无误，点击「继续」即可正式开始安装。因为 U 盘的读写性能比较低而且虚拟机有性能损耗，安装过程会比较长，不过一般都会在一个小时内结束。</p>\n<p><img src=\"https://i.imgur.com/sQJWjz0.png\" alt=\"\"></p>\n<p>注意一下，因为要调整虚拟机设置，所以安装完成后不要点击「现在重启」，而是点击「继续试用」然后通过右上角关机。</p>\n<p>关机后打开虚拟机设置，将「选项」选项卡中「高级」设置项中的「通过 EFI 而非 BIOS 引导」选中，然后点击「确定」。<br><img src=\"https://i.imgur.com/P4knOpg.png\" alt=\"\"></p>\n<p>再次打开虚拟机进入 LiveCD。这次不是直接进入 GUI 选择引导了，而是有一个引导菜单。选中「Try Ubuntu without installing」并按下回车即可进入 LiveCD。然后打开终端，输入以下指令获得 root 权限。</p>\n<p>sudo -s<br>然后输入以下指令查看分区情况。</p>\n<p>fdisk -l<br>从下图可以看到在我这 /dev/sda1 是格式为 FAT32 的 EFI 分区，/dev/sda2 是格式为 EXT4 的根分区。<br><img src=\"https://i.imgur.com/EdNP9Ge.png\" alt=\"\"></p>\n<p>执行如下指令挂载分区。</p>\n<p>我这里的分区情况和你那儿的可能不同，请根据你的实际分区情况操作。挂载分区时，应先挂载根分区，再挂载其他分区。</p>\n<pre><code>mount /dev/sda2 /mnt\nmount /dev/sda1 /mnt/boot/efi\n#如果你有划分其他分区，也需要使用 mount 指令挂载它们。如下指令为挂载 /dev/sda3 为 /home。\n#mount /dev/sda3 /mnt/home</code></pre><p>然后是著名的 dev，proc，sys，run 四部曲。</p>\n<pre><code>mount --o bind /dev /mnt/dev\nmount --o bind /proc /mnt/proc\nmount --o bind /sys /mnt/sys\nmount --o bind /run /mnt/run</code></pre><p>都挂载完成后，执行如下指令进入 chroot 环境。</p>\n<pre><code>chroot /mnt</code></pre><p>因为我们在一开始安装 Ubuntu 的时候使用的是 Legacy BIOS 引导而非 EFI，所以现在的 To Go 系统盘里的 Ubuntu 里的 grub 是不支持 EFI 启动的。执行如下指令安装 grub-efi。</p>\n<pre><code>apt-get install grub-efi -y</code></pre><p>grub-efi 安装完成后，使用如下指令安装引导。</p>\n<pre><code>grub-install --target=x86_64-efi --root-directory=/boot/efi --efi-directory=/boot/efi</code></pre><p>会出现一些你可能看不懂的奇♂怪的东西，但是管他呢，只要最后不报错就好。</p>\n<p>结束之后，执行如下指令完成安装并退出 chroot 环境。</p>\n<pre><code>umount /proc\numount /dev\numount /sys\numount /run\nsync\nexit</code></pre><p>终端先不要关，事情还没搞完。执行如下指令以避免安装完成后出现一些问题。</p>\n<pre><code>cd /mnt/boot\ncp grub/grub.cfg efi/boot/grub/\nmv efi/EFI/ubuntu efi/EFI/boot\nmv efi/EFI/boot/grubx64.efi efi/EFI/boot/bootx64.efi</code></pre><p>重启即可看到，已经引导到 Ubuntu To Go 启动而不是光驱了。你可以再次关机，更改虚拟机设置中的「通过 EFI 而非 BIOS 引导」设置项来分别测试在 Legacy BIOS 和 EFI 下的运行情况。你也可以关闭物理机，然后在 BIOS 中将第一启动设备设为 Ubuntu To Go 系统盘，即可在实体机上运行 Ubuntu To Go 系统盘中的 Ubuntu。</p>\n','','准备自己的移动操作系统',1,_binary '\0'),(7,'1NZhVF4ktYzGhvFSq2ZDKm','Linux分区解决方案','bkU6nGaHJrggRmNfUkQuU',1550837866,NULL,1550837866,1550837866,0,'## 菜鸟方案\n\n“/”与swap两个分区就可以应付绝大多数的应用\n\n## 常用方案\n\n分为3个区\n\n1. 挂载点/；主分区；安装系统和软件；大小为30G；分区格式为ext4； \n2. 挂载点/home；逻辑分区；相当于“我的文档”；大小为硬盘剩下的; 分区格式ext4； \n3. swap；逻辑分区；充当虚拟内存；大小等于内存大小（本人2G）；分区格式为swap \n4. /boot ；引导分区；逻辑分区； 大小为200M ；分区格式为ext4；\n\n> Ps：（本人安装的是Ubuntu14.04版本，100G硬盘分区方案） \n> +按钮与-按钮用以添加和删除分区 ,change改变分区属性（分区格式和挂载点）\n\n## 进阶方案\n\n> 因为Linux的文件系统是一种树状的结构，一个软件会把包含的众多文件，放置在不同的目录当中，所以不同的使用目的，每个目录扩张的速度会不一样。例如，当计算机当作服务器使用时，由于变动的文件一般是位于/var当中，所以/var应规划足够的空间；当计算机当作日常桌用应用时，/home会增加使用量，所以/home要放大空间；若计算机中会安装大量的软件，那你可能需要增加/usr的空间。 \n> 当然，不把这些空间独立分割出去，一起使用/的空间，会有最好的空间使用效率，但这样容易造成系统不稳定，或其它的问题的情形发生。前面提到Linux文件系统的根目录，是一定要挂载的。没有特定指定分区挂载的目录，都会与根目录放在同一个分区中。\n\n- 绝对不可以与根目录放在不同分区的目录： \n  /bin,/sbin, /lib, /etc, /dev 这五个目录。绝对不可与/所在的分区分开，因为这五个目录，有系统必要的工具与资料存放。当根目录在开机过程中被挂载进来时，需要这些工具与资料来维持正常的运作。若是把这五个目录放在其它分区当中，系统就不能正常引导。 \n  以上的目录，绝对不要额外挂载到其它分区上。\n- 不需要与根目录放在不同分区的目录： \n  如/cdrom,/mnt, /media, /proc, /run, /sys,/srv等。这些目录可以放到其它的分区，但不需要，因为这些目录的存在，只是运作过程中，维持运作所需，大多不会占用空间。放到其它分区，也无益于系统的性能。如/mnt,/media, /cdrom 只是提供一个挂载点，让实体存储媒体可以挂载而已；或如/sys,/proc其实是内存上的数据，上面所有的数据完全不会占用硬盘的空间。所以这些目录不需要额外的分区存放。 \n  在FHS的推荐当中提到，根目录所在的分区越小，越有助于系统的稳定，避免其它的干扰；发生错误时，也会比较容易进行维护修正；而且可以提高系统的性能。\n- 接下来是最好与根目录分开，到其它分区的目录： \n  所谓「最好」，是对整个作业系统的稳定而言，并非「必要」。如同我一开始所言，只要”/”存在，Linux系统即可运作。 \n  /home /var /usr三者必须思考哪些空间必须额外分割出来（参考后面各分区的作用）。当然就系统稳定来说，最好都分割出来。\n\n## 附：Linux各个分区的作用\n\n/ 根目录，建议在根目录下面只有目录，不要直接有文件。\n\nswap 交换空间，相当于Windows上的虚拟内存。\n\n/boot 包含了操作系统的内核和在启动系统过程中所要用到的文件，建这个分区是有必要的，因为目前大多数的PC机要受到BIOS的限制, 况且如果有了一个单独的/boot启动分区，即使主要的根分区出现了问题，计算机依然能够启动。这个分区的大小约在60MB—120MB之间。\n\n/home 用户的home目录所在地，这个分区的大小取决于有多少用户。如果是多用户共同使用一台电脑的话，这个分区是完全有必要的，况且根用户也可以很好地控制普通用户使用计算机，如对用户或者用户组实行硬盘限量使用，限制普通用户访问哪些文件等。 \n以往Linux系统主要是提供服务器使用，所以/home这个目录使用量并不高。但随著Linux的桌面应用发展，不少人也拿来在日常上使用，这时/home就变成存储媒体中，最占容量的目录。假如你安装Ubuntu主要是桌面应用，那你可能需要把最大的空间留给他。 \n额外分割出/home有个最大的好处，当你重新安装系统时，你不需要特别去备份你的个人文件，只要在安装时，选择不要格式化这个分区，重新挂载为/home就不会丢失你的数据。 \n还有一个特别的应用：假如你会在你的计算机上，安装两个或更多的Linux系统，你可以共享/home这个分区。简单地说，你的个人文件可以在切换到其它Linux系统时，仍正常使用\n\n/tmp 用来存放临时文件。这对于多用户系统或者网络服务器来说是有必要的。这样即使程序运行时生成大量的临时文件，或者用户对系统进行了错误的操作，文件系统的其它部分仍然是安全的。因为文件系统的这一部分仍然还承受着读写操作，所以它通常会比其它的部分更快地发生问题。这个目录是任何人都能访问的，所以需要定期清理。\n\n/usr Linux系统存放软件的地方，如有可能应将最大空间分给它 \n除了系统的基本程序外，其它所有的应用程序多放在这个目录当中。除了/home,/var这种变动数据的存放目录外，/usr大概是会是使用容量最大的目录，不过一般Linux下的应用程序通常不大，所以大多数的桌面应用顶多3~4GB的空间就已经相当足够了，若是服务器，多半也是2~3GB就足够了。\n\n/bin \n/usr/bin \n/usr/local/bin 存放标准系统实用程序。\n\n/srv 一些服务启动之后，这些服务所需要访问的数据目录，如WWW服务器需要的网页数据就可以放在/srv/www中。\n\n/etc 系统主要的设置文件几乎都放在这个目录内。\n\n/lib \n/usr/lib \n/usr/local/lib 系统使用的函数库的目录。\n\n/root 系统管理员的家目录。\n\n/lost+found 该目录在大多数情况下都是空的，但当实然停电或者非正常关机后，有些文件临时存入在此。\n\n/dev 设备文件，在Linux系统上，任何设备都以文件类型存放在这个目录中，如硬盘设备文件，软驱、光驱设备文件等。\n\n/mnt \n/media 挂载目录，用来临时挂载别的文件系统或者别的硬件设备（如光驱、软驱）。\n\n/opt 用于存储第三方软件的目录，不过我们还是习惯放在/usr/local下\n\n/proc 此目录信息是在内存中由系统自行产生的，存储了一些当前的进程ID号和CPU、内存的映射等，因为这个目录下的数据都在内存中，所以本身不占任何硬盘空间。\n\n/sbin \n/usr/sbin \n/usr/local/sbin 存放一些系统管理员才会用到的执行命令。\n\n/var 主要放置系统执行过程中经常变化的文件，例如缓存（cache）或者是随时更改的登录文件（log file）。 \n假如你的计算机主要是提供网页服务，或者是mysql数据库，那/var会大量增加，你最好能够把/var额外分割出来。与/home的概念类似，重新安装时，不要格式化，仍可保留原来的数据。 \n在服务器的应用时，数据的安全是相当重要的，额外分区对数据的安全也有所帮助。此外，/var/log是系统log档保存的位置，养成有问题就去找log的好习惯，有助于解决问题。所以这也加强了额外分区的重要性。当一个服务器出现系统问题，甚至毁损时，除了你的数据外，之前的系统纪录也相当重要，找出为什么系统会出问题，可以帮助管理器快速排除障碍。\n\n/var/log 系统日志记录分区，如果设立了这一单独的分区，这样即使系统的日志文件出现了问题，它们也不会影响到操作系统的主分区。','<h2 class=\"my-blog-head\" id=\"my-blog-head32\">菜鸟方案</h2><p>“/”与swap两个分区就可以应付绝大多数的应用</p>\n<h2 class=\"my-blog-head\" id=\"my-blog-head33\">常用方案</h2><p>分为3个区</p>\n<ol>\n<li>挂载点/；主分区；安装系统和软件；大小为30G；分区格式为ext4； </li>\n<li>挂载点/home；逻辑分区；相当于“我的文档”；大小为硬盘剩下的; 分区格式ext4； </li>\n<li>swap；逻辑分区；充当虚拟内存；大小等于内存大小（本人2G）；分区格式为swap </li>\n<li>/boot ；引导分区；逻辑分区； 大小为200M ；分区格式为ext4；</li>\n</ol>\n<blockquote>\n<p>Ps：（本人安装的是Ubuntu14.04版本，100G硬盘分区方案）<br>+按钮与-按钮用以添加和删除分区 ,change改变分区属性（分区格式和挂载点）</p>\n</blockquote>\n<h2 class=\"my-blog-head\" id=\"my-blog-head34\">进阶方案</h2><blockquote>\n<p>因为Linux的文件系统是一种树状的结构，一个软件会把包含的众多文件，放置在不同的目录当中，所以不同的使用目的，每个目录扩张的速度会不一样。例如，当计算机当作服务器使用时，由于变动的文件一般是位于/var当中，所以/var应规划足够的空间；当计算机当作日常桌用应用时，/home会增加使用量，所以/home要放大空间；若计算机中会安装大量的软件，那你可能需要增加/usr的空间。<br>当然，不把这些空间独立分割出去，一起使用/的空间，会有最好的空间使用效率，但这样容易造成系统不稳定，或其它的问题的情形发生。前面提到Linux文件系统的根目录，是一定要挂载的。没有特定指定分区挂载的目录，都会与根目录放在同一个分区中。</p>\n</blockquote>\n<ul>\n<li>绝对不可以与根目录放在不同分区的目录：<br>/bin,/sbin, /lib, /etc, /dev 这五个目录。绝对不可与/所在的分区分开，因为这五个目录，有系统必要的工具与资料存放。当根目录在开机过程中被挂载进来时，需要这些工具与资料来维持正常的运作。若是把这五个目录放在其它分区当中，系统就不能正常引导。<br>以上的目录，绝对不要额外挂载到其它分区上。</li>\n<li>不需要与根目录放在不同分区的目录：<br>如/cdrom,/mnt, /media, /proc, /run, /sys,/srv等。这些目录可以放到其它的分区，但不需要，因为这些目录的存在，只是运作过程中，维持运作所需，大多不会占用空间。放到其它分区，也无益于系统的性能。如/mnt,/media, /cdrom 只是提供一个挂载点，让实体存储媒体可以挂载而已；或如/sys,/proc其实是内存上的数据，上面所有的数据完全不会占用硬盘的空间。所以这些目录不需要额外的分区存放。<br>在FHS的推荐当中提到，根目录所在的分区越小，越有助于系统的稳定，避免其它的干扰；发生错误时，也会比较容易进行维护修正；而且可以提高系统的性能。</li>\n<li>接下来是最好与根目录分开，到其它分区的目录：<br>所谓「最好」，是对整个作业系统的稳定而言，并非「必要」。如同我一开始所言，只要”/”存在，Linux系统即可运作。<br>/home /var /usr三者必须思考哪些空间必须额外分割出来（参考后面各分区的作用）。当然就系统稳定来说，最好都分割出来。</li>\n</ul>\n<h2 class=\"my-blog-head\" id=\"my-blog-head35\">附：Linux各个分区的作用</h2><p>/ 根目录，建议在根目录下面只有目录，不要直接有文件。</p>\n<p>swap 交换空间，相当于Windows上的虚拟内存。</p>\n<p>/boot 包含了操作系统的内核和在启动系统过程中所要用到的文件，建这个分区是有必要的，因为目前大多数的PC机要受到BIOS的限制, 况且如果有了一个单独的/boot启动分区，即使主要的根分区出现了问题，计算机依然能够启动。这个分区的大小约在60MB—120MB之间。</p>\n<p>/home 用户的home目录所在地，这个分区的大小取决于有多少用户。如果是多用户共同使用一台电脑的话，这个分区是完全有必要的，况且根用户也可以很好地控制普通用户使用计算机，如对用户或者用户组实行硬盘限量使用，限制普通用户访问哪些文件等。<br>以往Linux系统主要是提供服务器使用，所以/home这个目录使用量并不高。但随著Linux的桌面应用发展，不少人也拿来在日常上使用，这时/home就变成存储媒体中，最占容量的目录。假如你安装Ubuntu主要是桌面应用，那你可能需要把最大的空间留给他。<br>额外分割出/home有个最大的好处，当你重新安装系统时，你不需要特别去备份你的个人文件，只要在安装时，选择不要格式化这个分区，重新挂载为/home就不会丢失你的数据。<br>还有一个特别的应用：假如你会在你的计算机上，安装两个或更多的Linux系统，你可以共享/home这个分区。简单地说，你的个人文件可以在切换到其它Linux系统时，仍正常使用</p>\n<p>/tmp 用来存放临时文件。这对于多用户系统或者网络服务器来说是有必要的。这样即使程序运行时生成大量的临时文件，或者用户对系统进行了错误的操作，文件系统的其它部分仍然是安全的。因为文件系统的这一部分仍然还承受着读写操作，所以它通常会比其它的部分更快地发生问题。这个目录是任何人都能访问的，所以需要定期清理。</p>\n<p>/usr Linux系统存放软件的地方，如有可能应将最大空间分给它<br>除了系统的基本程序外，其它所有的应用程序多放在这个目录当中。除了/home,/var这种变动数据的存放目录外，/usr大概是会是使用容量最大的目录，不过一般Linux下的应用程序通常不大，所以大多数的桌面应用顶多3~4GB的空间就已经相当足够了，若是服务器，多半也是2~3GB就足够了。</p>\n<p>/bin<br>/usr/bin<br>/usr/local/bin 存放标准系统实用程序。</p>\n<p>/srv 一些服务启动之后，这些服务所需要访问的数据目录，如WWW服务器需要的网页数据就可以放在/srv/www中。</p>\n<p>/etc 系统主要的设置文件几乎都放在这个目录内。</p>\n<p>/lib<br>/usr/lib<br>/usr/local/lib 系统使用的函数库的目录。</p>\n<p>/root 系统管理员的家目录。</p>\n<p>/lost+found 该目录在大多数情况下都是空的，但当实然停电或者非正常关机后，有些文件临时存入在此。</p>\n<p>/dev 设备文件，在Linux系统上，任何设备都以文件类型存放在这个目录中，如硬盘设备文件，软驱、光驱设备文件等。</p>\n<p>/mnt<br>/media 挂载目录，用来临时挂载别的文件系统或者别的硬件设备（如光驱、软驱）。</p>\n<p>/opt 用于存储第三方软件的目录，不过我们还是习惯放在/usr/local下</p>\n<p>/proc 此目录信息是在内存中由系统自行产生的，存储了一些当前的进程ID号和CPU、内存的映射等，因为这个目录下的数据都在内存中，所以本身不占任何硬盘空间。</p>\n<p>/sbin<br>/usr/sbin<br>/usr/local/sbin 存放一些系统管理员才会用到的执行命令。</p>\n<p>/var 主要放置系统执行过程中经常变化的文件，例如缓存（cache）或者是随时更改的登录文件（log file）。<br>假如你的计算机主要是提供网页服务，或者是mysql数据库，那/var会大量增加，你最好能够把/var额外分割出来。与/home的概念类似，重新安装时，不要格式化，仍可保留原来的数据。<br>在服务器的应用时，数据的安全是相当重要的，额外分区对数据的安全也有所帮助。此外，/var/log是系统log档保存的位置，养成有问题就去找log的好习惯，有助于解决问题。所以这也加强了额外分区的重要性。当一个服务器出现系统问题，甚至毁损时，除了你的数据外，之前的系统纪录也相当重要，找出为什么系统会出问题，可以帮助管理器快速排除障碍。</p>\n<p>/var/log 系统日志记录分区，如果设立了这一单独的分区，这样即使系统的日志文件出现了问题，它们也不会影响到操作系统的主分区。</p>\n','','新手必须了解的分区方案',2,_binary '\0');
/*!40000 ALTER TABLE `article` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `article_tag_mapper`
--

DROP TABLE IF EXISTS `article_tag_mapper`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `article_tag_mapper` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `article_id` varchar(128) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '文章id',
  `tag_id` varchar(128) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '标签id',
  `create_time` int(13) NOT NULL COMMENT '创建时间',
  PRIMARY KEY (`id`),
  UNIQUE KEY `id` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `article_tag_mapper`
--

LOCK TABLES `article_tag_mapper` WRITE;
/*!40000 ALTER TABLE `article_tag_mapper` DISABLE KEYS */;
INSERT INTO `article_tag_mapper` VALUES (2,'E0zqyEeRilsDSjSwFAU5I','5Q2wDehOynzl2ySFglgznU',1550816159),(3,'6D3i0FJnnvTtKPJJ8ByTlm','5S3rcqu91OLhMP9jBWKFpE',1550837353),(4,'382zLecONoQTT1Vjqa9sVa','7DRB1hQJRukVzRbWxgY16',1550837609),(5,'4fDo0bfqLHlXvT90PdVqNq','2EXWmbQS09QNCBjcR1Qdio',1550837785),(6,'1NZhVF4ktYzGhvFSq2ZDKm','2EXWmbQS09QNCBjcR1Qdio',1550837866);
/*!40000 ALTER TABLE `article_tag_mapper` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `blog_config`
--

DROP TABLE IF EXISTS `blog_config`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `blog_config` (
  `id` int(10) NOT NULL AUTO_INCREMENT COMMENT 'id',
  `blog_name` varchar(255) DEFAULT NULL COMMENT '博客名称',
  `avatar` text COMMENT '头像',
  `sign` text COMMENT '个性签名',
  `wxpay_qrcode` text COMMENT '微信支付二维码',
  `alipay_qrcode` text COMMENT '支付宝支付二维码',
  `github` text COMMENT 'github',
  `view_password` varchar(255) DEFAULT NULL COMMENT '阅读加密密码',
  `salt` varchar(255) DEFAULT NULL COMMENT '阅读加密秘钥',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `blog_config`
--

LOCK TABLES `blog_config` WRITE;
/*!40000 ALTER TABLE `blog_config` DISABLE KEYS */;
INSERT INTO `blog_config` VALUES (1,'Yuho\'s Blogs','http://www.yuhaochen.top:8888/imgs/2019/02/fade175043f0b23a.jpg','share my idea for world','http://www.yuhaochen.top:8888/imgs/2019/02/d1276e2e0c70327b.jpg','http://www.yuhaochen.top:8888/imgs/2019/02/151fcd6c6efc31bf.jpg','https://github.com/yuhochen','d9132638e54c11fac5e7cb6b2f65b1b8','$2y$10$KNWWPVVBOyh0YQ8gc8K3ROmJr.UNDomipCJrExxm.47kU58qzT.qy');
/*!40000 ALTER TABLE `blog_config` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `category`
--

DROP TABLE IF EXISTS `category`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `category` (
  `aid` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `id` varchar(128) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' COMMENT '分类id',
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL COMMENT '分类名称',
  `create_time` int(13) NOT NULL COMMENT '创建时间',
  `update_time` int(13) DEFAULT NULL COMMENT '更新时间',
  `status` bit(1) DEFAULT b'0' COMMENT '状态，0为正常，1为删除，默认0',
  `article_count` int(11) DEFAULT '0' COMMENT '该分类的文章数量',
  `can_del` bit(1) NOT NULL DEFAULT b'1' COMMENT '0表示不可删除，1表示可删除，默认1',
  PRIMARY KEY (`aid`,`id`) USING BTREE,
  UNIQUE KEY `id` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `category`
--

LOCK TABLES `category` WRITE;
/*!40000 ALTER TABLE `category` DISABLE KEYS */;
INSERT INTO `category` VALUES (1,'25ffz1VaNrpec1nc2zeSZu','测试分类',1550673286,1550832753,_binary '\0',0,_binary ''),(2,'5DKFCekkrGH981BYKRrqt8','深度学习',1550816123,1550837538,_binary '\0',2,_binary ''),(3,'61m7ZT6t49NNWlcDX9AAtQ','大数据',1550837259,1550837353,_binary '\0',1,_binary ''),(4,'bkU6nGaHJrggRmNfUkQuU','操作系统',1550837731,1550837866,_binary '\0',2,_binary '');
/*!40000 ALTER TABLE `category` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `comments`
--

DROP TABLE IF EXISTS `comments`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `comments` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `article_id` varchar(128) NOT NULL COMMENT '文章id',
  `parent_id` int(10) NOT NULL DEFAULT '0' COMMENT '父id, 默认0',
  `reply_id` int(10) DEFAULT NULL COMMENT '回复的评论id',
  `name` varchar(255) NOT NULL COMMENT '评论者名称',
  `email` varchar(128) DEFAULT NULL COMMENT '评论者邮箱',
  `content` text NOT NULL COMMENT '评论内容json',
  `source_content` text COMMENT '评论内容（原始内容）',
  `create_time` int(13) NOT NULL COMMENT '创建时间',
  `delete_time` int(13) DEFAULT NULL COMMENT '删除时间',
  `status` tinyint(1) DEFAULT '0' COMMENT '状态，0正常，1删除，默认0',
  `is_author` bit(1) DEFAULT b'0' COMMENT '是否是作者，0否，1是，默认0',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `comments`
--

LOCK TABLES `comments` WRITE;
/*!40000 ALTER TABLE `comments` DISABLE KEYS */;
/*!40000 ALTER TABLE `comments` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `friends`
--

DROP TABLE IF EXISTS `friends`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `friends` (
  `aid` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `friend_id` varchar(128) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '友链id',
  `name` varchar(255) NOT NULL COMMENT '友链名称',
  `url` text NOT NULL COMMENT '对应链接',
  `create_time` int(13) NOT NULL COMMENT '创建时间',
  `update_time` int(13) DEFAULT NULL COMMENT '更新时间',
  `delete_time` int(13) DEFAULT NULL COMMENT '删除时间',
  `status` bit(1) NOT NULL DEFAULT b'0' COMMENT '状态，0表示可用，1表示删除，默认0',
  `type_id` int(11) NOT NULL DEFAULT '0' COMMENT '所属分类id',
  PRIMARY KEY (`aid`,`friend_id`) USING BTREE,
  UNIQUE KEY `friend_id` (`friend_id`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `friends`
--

LOCK TABLES `friends` WRITE;
/*!40000 ALTER TABLE `friends` DISABLE KEYS */;
INSERT INTO `friends` VALUES (1,'3Gv287eltN1VtNmog5AraQ','百度','https://www.baidu.com',1550819274,1550820044,NULL,_binary '\0',1),(2,'61WfPWDTSzmknCKzYDXyg0','谷歌','https://www.google.com',1550832659,NULL,NULL,_binary '\0',1);
/*!40000 ALTER TABLE `friends` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `friends_type`
--

DROP TABLE IF EXISTS `friends_type`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `friends_type` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '分类id',
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL COMMENT '分类名称',
  `count` int(11) DEFAULT '0' COMMENT '该分类的友链数量',
  PRIMARY KEY (`id`),
  UNIQUE KEY `id` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `friends_type`
--

LOCK TABLES `friends_type` WRITE;
/*!40000 ALTER TABLE `friends_type` DISABLE KEYS */;
INSERT INTO `friends_type` VALUES (1,'搜索引擎',503);
/*!40000 ALTER TABLE `friends_type` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `pages`
--

DROP TABLE IF EXISTS `pages`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `pages` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `type` varchar(128) NOT NULL COMMENT '页面名称',
  `md` text COMMENT 'markdown内容',
  `html` text COMMENT '生成的html内容',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `pages`
--

LOCK TABLES `pages` WRITE;
/*!40000 ALTER TABLE `pages` DISABLE KEYS */;
/*!40000 ALTER TABLE `pages` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `sys_log`
--

DROP TABLE IF EXISTS `sys_log`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `sys_log` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT 'id',
  `time` int(13) NOT NULL COMMENT '时间',
  `content` text COMMENT '日志内容',
  `ip` varchar(30) DEFAULT NULL COMMENT '客户端IP地址',
  PRIMARY KEY (`id`),
  UNIQUE KEY `id` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=37 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `sys_log`
--

LOCK TABLES `sys_log` WRITE;
/*!40000 ALTER TABLE `sys_log` DISABLE KEYS */;
INSERT INTO `sys_log` VALUES (1,1550148298,'创建管理员账号 admin','127.0.0.1'),(2,1550148328,'管理员 admin 登录系统','127.0.0.1'),(3,1550666908,'管理员 admin 登录系统','127.0.0.1'),(4,1550673332,'管理员 admin 发布了文章(7hgdrxJvILWPMvwgcAtsX6)','127.0.0.1'),(5,1550715030,'管理员 admin 修改网站配置信息','127.0.0.1'),(6,1550716939,'管理员 admin 修改网站配置信息','127.0.0.1'),(7,1550738137,'管理员 admin 编辑了文章(7hgdrxJvILWPMvwgcAtsX6)','127.0.0.1'),(8,1550738180,'管理员 admin 修改网站配置信息','127.0.0.1'),(9,1550763298,'管理员 admin 登录系统','127.0.0.1'),(10,1550763346,'管理员 admin 编辑了文章(7hgdrxJvILWPMvwgcAtsX6)','127.0.0.1'),(11,1550763366,'管理员 admin 修改网站配置信息','127.0.0.1'),(12,1550805259,'管理员 admin 登录系统','127.0.0.1'),(13,1550816095,'管理员 admin 发布了文章(E0zqyEeRilsDSjSwFAU5I)','127.0.0.1'),(14,1550816159,'管理员 admin 编辑了文章(E0zqyEeRilsDSjSwFAU5I)','127.0.0.1'),(15,1550816242,'管理员 admin 修改网站配置信息','127.0.0.1'),(16,1550816450,'管理员 admin 修改网站配置信息','127.0.0.1'),(17,1550816566,'管理员 admin 修改网站配置信息','127.0.0.1'),(18,1550817132,'管理员 admin 修改网站配置信息','127.0.0.1'),(19,1550817182,'管理员 admin 编辑了文章(7hgdrxJvILWPMvwgcAtsX6)','127.0.0.1'),(20,1550817326,'管理员 admin 编辑了文章(7hgdrxJvILWPMvwgcAtsX6)','127.0.0.1'),(21,1550817426,'管理员 admin 编辑了文章(7hgdrxJvILWPMvwgcAtsX6)','127.0.0.1'),(22,1550832753,'管理员 admin 删除了文章(7hgdrxJvILWPMvwgcAtsX6)','127.0.0.1'),(23,1550832800,'管理员 admin 保存了文章(2vCfKELG6vfQqRZkp038Xa)','127.0.0.1'),(24,1550832809,'管理员 admin 删除了文章(2vCfKELG6vfQqRZkp038Xa)','127.0.0.1'),(25,1550832817,'管理员 admin 删除了文章(2vCfKELG6vfQqRZkp038Xa)','127.0.0.1'),(26,1550836924,'管理员 admin 登录系统','127.0.0.1'),(27,1550837242,'管理员 admin 保存了文章(6D3i0FJnnvTtKPJJ8ByTlm)','127.0.0.1'),(28,1550837288,'管理员 admin 保存了文章(6D3i0FJnnvTtKPJJ8ByTlm)','127.0.0.1'),(29,1550837353,'管理员 admin 发布了文章(6D3i0FJnnvTtKPJJ8ByTlm)','127.0.0.1'),(30,1550837538,'管理员 admin 发布了文章(382zLecONoQTT1Vjqa9sVa)','127.0.0.1'),(31,1550837609,'管理员 admin 编辑了文章(382zLecONoQTT1Vjqa9sVa)','127.0.0.1'),(32,1550837695,'管理员 admin 发布了文章(4fDo0bfqLHlXvT90PdVqNq)','127.0.0.1'),(33,1550837785,'管理员 admin 编辑了文章(4fDo0bfqLHlXvT90PdVqNq)','127.0.0.1'),(34,1550837866,'管理员 admin 发布了文章(1NZhVF4ktYzGhvFSq2ZDKm)','127.0.0.1'),(35,1550891510,'管理员 admin 登录系统','127.0.0.1'),(36,1550892547,'管理员 admin 登录系统','127.0.0.1');
/*!40000 ALTER TABLE `sys_log` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `tag`
--

DROP TABLE IF EXISTS `tag`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `tag` (
  `aid` int(11) NOT NULL AUTO_INCREMENT,
  `id` varchar(128) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' COMMENT '标签id',
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL COMMENT '标签名称',
  `create_time` int(13) NOT NULL COMMENT '创建时间',
  `update_time` int(13) DEFAULT NULL COMMENT '更新时间',
  `status` bit(1) DEFAULT b'0' COMMENT '状态，0表示正常，1表示删除，默认0',
  `article_count` int(11) DEFAULT '0' COMMENT '该标签的文章数量',
  PRIMARY KEY (`aid`,`id`) USING BTREE,
  UNIQUE KEY `id` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `tag`
--

LOCK TABLES `tag` WRITE;
/*!40000 ALTER TABLE `tag` DISABLE KEYS */;
INSERT INTO `tag` VALUES (1,'5KV7aVIveWtvmaFedLHnDC','测试标签',1550673298,1550832753,_binary '\0',0),(2,'5Q2wDehOynzl2ySFglgznU','AI',1550816129,1550816159,_binary '\0',1),(3,'5S3rcqu91OLhMP9jBWKFpE','spark',1550837269,1550837353,_binary '\0',1),(4,'7DRB1hQJRukVzRbWxgY16','pytorch',1550837596,1550837609,_binary '\0',1),(5,'2EXWmbQS09QNCBjcR1Qdio','操作系统',1550837762,1550837866,_binary '\0',2);
/*!40000 ALTER TABLE `tag` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2019-02-23 18:39:07
